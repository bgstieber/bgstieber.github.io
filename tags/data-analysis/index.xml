<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Analysis on Brad Stieber</title>
    <link>/tags/data-analysis/</link>
    <description>Recent content in Data Analysis on Brad Stieber</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 03 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/data-analysis/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>7 Tips for Delivering a Great Data Science Presentation</title>
      <link>/post/7-tips-for-delivering-a-great-data-science-presentation/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/7-tips-for-delivering-a-great-data-science-presentation/</guid>
      <description>Delivering a great data science presentation can seem daunting. By no means am I a communications expert, but I have presented my fair share of talks to a diverse group of audiences. Through my experience, I’ve developed a few easy-to-remember tips to hopefully make your next data science presentation your best yet. These are tips that have worked for me, and I hope they’re helpful!
Without further ado, here are seven tips for delivering a great data science presentation.</description>
    </item>
    
    <item>
      <title>Roulette Wheels for Multi-Armed Bandits: A Simulation in R</title>
      <link>/post/roulette-wheels-for-multi-armed-bandits-a-simulation-in-r/</link>
      <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/roulette-wheels-for-multi-armed-bandits-a-simulation-in-r/</guid>
      <description>One of my favorite data science blogs comes from James McCaffrey, a software engineer and researcher at Microsoft. He recently wrote a blog post on a method for allocating turns in a multi-armed bandit problem.
I really liked his post, and decided to take a look at the algorithm he described and code up a function to do the simulation in R.
Note: this is strictly an implementation of Dr. McCaffrey’s ideas from his blog post, and should not be taken as my own.</description>
    </item>
    
    <item>
      <title>Recommending Songs Using Cosine Similarity in R</title>
      <link>/post/recommending-songs-using-cosine-similarity-in-r/</link>
      <pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/recommending-songs-using-cosine-similarity-in-r/</guid>
      <description>Recommendation engines have a huge impact on our online lives. The content we watch on Netflix, the products we purchase on Amazon, and even the homes we buy are all served up using these algorithms. In this post, I’ll run through one of the key metrics used in developing recommendation engines: cosine similarity.
First, I’ll give a brief overview of some vocabulary we’ll need to understand recommendation systems. Then, I’ll look at the math behind cosine similarity.</description>
    </item>
    
    <item>
      <title>Iterating on a 2016 Election Analysis</title>
      <link>/post/iterating-on-a-2016-election-analysis/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/iterating-on-a-2016-election-analysis/</guid>
      <description>Jake Low wrote a really interesting piece that presented a few data visualizations that went beyond the typical 2016 election maps we’ve all gotten used to seeing.
I liked a lot of things about Jake’s post, here are three I was particularly fond of:
His color palette choicesEach color palette that was used had solid perceptual properties and made sense for the data being visualized (i.e. diverging versus sequential)He made residuals from a model interesting by visualizing and interpreting themHe explained the usage of a log-scale transformation in an intuitive way, putting it in terms of the data set being used for the analysis.</description>
    </item>
    
    <item>
      <title>Everything I Know About Machine Learning I Learned from Making Soup</title>
      <link>/post/everything-i-know-about-machine-learning-i-learned-from-making-soup/</link>
      <pubDate>Mon, 06 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/everything-i-know-about-machine-learning-i-learned-from-making-soup/</guid>
      <description>IntroductionIn this post, I’m going to make the claim that we can simplify some parts of the machine learning process by using the analogy of making soup. I think this analogy can improve how a data scientist explains machine learning to a broad audience, and it provides a helpful framework throughout the model building process.
Relying on some insight from the CRISP-DM framework, my own experience as an amateur chef, and the well-known iris data set, I’m going to explain why I think that the soup making and machine learning connection is a pretty decent first approximation you could use to understand the machine learning process.</description>
    </item>
    
    <item>
      <title>Golf, Tidy Data, and Using Data Analysis to Guide Strategy</title>
      <link>/post/golf-tidy-data-and-using-data-analysis-to-guide-strategy/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/golf-tidy-data-and-using-data-analysis-to-guide-strategy/</guid>
      <description>IntroductionI’m going to use this post to discuss some of the aspects of data science that interest me most (tidy data as well as using data to guide strategy). I’ll be discussing these topics through the lens of a data analysis of results from a few high school golf tournaments.
I’m going to take a little bit of time to talk about tidy data. When I scraped the data used for this analysis, it wasn’t really stored in a tidy format, and there’s a good reason for that.</description>
    </item>
    
    <item>
      <title>An Introduction to the kmeans Algorithm</title>
      <link>/post/an-introduction-to-the-kmeans-algorithm/</link>
      <pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/an-introduction-to-the-kmeans-algorithm/</guid>
      <description>This post will provide an R code-heavy, math-light introduction to selecting the \(k\) in k means. It presents the main idea of kmeans, demonstrates how to fit a kmeans in R, provides some components of the kmeans fit, and displays some methods for selecting k. In addition, the post provides some helpful functions which may make fitting kmeans a bit easier.
kmeans clustering is an example of unsupervised learning, where we do not have an output we’re explicitly trying to predict.</description>
    </item>
    
  </channel>
</rss>