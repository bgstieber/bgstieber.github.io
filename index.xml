<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Brad Stieber</title>
    <link>/</link>
    <description>Recent content on Brad Stieber</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>January 2, 2021</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>My Year in Books: Goodreads Data Analysis in R</title>
      <link>/post/my-year-in-books-goodreads-data-analysis-in-r/</link>
      <pubDate>January 2, 2021</pubDate>
      
      <guid>/post/my-year-in-books-goodreads-data-analysis-in-r/</guid>
      <description>&lt;p&gt;In 2020, I set a goal of reading 30 books. Aided by a last minute charge, I managed to hit this number. I finished my 30th book on December 31st.&lt;/p&gt;
&lt;p&gt;As I was finishing up my year of reading, I started thinking about some of the statistics of my year in books:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On average, how many pages did I read per day?&lt;/li&gt;
&lt;li&gt;Did I have any slumps during the year? If so, could the slumps be explained?&lt;/li&gt;
&lt;li&gt;What would be a reasonable reading goal for 2021?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I tracked all of my books using &lt;a href=&#34;https://www.goodreads.com/&#34;&gt;Goodreads&lt;/a&gt;, so I started poking around on the Goodreads website to see if I could access my library.&lt;/p&gt;
&lt;p&gt;I used the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://lubridate.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;lubridate&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, and &lt;a href=&#34;https://scales.r-lib.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;scales&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; packages in this analysis. You can find the code for this post &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/tree/master/goodreads-data-analysis&#34;&gt;on my GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;getting-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting the Data&lt;/h2&gt;
&lt;p&gt;Getting Goodreads data isn’t too difficult. They have a great export tool, and if you follow &lt;a href=&#34;https://www.goodreads.com/review/import&#34;&gt;this link&lt;/a&gt;, you can export your library. If you have a lot of books in your library the export can take a long time. The data export comes with 31 columns.&lt;/p&gt;
&lt;p&gt;For this analysis, the columns I’m interested in are &lt;strong&gt;Date Read&lt;/strong&gt;, &lt;strong&gt;My Rating&lt;/strong&gt; (what I rated the book, 0-5 stars), &lt;strong&gt;Average Rating&lt;/strong&gt;, &lt;strong&gt;Number of Pages&lt;/strong&gt;, and &lt;strong&gt;Original Publication Year&lt;/strong&gt;. I added &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/blob/master/goodreads-data-analysis/goodreads_library_export.csv&#34;&gt;my data&lt;/a&gt; to a &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/tree/master/goodreads-data-analysis&#34;&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One thing that’s missing from the Goodreads data export is the description of the book. I wrote a &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/blob/master/goodreads-data-analysis/scrape_goodreads.py&#34;&gt;python script&lt;/a&gt; that uses &lt;a href=&#34;https://www.crummy.com/software/BeautifulSoup/&#34;&gt;BeautifulSoup&lt;/a&gt; to scrape Goodreads for this information. I don’t use it in &lt;em&gt;this post&lt;/em&gt;, but I could see using it in a different post down the road.&lt;/p&gt;
&lt;p&gt;The data from Goodreads is mostly good to go, but there are a few tweaks to make before getting started.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gh_link1 &amp;lt;- &amp;quot;https://github.com/bgstieber/files_for_blog/raw/master/&amp;quot;
gh_link2 &amp;lt;- &amp;quot;goodreads-data-analysis/goodreads_library_export.csv&amp;quot;

goodreads_data &amp;lt;- read_csv(paste0(gh_link1, gh_link2)) %&amp;gt;%
  # fix issue with data export for a book
  mutate(`Number of Pages` = ifelse(grepl(&amp;quot;Be a Player&amp;quot;, Title),
                                    256, `Number of Pages`))

books_2020 &amp;lt;- goodreads_data %&amp;gt;%
  # only 2020 books
  filter(year(`Date Read`) == 2020) %&amp;gt;%
  # create rating_diff and publish_year columns
  mutate(rating_diff = `My Rating` - `Average Rating`,
         publish_year = coalesce(`Original Publication Year`,
                                  `Year Published`)) %&amp;gt;%
  # clean some column names
  rename(date_read = `Date Read`,
         page_count = `Number of Pages`,
         avg_rating = `Average Rating`,
         my_rating = `My Rating`) %&amp;gt;%
  # add when the previous book was finished, sort then lag
  arrange(date_read) %&amp;gt;%
  mutate(previous_book_date = lag(date_read))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this analysis, I make the assumption that I read only one book at a time (not always true), and that I start reading a book immediately after I finish the previous one (not always true either).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trends-and-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trends and Analysis&lt;/h2&gt;
&lt;p&gt;Here is the timeline of my year in books:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-01-02-my-year-in-books-goodreads-data-analysis-in-r.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;528&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sometimes the most basic data visualizations present the most compelling information.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here are a few things that stood out to me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;My sprint at the end of the year to hit my reading goal&lt;/li&gt;
&lt;li&gt;A few books with longer read times: &lt;em&gt;The Remains of the Day&lt;/em&gt;, &lt;em&gt;Never Let Me Go&lt;/em&gt;, and &lt;em&gt;Be a Player: How to Become a Better Golfer Every Time You Play&lt;/em&gt; (to a lesser extent)
&lt;ul&gt;
&lt;li&gt;These will come up again in calculating my 2021 goal&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Apart from the few books mentioned above, I had pretty consistent read times for my 2020 books. What might be driving this?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the code below, I create a &lt;code&gt;data.frame&lt;/code&gt; with cumulative pages and books read by date.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary_by_date &amp;lt;- books_2020 %&amp;gt;%
  group_by(date_read, Title) %&amp;gt;%
  summarise(pages = sum(page_count),
            books = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  # add dummy data for beginning of year
  bind_rows(tibble(date_read = as.Date(&amp;quot;2020-01-01&amp;quot;),
                   Title = NA_character_,
                   pages = 0,
                   books = 0)) %&amp;gt;%
  arrange(date_read) %&amp;gt;%
  mutate(previous_date = lag(date_read)) %&amp;gt;%
  mutate(days_since_last_book = as.numeric(difftime(
    date_read, previous_date, units = &amp;quot;days&amp;quot;
  ))) %&amp;gt;%
  mutate(cumu_pages_read = cumsum(pages),
         cumu_books_read = cumsum(books))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using this data, I can look at my progress toward 30 books through the year.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-01-02-my-year-in-books-goodreads-data-analysis-in-r.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My reading certainly slowed down during the summer months. Most of this is due to me doing other things during a beautiful Wisconsin summer like playing golf and riding my bike. Between January and May, I read an average of 39.6 pages per day, between June and September, I read about 14.8 pages per day, and finishing off the year, I read 31.3 pages per day from October through the end of the year.&lt;/p&gt;
&lt;p&gt;For most of the year, I had a fairly consistent book-finishing pace. I think a lot of this can be explained by choosing shorter books in 2020. 70% of the books I read this year were less than 400 pages long.&lt;/p&gt;
&lt;p&gt;Another interesting aspect of the books I read in 2020 was that they were mostly modern. 80% of the books I read in 2020 were published in 1990 or later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;books_2020 %&amp;gt;%
  ggplot(aes(publish_year))+
  geom_bar()+
  xlab(&amp;quot;Year Published&amp;quot;)+
  ylab(&amp;quot;Books&amp;quot;)+
  ggtitle(&amp;quot;When were my 2020 reads published?&amp;quot;,
          subtitle = paste0(percent(mean(books_2020$publish_year &amp;gt;= 1990)),
                            &amp;quot; of books I read in 2020 were published &amp;quot;,
                            &amp;quot;in 1990 or later.&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-01-02-my-year-in-books-goodreads-data-analysis-in-r.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The oldest book I read was &lt;em&gt;The House of Mirth&lt;/em&gt; by Edith Wharton, published in 1905. The most recent book I read was &lt;em&gt;The Art of Solitude&lt;/em&gt; by Stephen Batchelor, published in 2020.&lt;/p&gt;
&lt;p&gt;Finally, let’s take a look at how my rating of a book compared to the average rating from other Goodreads users.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;books_2020 %&amp;gt;%
    mutate(title_abbrev = 
               ifelse(nchar(Title) &amp;gt; 60,
                      paste0(substr(Title, 1, 60), &amp;quot;...&amp;quot;),
                      Title)) %&amp;gt;%
    ggplot(aes(reorder(title_abbrev, rating_diff),
               rating_diff,
               fill = factor(my_rating)))+
    geom_col(colour = &amp;quot;black&amp;quot;)+
    coord_flip()+
    scale_fill_viridis_d(&amp;quot;My Rating&amp;quot;, option = &amp;quot;cividis&amp;quot;)+
    xlab(&amp;quot;&amp;quot;)+
    ylab(&amp;quot;My Rating - Goodreads Avg&amp;quot;)+
    theme(legend.position = &amp;quot;top&amp;quot;,
          axis.text.y = element_text(size = 8))+
    ggtitle(&amp;quot;My Rating Versus the Goodreads Average&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-01-02-my-year-in-books-goodreads-data-analysis-in-r.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;816&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My average rating in 2020 was 4, the average Goodreads rating of the books I read in 2020 was 4.1. I gave 9 books 3 stars, 11 books a rating of 4 stars, and I gave 10 books 5 stars.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2021 Goal&lt;/h2&gt;
&lt;p&gt;This post has mostly been an exploratory analysis of my Goodreads data. To make it actionable, let’s focus on &lt;strong&gt;setting a data-driven reading goal for 2021&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To start, let’s look at the average number of pages I was reading throughout the year.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-01-02-my-year-in-books-goodreads-data-analysis-in-r.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I was reading at a pretty consistent pace in the beginning of the year, declined sharply during the warm summer months, and then picked back up at the end of the year.&lt;/p&gt;
&lt;p&gt;On average, it took me about 12.2 days to finish a book in 2020. I read at a pace of about 28.9 pages per day.&lt;/p&gt;
&lt;p&gt;There were a few clear outliers with respect to reading pace throughout the year. I read two novels (&lt;em&gt;The Remains of the Day&lt;/em&gt; and &lt;em&gt;Never Let Me Go&lt;/em&gt;, both by Kazuo Ishiguro) very slowly, taking 43 and 28 days to finish those books, respectively. I also read two books at a very fast pace (&lt;em&gt;Red Queen&lt;/em&gt; and &lt;em&gt;The Art of Solitude&lt;/em&gt;), where I was reading at a pace of 76.6 and 66.7 pages per day, respectively.&lt;/p&gt;
&lt;p&gt;If we eliminate those four books, we’re left with a set of books that more closely reflects my typical or baseline reading pace. Looking at the remaining 26 books, I was reading at a pace of about 32.9 pages per day, taking about 11 days to finish a book.&lt;/p&gt;
&lt;p&gt;Using the pace of 11 days to finish a book, I could create a goal of reading 365/11 = 33.2 books in 2021. Rounding up, I’ll set a goal of 34 books in 2021.&lt;/p&gt;
&lt;p&gt;This represents an increase of 13% over my goal last year, which seems pretty reasonable based on this analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;In 2020, I set a goal to finish 30 books. On December 31st, I finished &lt;em&gt;The Art of Solitude&lt;/em&gt; and completed my reading goal. I explored my Goodreads data to summarize my year in books:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I read a total of 10,536 pages in 2020, the average length of a book I read in 2020 was 351.2 pages&lt;/li&gt;
&lt;li&gt;I read at a pace of 28.9 pages per day&lt;/li&gt;
&lt;li&gt;On average it took me about 12.2 days to complete each book&lt;/li&gt;
&lt;li&gt;The longest it took me to finish a book was 43 days (&lt;em&gt;Never Let Me Go&lt;/em&gt;), my shortest read time was 3 days (&lt;em&gt;The Art of Solitude&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;My average rating was 4 stars, the average Goodreads rating of the books I read was 4.1 stars&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also used the Goodreads data to set a data-driven reading goal for 2021. I hope to increase my reading by 13% in 2021 by finishing 34 books.&lt;/p&gt;
&lt;p&gt;This was a fun way to look back on my year in books for 2020. There are a few aspects of this data that I could look into like the distribution of genres, the text summary of the book, and text reviews from other Goodreads users. That analysis will have to wait for another day!&lt;/p&gt;
&lt;p&gt;Happy reading!&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded><p>In 2020, I set a goal of reading 30 books. Aided by a last minute charge, I managed to hit this number. I finished my 30th book on December 31st.</p>
<p>As I was finishing up my year of reading, I started thinking about some of the statistics of my year in books:</p>
<ul>
<li>On average, how many pages did I read per day?</li>
<li>Did I have any slumps during the year? If so, could the slumps be explained?</li>
<li>What would be a reasonable reading goal for 2021?</li>
</ul>
<p>I tracked all of my books using <a href="https://www.goodreads.com/">Goodreads</a>, so I started poking around on the Goodreads website to see if I could access my library.</p>
<p>I used the <a href="https://www.tidyverse.org/"><strong><code>tidyverse</code></strong></a>, <a href="https://lubridate.tidyverse.org/"><strong><code>lubridate</code></strong></a>, and <a href="https://scales.r-lib.org/"><strong><code>scales</code></strong></a> packages in this analysis. You can find the code for this post <a href="https://github.com/bgstieber/files_for_blog/tree/master/goodreads-data-analysis">on my GitHub</a>.</p>
<div id="getting-the-data" class="section level2">
<h2>Getting the Data</h2>
<p>Getting Goodreads data isn’t too difficult. They have a great export tool, and if you follow <a href="https://www.goodreads.com/review/import">this link</a>, you can export your library. If you have a lot of books in your library the export can take a long time. The data export comes with 31 columns.</p>
<p>For this analysis, the columns I’m interested in are <strong>Date Read</strong>, <strong>My Rating</strong> (what I rated the book, 0-5 stars), <strong>Average Rating</strong>, <strong>Number of Pages</strong>, and <strong>Original Publication Year</strong>. I added <a href="https://github.com/bgstieber/files_for_blog/blob/master/goodreads-data-analysis/goodreads_library_export.csv">my data</a> to a <a href="https://github.com/bgstieber/files_for_blog/tree/master/goodreads-data-analysis">GitHub repository</a>.</p>
<p>One thing that’s missing from the Goodreads data export is the description of the book. I wrote a <a href="https://github.com/bgstieber/files_for_blog/blob/master/goodreads-data-analysis/scrape_goodreads.py">python script</a> that uses <a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> to scrape Goodreads for this information. I don’t use it in <em>this post</em>, but I could see using it in a different post down the road.</p>
<p>The data from Goodreads is mostly good to go, but there are a few tweaks to make before getting started.</p>
<pre class="r"><code>gh_link1 &lt;- &quot;https://github.com/bgstieber/files_for_blog/raw/master/&quot;
gh_link2 &lt;- &quot;goodreads-data-analysis/goodreads_library_export.csv&quot;

goodreads_data &lt;- read_csv(paste0(gh_link1, gh_link2)) %&gt;%
  # fix issue with data export for a book
  mutate(`Number of Pages` = ifelse(grepl(&quot;Be a Player&quot;, Title),
                                    256, `Number of Pages`))

books_2020 &lt;- goodreads_data %&gt;%
  # only 2020 books
  filter(year(`Date Read`) == 2020) %&gt;%
  # create rating_diff and publish_year columns
  mutate(rating_diff = `My Rating` - `Average Rating`,
         publish_year = coalesce(`Original Publication Year`,
                                  `Year Published`)) %&gt;%
  # clean some column names
  rename(date_read = `Date Read`,
         page_count = `Number of Pages`,
         avg_rating = `Average Rating`,
         my_rating = `My Rating`) %&gt;%
  # add when the previous book was finished, sort then lag
  arrange(date_read) %&gt;%
  mutate(previous_book_date = lag(date_read))</code></pre>
<p>For this analysis, I make the assumption that I read only one book at a time (not always true), and that I start reading a book immediately after I finish the previous one (not always true either).</p>
</div>
<div id="trends-and-analysis" class="section level2">
<h2>Trends and Analysis</h2>
<p>Here is the timeline of my year in books:</p>
<p><img src="/post/2021-01-02-my-year-in-books-goodreads-data-analysis-in-r.en_files/figure-html/unnamed-chunk-4-1.png" width="528" style="display: block; margin: auto;" /></p>
<p><strong>Sometimes the most basic data visualizations present the most compelling information.</strong></p>
<p>Here are a few things that stood out to me:</p>
<ul>
<li>My sprint at the end of the year to hit my reading goal</li>
<li>A few books with longer read times: <em>The Remains of the Day</em>, <em>Never Let Me Go</em>, and <em>Be a Player: How to Become a Better Golfer Every Time You Play</em> (to a lesser extent)
<ul>
<li>These will come up again in calculating my 2021 goal</li>
</ul></li>
<li>Apart from the few books mentioned above, I had pretty consistent read times for my 2020 books. What might be driving this?</li>
</ul>
<p>In the code below, I create a <code>data.frame</code> with cumulative pages and books read by date.</p>
<pre class="r"><code>summary_by_date &lt;- books_2020 %&gt;%
  group_by(date_read, Title) %&gt;%
  summarise(pages = sum(page_count),
            books = n()) %&gt;%
  ungroup() %&gt;%
  # add dummy data for beginning of year
  bind_rows(tibble(date_read = as.Date(&quot;2020-01-01&quot;),
                   Title = NA_character_,
                   pages = 0,
                   books = 0)) %&gt;%
  arrange(date_read) %&gt;%
  mutate(previous_date = lag(date_read)) %&gt;%
  mutate(days_since_last_book = as.numeric(difftime(
    date_read, previous_date, units = &quot;days&quot;
  ))) %&gt;%
  mutate(cumu_pages_read = cumsum(pages),
         cumu_books_read = cumsum(books))</code></pre>
<p>Using this data, I can look at my progress toward 30 books through the year.</p>
<p><img src="/post/2021-01-02-my-year-in-books-goodreads-data-analysis-in-r.en_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>My reading certainly slowed down during the summer months. Most of this is due to me doing other things during a beautiful Wisconsin summer like playing golf and riding my bike. Between January and May, I read an average of 39.6 pages per day, between June and September, I read about 14.8 pages per day, and finishing off the year, I read 31.3 pages per day from October through the end of the year.</p>
<p>For most of the year, I had a fairly consistent book-finishing pace. I think a lot of this can be explained by choosing shorter books in 2020. 70% of the books I read this year were less than 400 pages long.</p>
<p>Another interesting aspect of the books I read in 2020 was that they were mostly modern. 80% of the books I read in 2020 were published in 1990 or later.</p>
<pre class="r"><code>books_2020 %&gt;%
  ggplot(aes(publish_year))+
  geom_bar()+
  xlab(&quot;Year Published&quot;)+
  ylab(&quot;Books&quot;)+
  ggtitle(&quot;When were my 2020 reads published?&quot;,
          subtitle = paste0(percent(mean(books_2020$publish_year &gt;= 1990)),
                            &quot; of books I read in 2020 were published &quot;,
                            &quot;in 1990 or later.&quot;))</code></pre>
<p><img src="/post/2021-01-02-my-year-in-books-goodreads-data-analysis-in-r.en_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The oldest book I read was <em>The House of Mirth</em> by Edith Wharton, published in 1905. The most recent book I read was <em>The Art of Solitude</em> by Stephen Batchelor, published in 2020.</p>
<p>Finally, let’s take a look at how my rating of a book compared to the average rating from other Goodreads users.</p>
<pre class="r"><code>books_2020 %&gt;%
    mutate(title_abbrev = 
               ifelse(nchar(Title) &gt; 60,
                      paste0(substr(Title, 1, 60), &quot;...&quot;),
                      Title)) %&gt;%
    ggplot(aes(reorder(title_abbrev, rating_diff),
               rating_diff,
               fill = factor(my_rating)))+
    geom_col(colour = &quot;black&quot;)+
    coord_flip()+
    scale_fill_viridis_d(&quot;My Rating&quot;, option = &quot;cividis&quot;)+
    xlab(&quot;&quot;)+
    ylab(&quot;My Rating - Goodreads Avg&quot;)+
    theme(legend.position = &quot;top&quot;,
          axis.text.y = element_text(size = 8))+
    ggtitle(&quot;My Rating Versus the Goodreads Average&quot;)</code></pre>
<p><img src="/post/2021-01-02-my-year-in-books-goodreads-data-analysis-in-r.en_files/figure-html/unnamed-chunk-9-1.png" width="816" /></p>
<p>My average rating in 2020 was 4, the average Goodreads rating of the books I read in 2020 was 4.1. I gave 9 books 3 stars, 11 books a rating of 4 stars, and I gave 10 books 5 stars.</p>
</div>
<div id="goal" class="section level2">
<h2>2021 Goal</h2>
<p>This post has mostly been an exploratory analysis of my Goodreads data. To make it actionable, let’s focus on <strong>setting a data-driven reading goal for 2021</strong>.</p>
<p>To start, let’s look at the average number of pages I was reading throughout the year.</p>
<p><img src="/post/2021-01-02-my-year-in-books-goodreads-data-analysis-in-r.en_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>I was reading at a pretty consistent pace in the beginning of the year, declined sharply during the warm summer months, and then picked back up at the end of the year.</p>
<p>On average, it took me about 12.2 days to finish a book in 2020. I read at a pace of about 28.9 pages per day.</p>
<p>There were a few clear outliers with respect to reading pace throughout the year. I read two novels (<em>The Remains of the Day</em> and <em>Never Let Me Go</em>, both by Kazuo Ishiguro) very slowly, taking 43 and 28 days to finish those books, respectively. I also read two books at a very fast pace (<em>Red Queen</em> and <em>The Art of Solitude</em>), where I was reading at a pace of 76.6 and 66.7 pages per day, respectively.</p>
<p>If we eliminate those four books, we’re left with a set of books that more closely reflects my typical or baseline reading pace. Looking at the remaining 26 books, I was reading at a pace of about 32.9 pages per day, taking about 11 days to finish a book.</p>
<p>Using the pace of 11 days to finish a book, I could create a goal of reading 365/11 = 33.2 books in 2021. Rounding up, I’ll set a goal of 34 books in 2021.</p>
<p>This represents an increase of 13% over my goal last year, which seems pretty reasonable based on this analysis.</p>
</div>
<div id="wrapping-up" class="section level2">
<h2>Wrapping Up</h2>
<p>In 2020, I set a goal to finish 30 books. On December 31st, I finished <em>The Art of Solitude</em> and completed my reading goal. I explored my Goodreads data to summarize my year in books:</p>
<ul>
<li>I read a total of 10,536 pages in 2020, the average length of a book I read in 2020 was 351.2 pages</li>
<li>I read at a pace of 28.9 pages per day</li>
<li>On average it took me about 12.2 days to complete each book</li>
<li>The longest it took me to finish a book was 43 days (<em>Never Let Me Go</em>), my shortest read time was 3 days (<em>The Art of Solitude</em>)</li>
<li>My average rating was 4 stars, the average Goodreads rating of the books I read was 4.1 stars</li>
</ul>
<p>I also used the Goodreads data to set a data-driven reading goal for 2021. I hope to increase my reading by 13% in 2021 by finishing 34 books.</p>
<p>This was a fun way to look back on my year in books for 2020. There are a few aspects of this data that I could look into like the distribution of genres, the text summary of the book, and text reviews from other Goodreads users. That analysis will have to wait for another day!</p>
<p>Happy reading!</p>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>An Introduction to the data.table Package</title>
      <link>/post/an-introduction-to-the-data-table-package/</link>
      <pubDate>October 1, 2020</pubDate>
      
      <guid>/post/an-introduction-to-the-data-table-package/</guid>
      <description>&lt;p&gt;This post was originally meant for the R Users Group at my organization. I thought it would be worthwhile to have it on my blog as well, in case anyone out there is searching for a short introduction to the &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/&#34;&gt;&lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Although the primary data wrangling package I use is &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, it’s worthwhile to explore other packages that do similar data manipulations. The closest “competitor” to the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; is the &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/&#34;&gt;&lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media.giphy.com/media/wRKeX8o1eIxxu/giphy.gif&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Data Wrangling&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Three of the main selling points for using &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/&#34;&gt;&lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; are that it’s&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fast
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/fread.html&#34;&gt;&lt;code&gt;fread&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/fwrite.html&#34;&gt;&lt;code&gt;fwrite&lt;/code&gt;&lt;/a&gt; should be a part of any data scientist’s data file management toolbox&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Concise
&lt;ul&gt;
&lt;li&gt;We’ll go through a few examples using the &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/&#34;&gt;&lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; syntax&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Efficient
&lt;ul&gt;
&lt;li&gt;Works well with large data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are three qualities we look for in data manipulation.&lt;/p&gt;
&lt;p&gt;If you’re frustrated by how verbose data manipulation chains can get using &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; packages, &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/&#34;&gt;&lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; might be right for you.&lt;/p&gt;
&lt;p&gt;Here are the packages we’ll need for this post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(nycflights13)
library(microbenchmark)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data set we’ll be working with in this post comes from the &lt;a href=&#34;https://cran.r-project.org/package=nycflights13&#34;&gt;&lt;strong&gt;&lt;code&gt;nycflights13&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package. It shows on-time data for all flights that departed NYC in 2013.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(flights_dt &amp;lt;- as.data.table(flights)) # convert to a data.table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         year month day dep_time sched_dep_time dep_delay arr_time
##      1: 2013     1   1      517            515         2      830
##      2: 2013     1   1      533            529         4      850
##     ---                                                          
## 336775: 2013     9  30       NA           1159        NA       NA
## 336776: 2013     9  30       NA            840        NA       NA
##         sched_arr_time arr_delay carrier flight tailnum origin dest air_time
##      1:            819        11      UA   1545  N14228    EWR  IAH      227
##      2:            830        20      UA   1714  N24211    LGA  IAH      227
##     ---                                                                     
## 336775:           1344        NA      MQ   3572  N511MQ    LGA  CLE       NA
## 336776:           1020        NA      MQ   3531  N839MQ    LGA  RDU       NA
##         distance hour minute           time_hour
##      1:     1400    5     15 2013-01-01 05:00:00
##      2:     1416    5     29 2013-01-01 05:00:00
##     ---                                         
## 336775:      419   11     59 2013-09-30 11:00:00
## 336776:      431    8     40 2013-09-30 08:00:00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste(dim(flights_dt), c(&amp;quot;rows&amp;quot;, &amp;quot;columns&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;336776 rows&amp;quot; &amp;quot;19 columns&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;demo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Demo&lt;/h2&gt;
&lt;div id=&#34;the-basics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The basics&lt;/h3&gt;
&lt;p&gt;If you’re familiar with &lt;code&gt;SQL&lt;/code&gt;, &lt;code&gt;data.table&lt;/code&gt; syntax should make a good amount of sense. The syntax allows you to do a lot more than the common operations we expect with a base &lt;code&gt;data.frame&lt;/code&gt;. Here is the general form of &lt;code&gt;data.table&lt;/code&gt; syntax:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DT[i, j, by]&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;i&lt;/code&gt;: where (subset) / order by (sort)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;j&lt;/code&gt;: select (grab certain columns) / update (add/modify columns)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;by&lt;/code&gt;: group by&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/700/1*odc8US0simd-2Le1OHwlhw.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Image source: &lt;a href=&#34;https://towardsdatascience.com/blazing-fast-data-wrangling-with-r-data-table-de5045cc4b4d&#34;&gt;Blazing Fast Data Wrangling With R data.table&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To demonstrate, let’s take a look at each of these components.&lt;/p&gt;
&lt;div id=&#34;subset-and-sort-with-i&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Subset and sort with &lt;code&gt;i&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;The first three examples look at using &lt;code&gt;i&lt;/code&gt; to filter/subset your data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# flights departing in January
flights_dt[month == 1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time
##     1: 2013     1   1      517            515         2      830            819
##     2: 2013     1   1      533            529         4      850            830
##    ---                                                                         
## 27003: 2013     1  31       NA           1446        NA       NA           1757
## 27004: 2013     1  31       NA            625        NA       NA            934
##        arr_delay carrier flight tailnum origin dest air_time distance hour
##     1:        11      UA   1545  N14228    EWR  IAH      227     1400    5
##     2:        20      UA   1714  N24211    LGA  IAH      227     1416    5
##    ---                                                                    
## 27003:        NA      UA    337    &amp;lt;NA&amp;gt;    LGA  IAH       NA     1416   14
## 27004:        NA      UA   1497    &amp;lt;NA&amp;gt;    LGA  IAH       NA     1416    6
##        minute           time_hour
##     1:     15 2013-01-01 05:00:00
##     2:     29 2013-01-01 05:00:00
##    ---                           
## 27003:     46 2013-01-31 14:00:00
## 27004:     25 2013-01-31 06:00:00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# flights departing on March 10th
flights_dt[month == 3 &amp;amp; day == 10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time
##   1: 2013     3  10        6           2359         7      336            338
##   2: 2013     3  10       41           2100       221      230           2257
##  ---                                                                         
## 907: 2013     3  10       NA           2000        NA       NA           2335
## 908: 2013     3  10       NA           1730        NA       NA           1923
##      arr_delay carrier flight tailnum origin dest air_time distance hour minute
##   1:        -2      B6    727  N547JB    JFK  BQN      186     1576   23     59
##   2:       213      EV   4368  N14116    EWR  DAY       82      533   21      0
##  ---                                                                           
## 907:        NA      UA    424    &amp;lt;NA&amp;gt;    EWR  SAT       NA     1569   20      0
## 908:        NA      US    449    &amp;lt;NA&amp;gt;    EWR  CLT       NA      529   17     30
##                time_hour
##   1: 2013-03-10 23:00:00
##   2: 2013-03-10 21:00:00
##  ---                    
## 907: 2013-03-10 20:00:00
## 908: 2013-03-10 17:00:00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# flights where the total delay (dep_delay + arr_delay) is 10 minutes or more,
# the destination was Dallas (DFW) and was in January, February, or March
flights_dt[(dep_delay + arr_delay) &amp;gt;= 10 &amp;amp; dest == &amp;quot;DFW&amp;quot; &amp;amp; month &amp;lt;= 3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time
##   1: 2013     1   1      559            600        -1      941            910
##   2: 2013     1   1      635            635         0     1028            940
##  ---                                                                         
## 574: 2013     3  31     1308           1300         8     1622           1605
## 575: 2013     3  31     1923           1825        58     2202           2127
##      arr_delay carrier flight tailnum origin dest air_time distance hour minute
##   1:        31      AA    707  N3DUAA    LGA  DFW      257     1389    6      0
##   2:        48      AA    711  N3GKAA    LGA  DFW      248     1389    6     35
##  ---                                                                           
## 574:        17      AA    745  N3CAAA    LGA  DFW      218     1389   13      0
## 575:        35      UA   1221  N35271    EWR  DFW      196     1372   18     25
##                time_hour
##   1: 2013-01-01 06:00:00
##   2: 2013-01-01 06:00:00
##  ---                    
## 574: 2013-03-31 13:00:00
## 575: 2013-03-31 18:00:00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also sort using &lt;code&gt;i&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sort by total delay
flights_dt[order(dep_delay + arr_delay, decreasing = T)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         year month day dep_time sched_dep_time dep_delay arr_time
##      1: 2013     1   9      641            900      1301     1242
##      2: 2013     6  15     1432           1935      1137     1607
##     ---                                                          
## 336775: 2013     9  30       NA           1159        NA       NA
## 336776: 2013     9  30       NA            840        NA       NA
##         sched_arr_time arr_delay carrier flight tailnum origin dest air_time
##      1:           1530      1272      HA     51  N384HA    JFK  HNL      640
##      2:           2120      1127      MQ   3535  N504MQ    JFK  CMH       74
##     ---                                                                     
## 336775:           1344        NA      MQ   3572  N511MQ    LGA  CLE       NA
## 336776:           1020        NA      MQ   3531  N839MQ    LGA  RDU       NA
##         distance hour minute           time_hour
##      1:     4983    9      0 2013-01-09 09:00:00
##      2:      483   19     35 2013-06-15 19:00:00
##     ---                                         
## 336775:      419   11     59 2013-09-30 11:00:00
## 336776:      431    8     40 2013-09-30 08:00:00&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;select-and-create-new-columns-using-j&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Select and create new columns using &lt;code&gt;j&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;This first example shows how to select a column. It looks very similar to what we’d do in base R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get flight destination
destination &amp;lt;- flights_dt[, dest]
head(destination)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;IAH&amp;quot; &amp;quot;IAH&amp;quot; &amp;quot;MIA&amp;quot; &amp;quot;BQN&amp;quot; &amp;quot;ATL&amp;quot; &amp;quot;ORD&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll notice that the result of the previous operation was a vector. Sometimes this is what we want, other times it’s not. So, how can we select a column and have a &lt;code&gt;data.table&lt;/code&gt; returned instead of a vector?&lt;/p&gt;
&lt;p&gt;We can use &lt;code&gt;.(column_I_want, another_column_I_want)&lt;/code&gt; or &lt;code&gt;list(this_column_too, and_this_one_also)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# use .(columns_to_select) or list(columns_to_select)
# .(columns_to_select) acts as &amp;quot;shorthand&amp;quot; for list(columns_to_select)
flights_dt[,.(dest)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         dest
##      1:  IAH
##      2:  IAH
##     ---     
## 336775:  CLE
## 336776:  RDU&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we can select multiple columns using .()
flights_dt[,.(year, month, day, origin, dest)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         year month day origin dest
##      1: 2013     1   1    EWR  IAH
##      2: 2013     1   1    LGA  IAH
##     ---                           
## 336775: 2013     9  30    LGA  CLE
## 336776: 2013     9  30    LGA  RDU&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# rename columns
flights_dt[,.(Origin = origin, Destination = dest)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Origin Destination
##      1:    EWR         IAH
##      2:    LGA         IAH
##     ---                   
## 336775:    LGA         CLE
## 336776:    LGA         RDU&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can create columns using &lt;code&gt;:=&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create total delay column
flights_dt[,total_delay := arr_delay + dep_delay]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/l2JIk0sWj9sUvbvCU/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One major difference between “standard” operations in R and some operations in &lt;code&gt;data.table&lt;/code&gt; is that &lt;code&gt;data.table&lt;/code&gt; will make &lt;a href=&#34;https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reference-semantics.html#section-2&#34;&gt;&lt;strong&gt;modifications in place&lt;/strong&gt;&lt;/a&gt;, meaning we don’t have to use the assignment operator (&lt;code&gt;&amp;lt;-&lt;/code&gt; or &lt;code&gt;=&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If we inspect &lt;code&gt;flights_dt&lt;/code&gt;, we can confirm that the &lt;code&gt;total_delay&lt;/code&gt; column was added.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;flights_dt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         year month day dep_time sched_dep_time dep_delay arr_time
##      1: 2013     1   1      517            515         2      830
##      2: 2013     1   1      533            529         4      850
##     ---                                                          
## 336775: 2013     9  30       NA           1159        NA       NA
## 336776: 2013     9  30       NA            840        NA       NA
##         sched_arr_time arr_delay carrier flight tailnum origin dest air_time
##      1:            819        11      UA   1545  N14228    EWR  IAH      227
##      2:            830        20      UA   1714  N24211    LGA  IAH      227
##     ---                                                                     
## 336775:           1344        NA      MQ   3572  N511MQ    LGA  CLE       NA
## 336776:           1020        NA      MQ   3531  N839MQ    LGA  RDU       NA
##         distance hour minute           time_hour total_delay
##      1:     1400    5     15 2013-01-01 05:00:00          13
##      2:     1416    5     29 2013-01-01 05:00:00          24
##     ---                                                     
## 336775:      419   11     59 2013-09-30 11:00:00          NA
## 336776:      431    8     40 2013-09-30 08:00:00          NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can remove a column by setting it &lt;code&gt;:=&lt;/code&gt; to &lt;code&gt;NULL&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove that column
flights_dt[,total_delay:=NULL] 
flights_dt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         year month day dep_time sched_dep_time dep_delay arr_time
##      1: 2013     1   1      517            515         2      830
##      2: 2013     1   1      533            529         4      850
##     ---                                                          
## 336775: 2013     9  30       NA           1159        NA       NA
## 336776: 2013     9  30       NA            840        NA       NA
##         sched_arr_time arr_delay carrier flight tailnum origin dest air_time
##      1:            819        11      UA   1545  N14228    EWR  IAH      227
##      2:            830        20      UA   1714  N24211    LGA  IAH      227
##     ---                                                                     
## 336775:           1344        NA      MQ   3572  N511MQ    LGA  CLE       NA
## 336776:           1020        NA      MQ   3531  N839MQ    LGA  RDU       NA
##         distance hour minute           time_hour
##      1:     1400    5     15 2013-01-01 05:00:00
##      2:     1416    5     29 2013-01-01 05:00:00
##     ---                                         
## 336775:      419   11     59 2013-09-30 11:00:00
## 336776:      431    8     40 2013-09-30 08:00:00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add multiple columns
flights_dt[, `:=`(date = lubridate::ymd(paste(year, month, day, sep = &amp;quot;-&amp;quot;)),
                  log_distance = log(distance),
                  air_time_in_hours = air_time / 60)]
flights_dt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         year month day dep_time sched_dep_time dep_delay arr_time
##      1: 2013     1   1      517            515         2      830
##      2: 2013     1   1      533            529         4      850
##     ---                                                          
## 336775: 2013     9  30       NA           1159        NA       NA
## 336776: 2013     9  30       NA            840        NA       NA
##         sched_arr_time arr_delay carrier flight tailnum origin dest air_time
##      1:            819        11      UA   1545  N14228    EWR  IAH      227
##      2:            830        20      UA   1714  N24211    LGA  IAH      227
##     ---                                                                     
## 336775:           1344        NA      MQ   3572  N511MQ    LGA  CLE       NA
## 336776:           1020        NA      MQ   3531  N839MQ    LGA  RDU       NA
##         distance hour minute           time_hour       date log_distance
##      1:     1400    5     15 2013-01-01 05:00:00 2013-01-01     7.244228
##      2:     1416    5     29 2013-01-01 05:00:00 2013-01-01     7.255591
##     ---                                                                 
## 336775:      419   11     59 2013-09-30 11:00:00 2013-09-30     6.037871
## 336776:      431    8     40 2013-09-30 08:00:00 2013-09-30     6.066108
##         air_time_in_hours
##      1:          3.783333
##      2:          3.783333
##     ---                  
## 336775:                NA
## 336776:                NA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;grouping-with-by&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Grouping with &lt;code&gt;by&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;In the first two examples, we use &lt;code&gt;.N&lt;/code&gt;, which is a &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/special-symbols.html&#34;&gt;special symbol&lt;/a&gt; which allows us to count rows in our data. &lt;code&gt;.SD&lt;/code&gt;, which is used later on in this post, is also a special symbol in &lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A simple example, counting by origin of flight.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;flights_dt[,.N,origin]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    origin      N
## 1:    EWR 120835
## 2:    LGA 104662
## 3:    JFK 111279&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A little more complicated, counting by origin and destination, then sorting to show most frequent, then slice top 10 rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;flights_dt[,.N, .(origin, dest)][order(-N)][1:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     origin dest     N
##  1:    JFK  LAX 11262
##  2:    LGA  ATL 10263
##  3:    LGA  ORD  8857
##  4:    JFK  SFO  8204
##  5:    LGA  CLT  6168
##  6:    EWR  ORD  6100
##  7:    JFK  BOS  5898
##  8:    LGA  MIA  5781
##  9:    JFK  MCO  5464
## 10:    EWR  BOS  5327&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To wrap up this section, let’s show the median and average total delay by origin and destination airport, and then sort by average total delay. We also add in &lt;code&gt;.N&lt;/code&gt;, because it’s always good to show the sample size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;flights_dt[!is.na(arr_delay) &amp;amp; !is.na(dep_delay),
           .(avg_delay = mean(arr_delay + dep_delay),
             median_delay = median(arr_delay + dep_delay),
             .N),
           .(origin, dest)
           ][order(-avg_delay)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      origin dest avg_delay median_delay   N
##   1:    EWR  TYS  82.80511           18 313
##   2:    EWR  CAE  78.94681           40  94
##  ---                                       
## 222:    JFK  PSP -15.66667          -14  18
## 223:    LGA  LEX -31.00000          -31   1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-little-more-advanced&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A little more advanced&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media.giphy.com/media/5MGFEJS7FIxK8/giphy.gif&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Turning it up&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We often want to perform multiple operations on a single &lt;code&gt;data.frame&lt;/code&gt;. If we keep all of the code to perform these operations on a single line, our scripts can become illegible and unwieldy. Similar to how &lt;strong&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/strong&gt; pipes might span multiple lines:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  mutate(new_columns) %&amp;gt;%
  group_by(grouping_columns) %&amp;gt;%
  summarise(other_columns) %&amp;gt;%
  arrange(desc(some_column))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can “chain” &lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt; expressions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DT[ ...
   ][ ...
     ][ ...
       ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This example gets the cumulative total delay over the course of a year by origin airport. It utilizes filtering, sorting, and grouping.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get cumulative delay by origin airport
# uses &amp;quot;chaining&amp;quot;
cumulative_delay_by_origin &amp;lt;-
  flights_dt[!is.na(dep_delay) &amp;amp; !is.na(arr_delay) # keep valid flights
             ][order(time_hour),                   # sort by date 
               .(time_hour,                        # select date and cumsum delay
                 cumu_delay=cumsum(arr_delay+dep_delay)), 
               origin]                             # group by origin airport

ggplot(cumulative_delay_by_origin,
       aes(time_hour, cumu_delay/60, colour = origin))+
  geom_line()+
  theme_bw()+
  xlab(&amp;quot;Date&amp;quot;) + ylab(&amp;quot;Cumulative total delay (hours)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-01-an-introduction-to-the-data-table-package.en_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s get even crazier with chaining.&lt;/p&gt;
&lt;p&gt;The next example finds the “biggest loser” on each day (i.e. which flight had the worst total delay). We then count up (using the &lt;code&gt;ones&lt;/code&gt; column) which origin airport the biggest loser was departing from. We calculate this cumulatively over the course of the year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_delay &amp;lt;- flights_dt[!is.na(arr_delay) &amp;amp; !is.na(dep_delay)
                        ][,`:=`(total_delay=arr_delay+dep_delay, ones=1)
                          ][, .SD[ which.max(total_delay) ], date
                            ][order(date)
                              ][,.(cumu_obs = cumsum(ones), date),.(origin)]

ggplot(top_delay, aes(date, cumu_obs, colour = origin))+
  geom_line()+
  theme_bw()+
  xlab(&amp;quot;Date&amp;quot;)+
  ylab(&amp;quot;Cumulative # of Days with Worst Delay&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-01-an-introduction-to-the-data-table-package.en_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-evaluation-base-r-tidyverse-and-data.table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Performance evaluation: base &lt;strong&gt;&lt;code&gt;R&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/strong&gt;, and &lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Let’s demonstrate a typical calculation you might do in R: an aggregation of two columns based on grouping by three columns. In this specific example, we’re calculating the average departure delay and average arrival delay by origin airport, destination airport, and month of flight.&lt;/p&gt;
&lt;p&gt;We use the &lt;a href=&#34;https://cran.r-project.org/package=microbenchmark&#34;&gt;&lt;strong&gt;&lt;code&gt;microbenchmark&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package to time how long it takes to perform the different operations. We can then take the results and visualize them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1848)
benchmark_data &amp;lt;- microbenchmark(
  # base R solution
  base_R = aggregate(list(flights$dep_delay, flights$arr_delay),
                     list(flights$origin, flights$dest, flights$month),
                     mean, na.rm = TRUE),
  # tidyverse solution
  tidy_verse = flights %&amp;gt;% 
    group_by(origin, dest, month) %&amp;gt;% 
    summarise_at(c(&amp;quot;dep_delay&amp;quot;, &amp;quot;arr_delay&amp;quot;), mean, na.rm = TRUE),
  
  #data.table
  data_table = flights_dt[,lapply(.SD, mean, na.rm = TRUE), 
                          .(origin, dest, month), 
                          .SDcols = c(&amp;quot;dep_delay&amp;quot;, &amp;quot;arr_delay&amp;quot;)],
  
  times = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggridges)
benchmark_data_dt &amp;lt;- as.data.table(benchmark_data)
benchmark_data_dt[,time_in_ms := time / 1000000]

ggplot(benchmark_data_dt, aes(x = time_in_ms, y = expr))+
  geom_density_ridges2(rel_min_height = 0.01, scale = 1.5, fill = &amp;quot;#c5050c&amp;quot;)+
  geom_boxplot(width = 0.25)+
  theme_bw()+
  scale_x_log10(&amp;quot;Time (milliseconds, log10)&amp;quot;)+
  scale_y_discrete(&amp;quot;Operation&amp;quot;,
                   labels = c(&amp;quot;base_R&amp;quot; = &amp;quot;Base R&amp;quot;,
                              &amp;quot;tidy_verse&amp;quot; = &amp;quot;tidyverse&amp;quot;,
                              &amp;quot;data_table&amp;quot; = &amp;quot;data.table&amp;quot;))+
  ggtitle(&amp;quot;Grouped Aggregation Timing Summary&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-01-an-introduction-to-the-data-table-package.en_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is how much slower the median operation is compared to &lt;code&gt;data.table&lt;/code&gt;. Since this &lt;em&gt;is&lt;/em&gt; a tutorial about &lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;, we use its &lt;a href=&#34;https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html&#34;&gt;&lt;code&gt;dcast&lt;/code&gt;&lt;/a&gt; function to convert long data to wide.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dcast(benchmark_data_dt, 
      .~expr, 
      value.var = &amp;quot;time_in_ms&amp;quot;, 
      fun = median)[, round(.SD/data_table,1), 
                    .SDcols = c(&amp;quot;base_R&amp;quot;, &amp;quot;tidy_verse&amp;quot;, &amp;quot;data_table&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    base_R tidy_verse data_table
## 1:   17.2        5.5          1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The short demo above demonstrates just how much more performant using &lt;code&gt;data.table&lt;/code&gt; can be.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://h2oai.github.io/db-benchmark/&#34;&gt;Here’s a more complete reference&lt;/a&gt; on benchmarking, with comparisons across R and Python.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;I hope this was a gentle introduction to the &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/&#34;&gt;&lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package. I think the key to getting off on the right foot with this package is understanding the syntax.&lt;/p&gt;
&lt;p&gt;The syntax allows you to do a lot more than the common operations we expect with a base &lt;code&gt;data.frame&lt;/code&gt;. Here is the general form of &lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt; syntax:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DT[i, j, by]&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;i&lt;/code&gt;: where (subset) / order by (sort)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;j&lt;/code&gt;: select (grab certain columns) / update (add/modify columns)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;by&lt;/code&gt;: group by&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/700/1*odc8US0simd-2Le1OHwlhw.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Image source: &lt;a href=&#34;https://towardsdatascience.com/blazing-fast-data-wrangling-with-r-data-table-de5045cc4b4d&#34;&gt;Blazing Fast Data Wrangling With R data.table&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;By no means did I intend for this introduction to be an exhaustive guide to all things &lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;. If you’re interested in exploring the package further, take a look at these resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html&#34;&gt;Introduction to data.table&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://s3.amazonaws.com/assets.datacamp.com/img/blog/data+table+cheat+sheet.pdf&#34;&gt;&lt;code&gt;data.table&lt;/code&gt; cheat sheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/data.table.html#examples&#34;&gt;&lt;code&gt;data.table&lt;/code&gt; examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://brooksandrew.github.io/simpleblog/articles/advanced-data-table/&#34;&gt;Advanced tips and tricks with data.table&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/blazing-fast-data-wrangling-with-r-data-table-de5045cc4b4d&#34;&gt;Blazing Fast Data Wrangling With R data.table&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://franknarf1.github.io/r-tutorial/_book/tables.html#tables&#34;&gt;Chapter 3: Tables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;postscript-fun-fact&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Postscript: Fun fact&lt;/h2&gt;
&lt;p&gt;Fun &lt;code&gt;R&lt;/code&gt; fact: &lt;code&gt;&amp;lt;-&lt;/code&gt; and &lt;code&gt;=&lt;/code&gt; are actually functions, and can be called like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`&amp;lt;-`(x, 1:5)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`=`(x, 5:1)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5 4 3 2 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`&amp;lt;-`(x, c(rev(x), x))
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 2 3 4 5 5 4 3 2 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`&amp;lt;-`(&amp;quot;ill-advised variable name&amp;quot;, 1:3)
`ill-advised variable name`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`=`(&amp;quot;Christopher Guest Movies&amp;quot;, &amp;quot;awesome&amp;quot;)
`Christopher Guest Movies`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;awesome&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
      <content:encoded><p>This post was originally meant for the R Users Group at my organization. I thought it would be worthwhile to have it on my blog as well, in case anyone out there is searching for a short introduction to the <a href="https://rdatatable.gitlab.io/data.table/"><strong><code>data.table</code></strong></a> package.</p>
<p>Although the primary data wrangling package I use is <a href="https://www.tidyverse.org/"><strong><code>tidyverse</code></strong></a>, it’s worthwhile to explore other packages that do similar data manipulations. The closest “competitor” to the <a href="https://www.tidyverse.org/"><strong><code>tidyverse</code></strong></a> is the <a href="https://rdatatable.gitlab.io/data.table/"><strong><code>data.table</code></strong></a> package.</p>
<div class="figure">
<img src="https://media.giphy.com/media/wRKeX8o1eIxxu/giphy.gif" alt="" />
<p class="caption">Data Wrangling</p>
</div>
<p>Three of the main selling points for using <a href="https://rdatatable.gitlab.io/data.table/"><strong><code>data.table</code></strong></a> are that it’s</p>
<ul>
<li>Fast
<ul>
<li><a href="https://rdatatable.gitlab.io/data.table/reference/fread.html"><code>fread</code></a> and <a href="https://rdatatable.gitlab.io/data.table/reference/fwrite.html"><code>fwrite</code></a> should be a part of any data scientist’s data file management toolbox</li>
</ul></li>
<li>Concise
<ul>
<li>We’ll go through a few examples using the <a href="https://rdatatable.gitlab.io/data.table/"><strong><code>data.table</code></strong></a> syntax</li>
</ul></li>
<li>Efficient
<ul>
<li>Works well with large data</li>
</ul></li>
</ul>
<p>These are three qualities we look for in data manipulation.</p>
<p>If you’re frustrated by how verbose data manipulation chains can get using <a href="https://www.tidyverse.org/"><strong><code>tidyverse</code></strong></a> packages, <a href="https://rdatatable.gitlab.io/data.table/"><strong><code>data.table</code></strong></a> might be right for you.</p>
<p>Here are the packages we’ll need for this post.</p>
<pre class="r"><code>library(data.table)
library(nycflights13)
library(microbenchmark)
library(tidyverse)</code></pre>
<p>The data set we’ll be working with in this post comes from the <a href="https://cran.r-project.org/package=nycflights13"><strong><code>nycflights13</code></strong></a> package. It shows on-time data for all flights that departed NYC in 2013.</p>
<pre class="r"><code>(flights_dt &lt;- as.data.table(flights)) # convert to a data.table</code></pre>
<pre><code>##         year month day dep_time sched_dep_time dep_delay arr_time
##      1: 2013     1   1      517            515         2      830
##      2: 2013     1   1      533            529         4      850
##     ---                                                          
## 336775: 2013     9  30       NA           1159        NA       NA
## 336776: 2013     9  30       NA            840        NA       NA
##         sched_arr_time arr_delay carrier flight tailnum origin dest air_time
##      1:            819        11      UA   1545  N14228    EWR  IAH      227
##      2:            830        20      UA   1714  N24211    LGA  IAH      227
##     ---                                                                     
## 336775:           1344        NA      MQ   3572  N511MQ    LGA  CLE       NA
## 336776:           1020        NA      MQ   3531  N839MQ    LGA  RDU       NA
##         distance hour minute           time_hour
##      1:     1400    5     15 2013-01-01 05:00:00
##      2:     1416    5     29 2013-01-01 05:00:00
##     ---                                         
## 336775:      419   11     59 2013-09-30 11:00:00
## 336776:      431    8     40 2013-09-30 08:00:00</code></pre>
<pre class="r"><code>paste(dim(flights_dt), c(&quot;rows&quot;, &quot;columns&quot;))</code></pre>
<pre><code>## [1] &quot;336776 rows&quot; &quot;19 columns&quot;</code></pre>
<div id="demo" class="section level2">
<h2>Demo</h2>
<div id="the-basics" class="section level3">
<h3>The basics</h3>
<p>If you’re familiar with <code>SQL</code>, <code>data.table</code> syntax should make a good amount of sense. The syntax allows you to do a lot more than the common operations we expect with a base <code>data.frame</code>. Here is the general form of <code>data.table</code> syntax:</p>
<pre class="r"><code>DT[i, j, by]</code></pre>
<ul>
<li><code>i</code>: where (subset) / order by (sort)</li>
<li><code>j</code>: select (grab certain columns) / update (add/modify columns)</li>
<li><code>by</code>: group by</li>
</ul>
<p><img src="https://miro.medium.com/max/700/1*odc8US0simd-2Le1OHwlhw.png" /></p>
<p>Image source: <a href="https://towardsdatascience.com/blazing-fast-data-wrangling-with-r-data-table-de5045cc4b4d">Blazing Fast Data Wrangling With R data.table</a></p>
<p>To demonstrate, let’s take a look at each of these components.</p>
<div id="subset-and-sort-with-i" class="section level4">
<h4>Subset and sort with <code>i</code></h4>
<p>The first three examples look at using <code>i</code> to filter/subset your data.</p>
<pre class="r"><code># flights departing in January
flights_dt[month == 1]</code></pre>
<pre><code>##        year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time
##     1: 2013     1   1      517            515         2      830            819
##     2: 2013     1   1      533            529         4      850            830
##    ---                                                                         
## 27003: 2013     1  31       NA           1446        NA       NA           1757
## 27004: 2013     1  31       NA            625        NA       NA            934
##        arr_delay carrier flight tailnum origin dest air_time distance hour
##     1:        11      UA   1545  N14228    EWR  IAH      227     1400    5
##     2:        20      UA   1714  N24211    LGA  IAH      227     1416    5
##    ---                                                                    
## 27003:        NA      UA    337    &lt;NA&gt;    LGA  IAH       NA     1416   14
## 27004:        NA      UA   1497    &lt;NA&gt;    LGA  IAH       NA     1416    6
##        minute           time_hour
##     1:     15 2013-01-01 05:00:00
##     2:     29 2013-01-01 05:00:00
##    ---                           
## 27003:     46 2013-01-31 14:00:00
## 27004:     25 2013-01-31 06:00:00</code></pre>
<pre class="r"><code># flights departing on March 10th
flights_dt[month == 3 &amp; day == 10]</code></pre>
<pre><code>##      year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time
##   1: 2013     3  10        6           2359         7      336            338
##   2: 2013     3  10       41           2100       221      230           2257
##  ---                                                                         
## 907: 2013     3  10       NA           2000        NA       NA           2335
## 908: 2013     3  10       NA           1730        NA       NA           1923
##      arr_delay carrier flight tailnum origin dest air_time distance hour minute
##   1:        -2      B6    727  N547JB    JFK  BQN      186     1576   23     59
##   2:       213      EV   4368  N14116    EWR  DAY       82      533   21      0
##  ---                                                                           
## 907:        NA      UA    424    &lt;NA&gt;    EWR  SAT       NA     1569   20      0
## 908:        NA      US    449    &lt;NA&gt;    EWR  CLT       NA      529   17     30
##                time_hour
##   1: 2013-03-10 23:00:00
##   2: 2013-03-10 21:00:00
##  ---                    
## 907: 2013-03-10 20:00:00
## 908: 2013-03-10 17:00:00</code></pre>
<pre class="r"><code># flights where the total delay (dep_delay + arr_delay) is 10 minutes or more,
# the destination was Dallas (DFW) and was in January, February, or March
flights_dt[(dep_delay + arr_delay) &gt;= 10 &amp; dest == &quot;DFW&quot; &amp; month &lt;= 3]</code></pre>
<pre><code>##      year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time
##   1: 2013     1   1      559            600        -1      941            910
##   2: 2013     1   1      635            635         0     1028            940
##  ---                                                                         
## 574: 2013     3  31     1308           1300         8     1622           1605
## 575: 2013     3  31     1923           1825        58     2202           2127
##      arr_delay carrier flight tailnum origin dest air_time distance hour minute
##   1:        31      AA    707  N3DUAA    LGA  DFW      257     1389    6      0
##   2:        48      AA    711  N3GKAA    LGA  DFW      248     1389    6     35
##  ---                                                                           
## 574:        17      AA    745  N3CAAA    LGA  DFW      218     1389   13      0
## 575:        35      UA   1221  N35271    EWR  DFW      196     1372   18     25
##                time_hour
##   1: 2013-01-01 06:00:00
##   2: 2013-01-01 06:00:00
##  ---                    
## 574: 2013-03-31 13:00:00
## 575: 2013-03-31 18:00:00</code></pre>
<p>We can also sort using <code>i</code>:</p>
<pre class="r"><code># sort by total delay
flights_dt[order(dep_delay + arr_delay, decreasing = T)]</code></pre>
<pre><code>##         year month day dep_time sched_dep_time dep_delay arr_time
##      1: 2013     1   9      641            900      1301     1242
##      2: 2013     6  15     1432           1935      1137     1607
##     ---                                                          
## 336775: 2013     9  30       NA           1159        NA       NA
## 336776: 2013     9  30       NA            840        NA       NA
##         sched_arr_time arr_delay carrier flight tailnum origin dest air_time
##      1:           1530      1272      HA     51  N384HA    JFK  HNL      640
##      2:           2120      1127      MQ   3535  N504MQ    JFK  CMH       74
##     ---                                                                     
## 336775:           1344        NA      MQ   3572  N511MQ    LGA  CLE       NA
## 336776:           1020        NA      MQ   3531  N839MQ    LGA  RDU       NA
##         distance hour minute           time_hour
##      1:     4983    9      0 2013-01-09 09:00:00
##      2:      483   19     35 2013-06-15 19:00:00
##     ---                                         
## 336775:      419   11     59 2013-09-30 11:00:00
## 336776:      431    8     40 2013-09-30 08:00:00</code></pre>
</div>
<div id="select-and-create-new-columns-using-j" class="section level4">
<h4>Select and create new columns using <code>j</code></h4>
<p>This first example shows how to select a column. It looks very similar to what we’d do in base R.</p>
<pre class="r"><code># get flight destination
destination &lt;- flights_dt[, dest]
head(destination)</code></pre>
<pre><code>## [1] &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; &quot;ATL&quot; &quot;ORD&quot;</code></pre>
<p>You’ll notice that the result of the previous operation was a vector. Sometimes this is what we want, other times it’s not. So, how can we select a column and have a <code>data.table</code> returned instead of a vector?</p>
<p>We can use <code>.(column_I_want, another_column_I_want)</code> or <code>list(this_column_too, and_this_one_also)</code>.</p>
<pre class="r"><code># use .(columns_to_select) or list(columns_to_select)
# .(columns_to_select) acts as &quot;shorthand&quot; for list(columns_to_select)
flights_dt[,.(dest)]</code></pre>
<pre><code>##         dest
##      1:  IAH
##      2:  IAH
##     ---     
## 336775:  CLE
## 336776:  RDU</code></pre>
<pre class="r"><code># we can select multiple columns using .()
flights_dt[,.(year, month, day, origin, dest)]</code></pre>
<pre><code>##         year month day origin dest
##      1: 2013     1   1    EWR  IAH
##      2: 2013     1   1    LGA  IAH
##     ---                           
## 336775: 2013     9  30    LGA  CLE
## 336776: 2013     9  30    LGA  RDU</code></pre>
<pre class="r"><code># rename columns
flights_dt[,.(Origin = origin, Destination = dest)]</code></pre>
<pre><code>##         Origin Destination
##      1:    EWR         IAH
##      2:    LGA         IAH
##     ---                   
## 336775:    LGA         CLE
## 336776:    LGA         RDU</code></pre>
<p>We can create columns using <code>:=</code>:</p>
<pre class="r"><code># create total delay column
flights_dt[,total_delay := arr_delay + dep_delay]</code></pre>
<p><img src="https://media.giphy.com/media/l2JIk0sWj9sUvbvCU/giphy.gif" /></p>
<p>One major difference between “standard” operations in R and some operations in <code>data.table</code> is that <code>data.table</code> will make <a href="https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reference-semantics.html#section-2"><strong>modifications in place</strong></a>, meaning we don’t have to use the assignment operator (<code>&lt;-</code> or <code>=</code>).</p>
<p>If we inspect <code>flights_dt</code>, we can confirm that the <code>total_delay</code> column was added.</p>
<pre class="r"><code>flights_dt</code></pre>
<pre><code>##         year month day dep_time sched_dep_time dep_delay arr_time
##      1: 2013     1   1      517            515         2      830
##      2: 2013     1   1      533            529         4      850
##     ---                                                          
## 336775: 2013     9  30       NA           1159        NA       NA
## 336776: 2013     9  30       NA            840        NA       NA
##         sched_arr_time arr_delay carrier flight tailnum origin dest air_time
##      1:            819        11      UA   1545  N14228    EWR  IAH      227
##      2:            830        20      UA   1714  N24211    LGA  IAH      227
##     ---                                                                     
## 336775:           1344        NA      MQ   3572  N511MQ    LGA  CLE       NA
## 336776:           1020        NA      MQ   3531  N839MQ    LGA  RDU       NA
##         distance hour minute           time_hour total_delay
##      1:     1400    5     15 2013-01-01 05:00:00          13
##      2:     1416    5     29 2013-01-01 05:00:00          24
##     ---                                                     
## 336775:      419   11     59 2013-09-30 11:00:00          NA
## 336776:      431    8     40 2013-09-30 08:00:00          NA</code></pre>
<p>We can remove a column by setting it <code>:=</code> to <code>NULL</code>.</p>
<pre class="r"><code># remove that column
flights_dt[,total_delay:=NULL] 
flights_dt</code></pre>
<pre><code>##         year month day dep_time sched_dep_time dep_delay arr_time
##      1: 2013     1   1      517            515         2      830
##      2: 2013     1   1      533            529         4      850
##     ---                                                          
## 336775: 2013     9  30       NA           1159        NA       NA
## 336776: 2013     9  30       NA            840        NA       NA
##         sched_arr_time arr_delay carrier flight tailnum origin dest air_time
##      1:            819        11      UA   1545  N14228    EWR  IAH      227
##      2:            830        20      UA   1714  N24211    LGA  IAH      227
##     ---                                                                     
## 336775:           1344        NA      MQ   3572  N511MQ    LGA  CLE       NA
## 336776:           1020        NA      MQ   3531  N839MQ    LGA  RDU       NA
##         distance hour minute           time_hour
##      1:     1400    5     15 2013-01-01 05:00:00
##      2:     1416    5     29 2013-01-01 05:00:00
##     ---                                         
## 336775:      419   11     59 2013-09-30 11:00:00
## 336776:      431    8     40 2013-09-30 08:00:00</code></pre>
<pre class="r"><code># add multiple columns
flights_dt[, `:=`(date = lubridate::ymd(paste(year, month, day, sep = &quot;-&quot;)),
                  log_distance = log(distance),
                  air_time_in_hours = air_time / 60)]
flights_dt</code></pre>
<pre><code>##         year month day dep_time sched_dep_time dep_delay arr_time
##      1: 2013     1   1      517            515         2      830
##      2: 2013     1   1      533            529         4      850
##     ---                                                          
## 336775: 2013     9  30       NA           1159        NA       NA
## 336776: 2013     9  30       NA            840        NA       NA
##         sched_arr_time arr_delay carrier flight tailnum origin dest air_time
##      1:            819        11      UA   1545  N14228    EWR  IAH      227
##      2:            830        20      UA   1714  N24211    LGA  IAH      227
##     ---                                                                     
## 336775:           1344        NA      MQ   3572  N511MQ    LGA  CLE       NA
## 336776:           1020        NA      MQ   3531  N839MQ    LGA  RDU       NA
##         distance hour minute           time_hour       date log_distance
##      1:     1400    5     15 2013-01-01 05:00:00 2013-01-01     7.244228
##      2:     1416    5     29 2013-01-01 05:00:00 2013-01-01     7.255591
##     ---                                                                 
## 336775:      419   11     59 2013-09-30 11:00:00 2013-09-30     6.037871
## 336776:      431    8     40 2013-09-30 08:00:00 2013-09-30     6.066108
##         air_time_in_hours
##      1:          3.783333
##      2:          3.783333
##     ---                  
## 336775:                NA
## 336776:                NA</code></pre>
</div>
<div id="grouping-with-by" class="section level4">
<h4>Grouping with <code>by</code></h4>
<p>In the first two examples, we use <code>.N</code>, which is a <a href="https://rdatatable.gitlab.io/data.table/reference/special-symbols.html">special symbol</a> which allows us to count rows in our data. <code>.SD</code>, which is used later on in this post, is also a special symbol in <strong><code>data.table</code></strong>.</p>
<p>A simple example, counting by origin of flight.</p>
<pre class="r"><code>flights_dt[,.N,origin]</code></pre>
<pre><code>##    origin      N
## 1:    EWR 120835
## 2:    LGA 104662
## 3:    JFK 111279</code></pre>
<p>A little more complicated, counting by origin and destination, then sorting to show most frequent, then slice top 10 rows.</p>
<pre class="r"><code>flights_dt[,.N, .(origin, dest)][order(-N)][1:10]</code></pre>
<pre><code>##     origin dest     N
##  1:    JFK  LAX 11262
##  2:    LGA  ATL 10263
##  3:    LGA  ORD  8857
##  4:    JFK  SFO  8204
##  5:    LGA  CLT  6168
##  6:    EWR  ORD  6100
##  7:    JFK  BOS  5898
##  8:    LGA  MIA  5781
##  9:    JFK  MCO  5464
## 10:    EWR  BOS  5327</code></pre>
<p>To wrap up this section, let’s show the median and average total delay by origin and destination airport, and then sort by average total delay. We also add in <code>.N</code>, because it’s always good to show the sample size.</p>
<pre class="r"><code>flights_dt[!is.na(arr_delay) &amp; !is.na(dep_delay),
           .(avg_delay = mean(arr_delay + dep_delay),
             median_delay = median(arr_delay + dep_delay),
             .N),
           .(origin, dest)
           ][order(-avg_delay)]</code></pre>
<pre><code>##      origin dest avg_delay median_delay   N
##   1:    EWR  TYS  82.80511           18 313
##   2:    EWR  CAE  78.94681           40  94
##  ---                                       
## 222:    JFK  PSP -15.66667          -14  18
## 223:    LGA  LEX -31.00000          -31   1</code></pre>
</div>
</div>
<div id="a-little-more-advanced" class="section level3">
<h3>A little more advanced</h3>
<div class="figure">
<img src="https://media.giphy.com/media/5MGFEJS7FIxK8/giphy.gif" alt="" />
<p class="caption">Turning it up</p>
</div>
<p>We often want to perform multiple operations on a single <code>data.frame</code>. If we keep all of the code to perform these operations on a single line, our scripts can become illegible and unwieldy. Similar to how <strong><code>tidyverse</code></strong> pipes might span multiple lines:</p>
<pre class="r"><code>data %&gt;%
  mutate(new_columns) %&gt;%
  group_by(grouping_columns) %&gt;%
  summarise(other_columns) %&gt;%
  arrange(desc(some_column))</code></pre>
<p>We can “chain” <strong><code>data.table</code></strong> expressions:</p>
<pre class="r"><code>DT[ ...
   ][ ...
     ][ ...
       ]</code></pre>
<p>This example gets the cumulative total delay over the course of a year by origin airport. It utilizes filtering, sorting, and grouping.</p>
<pre class="r"><code># get cumulative delay by origin airport
# uses &quot;chaining&quot;
cumulative_delay_by_origin &lt;-
  flights_dt[!is.na(dep_delay) &amp; !is.na(arr_delay) # keep valid flights
             ][order(time_hour),                   # sort by date 
               .(time_hour,                        # select date and cumsum delay
                 cumu_delay=cumsum(arr_delay+dep_delay)), 
               origin]                             # group by origin airport

ggplot(cumulative_delay_by_origin,
       aes(time_hour, cumu_delay/60, colour = origin))+
  geom_line()+
  theme_bw()+
  xlab(&quot;Date&quot;) + ylab(&quot;Cumulative total delay (hours)&quot;)</code></pre>
<p><img src="/post/2020-10-01-an-introduction-to-the-data-table-package.en_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Let’s get even crazier with chaining.</p>
<p>The next example finds the “biggest loser” on each day (i.e. which flight had the worst total delay). We then count up (using the <code>ones</code> column) which origin airport the biggest loser was departing from. We calculate this cumulatively over the course of the year.</p>
<pre class="r"><code>top_delay &lt;- flights_dt[!is.na(arr_delay) &amp; !is.na(dep_delay)
                        ][,`:=`(total_delay=arr_delay+dep_delay, ones=1)
                          ][, .SD[ which.max(total_delay) ], date
                            ][order(date)
                              ][,.(cumu_obs = cumsum(ones), date),.(origin)]

ggplot(top_delay, aes(date, cumu_obs, colour = origin))+
  geom_line()+
  theme_bw()+
  xlab(&quot;Date&quot;)+
  ylab(&quot;Cumulative # of Days with Worst Delay&quot;)</code></pre>
<p><img src="/post/2020-10-01-an-introduction-to-the-data-table-package.en_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="performance-evaluation-base-r-tidyverse-and-data.table" class="section level3">
<h3>Performance evaluation: base <strong><code>R</code></strong>, <strong><code>tidyverse</code></strong>, and <strong><code>data.table</code></strong></h3>
<p>Let’s demonstrate a typical calculation you might do in R: an aggregation of two columns based on grouping by three columns. In this specific example, we’re calculating the average departure delay and average arrival delay by origin airport, destination airport, and month of flight.</p>
<p>We use the <a href="https://cran.r-project.org/package=microbenchmark"><strong><code>microbenchmark</code></strong></a> package to time how long it takes to perform the different operations. We can then take the results and visualize them.</p>
<pre class="r"><code>set.seed(1848)
benchmark_data &lt;- microbenchmark(
  # base R solution
  base_R = aggregate(list(flights$dep_delay, flights$arr_delay),
                     list(flights$origin, flights$dest, flights$month),
                     mean, na.rm = TRUE),
  # tidyverse solution
  tidy_verse = flights %&gt;% 
    group_by(origin, dest, month) %&gt;% 
    summarise_at(c(&quot;dep_delay&quot;, &quot;arr_delay&quot;), mean, na.rm = TRUE),
  
  #data.table
  data_table = flights_dt[,lapply(.SD, mean, na.rm = TRUE), 
                          .(origin, dest, month), 
                          .SDcols = c(&quot;dep_delay&quot;, &quot;arr_delay&quot;)],
  
  times = 100)</code></pre>
<pre class="r"><code>library(ggridges)
benchmark_data_dt &lt;- as.data.table(benchmark_data)
benchmark_data_dt[,time_in_ms := time / 1000000]

ggplot(benchmark_data_dt, aes(x = time_in_ms, y = expr))+
  geom_density_ridges2(rel_min_height = 0.01, scale = 1.5, fill = &quot;#c5050c&quot;)+
  geom_boxplot(width = 0.25)+
  theme_bw()+
  scale_x_log10(&quot;Time (milliseconds, log10)&quot;)+
  scale_y_discrete(&quot;Operation&quot;,
                   labels = c(&quot;base_R&quot; = &quot;Base R&quot;,
                              &quot;tidy_verse&quot; = &quot;tidyverse&quot;,
                              &quot;data_table&quot; = &quot;data.table&quot;))+
  ggtitle(&quot;Grouped Aggregation Timing Summary&quot;)</code></pre>
<p><img src="/post/2020-10-01-an-introduction-to-the-data-table-package.en_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Here is how much slower the median operation is compared to <code>data.table</code>. Since this <em>is</em> a tutorial about <strong><code>data.table</code></strong>, we use its <a href="https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html"><code>dcast</code></a> function to convert long data to wide.</p>
<pre class="r"><code>dcast(benchmark_data_dt, 
      .~expr, 
      value.var = &quot;time_in_ms&quot;, 
      fun = median)[, round(.SD/data_table,1), 
                    .SDcols = c(&quot;base_R&quot;, &quot;tidy_verse&quot;, &quot;data_table&quot;)]</code></pre>
<pre><code>##    base_R tidy_verse data_table
## 1:   17.2        5.5          1</code></pre>
<p>The short demo above demonstrates just how much more performant using <code>data.table</code> can be.</p>
<p><a href="https://h2oai.github.io/db-benchmark/">Here’s a more complete reference</a> on benchmarking, with comparisons across R and Python.</p>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>I hope this was a gentle introduction to the <a href="https://rdatatable.gitlab.io/data.table/"><strong><code>data.table</code></strong></a> package. I think the key to getting off on the right foot with this package is understanding the syntax.</p>
<p>The syntax allows you to do a lot more than the common operations we expect with a base <code>data.frame</code>. Here is the general form of <strong><code>data.table</code></strong> syntax:</p>
<pre class="r"><code>DT[i, j, by]</code></pre>
<ul>
<li><code>i</code>: where (subset) / order by (sort)</li>
<li><code>j</code>: select (grab certain columns) / update (add/modify columns)</li>
<li><code>by</code>: group by</li>
</ul>
<p><img src="https://miro.medium.com/max/700/1*odc8US0simd-2Le1OHwlhw.png" /></p>
<p>Image source: <a href="https://towardsdatascience.com/blazing-fast-data-wrangling-with-r-data-table-de5045cc4b4d">Blazing Fast Data Wrangling With R data.table</a></p>
<p>By no means did I intend for this introduction to be an exhaustive guide to all things <strong><code>data.table</code></strong>. If you’re interested in exploring the package further, take a look at these resources:</p>
<ul>
<li><a href="https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html">Introduction to data.table</a></li>
<li><a href="https://s3.amazonaws.com/assets.datacamp.com/img/blog/data+table+cheat+sheet.pdf"><code>data.table</code> cheat sheet</a></li>
<li><a href="https://rdatatable.gitlab.io/data.table/reference/data.table.html#examples"><code>data.table</code> examples</a></li>
<li><a href="https://brooksandrew.github.io/simpleblog/articles/advanced-data-table/">Advanced tips and tricks with data.table</a></li>
<li><a href="https://towardsdatascience.com/blazing-fast-data-wrangling-with-r-data-table-de5045cc4b4d">Blazing Fast Data Wrangling With R data.table</a></li>
<li><a href="https://franknarf1.github.io/r-tutorial/_book/tables.html#tables">Chapter 3: Tables</a></li>
</ul>
</div>
<div id="postscript-fun-fact" class="section level2">
<h2>Postscript: Fun fact</h2>
<p>Fun <code>R</code> fact: <code>&lt;-</code> and <code>=</code> are actually functions, and can be called like so:</p>
<pre class="r"><code>`&lt;-`(x, 1:5)
x</code></pre>
<pre><code>## [1] 1 2 3 4 5</code></pre>
<pre class="r"><code>`=`(x, 5:1)
x</code></pre>
<pre><code>## [1] 5 4 3 2 1</code></pre>
<pre class="r"><code>`&lt;-`(x, c(rev(x), x))
x</code></pre>
<pre><code>##  [1] 1 2 3 4 5 5 4 3 2 1</code></pre>
<pre class="r"><code>`&lt;-`(&quot;ill-advised variable name&quot;, 1:3)
`ill-advised variable name`</code></pre>
<pre><code>## [1] 1 2 3</code></pre>
<pre class="r"><code>`=`(&quot;Christopher Guest Movies&quot;, &quot;awesome&quot;)
`Christopher Guest Movies`</code></pre>
<pre><code>## [1] &quot;awesome&quot;</code></pre>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>An Introduction to Reading Data into R</title>
      <link>/post/a-brief-overview-on-ways-to-read-data-into-r/</link>
      <pubDate>February 2, 2020</pubDate>
      
      <guid>/post/a-brief-overview-on-ways-to-read-data-into-r/</guid>
      <description>&lt;p&gt;This post was originally meant for the R Users Group at my organization. I thought it would be worthwhile to have it on my blog as well, in case anyone out there is searching for a tutorial on reading data into R.&lt;/p&gt;
&lt;p&gt;There are a lot of different ways to get data into R, and this post highlights a few of the common ways of doing that. This post assumes you have some flat file of data (e.g. csv, txt, excel) you’re trying to read into R. Maybe I’ll write a follow-up post where the data is in a less common format.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Today, we’re going to be taking a quick ride through a few ways to get data from flat files (txt, csv, excel) into R.&lt;/p&gt;
&lt;p&gt;Here are links to the documentation for each of the functions discussed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html&#34;&gt;&lt;code&gt;read.table&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://readr.tidyverse.org/reference/read_delim.html&#34;&gt;&lt;code&gt;readr::read_delim&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/fread.html&#34;&gt;&lt;code&gt;data.table::fread&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://readxl.tidyverse.org/reference/read_excel.html&#34;&gt;&lt;code&gt;readxl::read_excel&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can find all the code and data &lt;a href=&#34;https://github.com/bgstieber/wfaa_rug_2020_01_24&#34;&gt;on my GitHub&lt;/a&gt;. If you &lt;a href=&#34;https://help.github.com/en/desktop/contributing-to-projects/cloning-a-repository-from-github-to-github-desktop&#34;&gt;clone that repository&lt;/a&gt; you should be able to run all of this on your own machine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level2 tabset tabset-pills&#34;&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;div id=&#34;read.csv---basics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;read.csv - basics&lt;/h3&gt;
&lt;p&gt;The first function you probably used to read data into R was &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html&#34;&gt;&lt;code&gt;read.csv&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s suppose you get a basic flat file&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;Sepal.Length&amp;quot;,&amp;quot;Sepal.Width&amp;quot;,&amp;quot;Petal.Length&amp;quot;,&amp;quot;Petal.Width&amp;quot;,&amp;quot;Species&amp;quot;
5.1,3.5,1.4,0.2,&amp;quot;setosa&amp;quot;
4.9,3,1.4,0.2,&amp;quot;setosa&amp;quot;
4.7,3.2,1.3,0.2,&amp;quot;setosa&amp;quot;
4.6,3.1,1.5,0.2,&amp;quot;setosa&amp;quot;
5,3.6,1.4,0.2,&amp;quot;setosa&amp;quot;
5.4,3.9,1.7,0.4,&amp;quot;setosa&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;read.csv&lt;/code&gt; results in:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- read.csv(&amp;quot;data/iris.txt&amp;quot;)
tail(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But wait, something weird happened with &lt;code&gt;Species&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat$Species&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] setosa setosa setosa setosa setosa setosa
## Levels: setosa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;read.csv&lt;/code&gt; has an argument called &lt;code&gt;stringsAsFactors&lt;/code&gt;, and its default is TRUE. This means that any string/character type columns you have in your data will be converted to factors (further reading: &lt;a href=&#34;https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/&#34;&gt;stringsAsFactors: An unauthorized biography&lt;/a&gt;). This is generally not what we want.&lt;/p&gt;
&lt;p&gt;So, in the event that I use &lt;code&gt;read.csv&lt;/code&gt; (I typically prefer &lt;code&gt;readr::read_csv&lt;/code&gt; or &lt;code&gt;data.table::fread&lt;/code&gt;, discussed below), I set &lt;code&gt;stringsAsFactors = FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat2 &amp;lt;- read.csv(&amp;quot;data/iris.txt&amp;quot;, stringsAsFactors = FALSE)
dat2$Species&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;setosa&amp;quot; &amp;quot;setosa&amp;quot; &amp;quot;setosa&amp;quot; &amp;quot;setosa&amp;quot; &amp;quot;setosa&amp;quot; &amp;quot;setosa&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;read.csv&lt;/code&gt; works really well without specifying many arguments when your data is nice. What happens if your data is a little messier?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;read.table---column-headers-are-separated-weird-delimiter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;read.table - Column headers are separated, weird delimiter&lt;/h3&gt;
&lt;p&gt;Here’s the data:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;Sepal.Length&amp;quot;|&amp;quot;Sepal.Width&amp;quot;|&amp;quot;Petal.Length&amp;quot;|&amp;quot;Petal.Width&amp;quot;|&amp;quot;Species&amp;quot;
-----------------------------------------------------------------
----------------------------------------------------------------- 
5.1|3.5|1.4|0.2|&amp;quot;setosa&amp;quot;
4.9|3|1.4|0.2|&amp;quot;setosa&amp;quot;
4.7|3.2|1.3|0.2|&amp;quot;setosa&amp;quot;
4.6|3.1|1.5|0.2|&amp;quot;setosa&amp;quot;
5|3.6|1.4|0.2|&amp;quot;setosa&amp;quot;
5.4|3.9|1.7|0.4|&amp;quot;setosa&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two things are weird with this data&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Column headers are separated from the data with two rows &lt;code&gt;---&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;skip&lt;/code&gt; argument&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A pipe (&lt;code&gt;|&lt;/code&gt;) delimiter is used
&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;sep = &#34;|&#34;&lt;/code&gt; argument&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These two oddities require that we use &lt;code&gt;read.table&lt;/code&gt; instead of &lt;code&gt;read.csv&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first, get column names from first row of data
column_names &amp;lt;- read.table(&amp;quot;data/iris_pipe_delim_edit.txt&amp;quot;,
                           sep = &amp;quot;|&amp;quot;, # pipe delim
                           nrows = 1, # only read first row
                           stringsAsFactors = FALSE,
                           header = FALSE) # no headers
# convert to a character vector
(column_names &amp;lt;- as.character(column_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Sepal.Length&amp;quot; &amp;quot;Sepal.Width&amp;quot;  &amp;quot;Petal.Length&amp;quot; &amp;quot;Petal.Width&amp;quot; 
## [5] &amp;quot;Species&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# then, read in remaining rows, using `skip` argument
full_data &amp;lt;- read.table(&amp;quot;data/iris_pipe_delim_edit.txt&amp;quot;,
                        sep = &amp;quot;|&amp;quot;,
                        skip = 3, # skip first 3 rows
                        stringsAsFactors = FALSE,
                        col.names = column_names) # specify column names

tail(full_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;read.csv---missing-values-are-coded-strangely&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;read.csv - Missing Values are Coded Strangely&lt;/h3&gt;
&lt;p&gt;What if our data has weird missing value codes, maybe due to human input.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;Sepal.Length&amp;quot;,&amp;quot;Sepal.Width&amp;quot;,&amp;quot;Petal.Length&amp;quot;,&amp;quot;Petal.Width&amp;quot;,&amp;quot;Species&amp;quot;
5.1,3.5,1.4,0.2,&amp;quot;setosa&amp;quot;
4.9,3,1.4,0.2,&amp;quot;setosa&amp;quot;
4.7,3.2,NULL,0.2,&amp;quot;setosa&amp;quot;
4.6,3.1,1.5,0.2,&amp;quot;setosa&amp;quot;
5,MISSING,1.4,0.2,&amp;quot;setosa&amp;quot;
5.4,3.9,9999,0.4,&amp;quot;setosa&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, when you’re reading in data with the &lt;code&gt;read.table&lt;/code&gt;/&lt;code&gt;read.csv&lt;/code&gt; family of functions, R treats any columns containing the string &lt;code&gt;&#34;NA&#34;&lt;/code&gt; as an &lt;code&gt;NA&lt;/code&gt; value. Sometimes we have missing values that take other values, like &lt;code&gt;999&lt;/code&gt;, &lt;code&gt;&#34;&#34;&lt;/code&gt;, and &lt;code&gt;NULL&lt;/code&gt;. Using the &lt;code&gt;na.strings&lt;/code&gt; argument can help us with this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_default_NA &amp;lt;- read.csv(&amp;quot;data/iris_weird_NA_edit.txt&amp;quot;, 
                           stringsAsFactors = FALSE)

dat_default_NA$Sepal.Width&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;3.5&amp;quot;     &amp;quot;3&amp;quot;       &amp;quot;3.2&amp;quot;     &amp;quot;3.1&amp;quot;     &amp;quot;MISSING&amp;quot; &amp;quot;3.9&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_default_NA$Petal.Length&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1.4&amp;quot;  &amp;quot;1.4&amp;quot;  &amp;quot;NULL&amp;quot; &amp;quot;1.5&amp;quot;  &amp;quot;1.4&amp;quot;  &amp;quot;9999&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since &lt;code&gt;R&lt;/code&gt; found character values in the &lt;code&gt;Sepal.Width&lt;/code&gt; and &lt;code&gt;Petal.Length&lt;/code&gt; columns, it treats those as characters. We know this is wrong, and can fix it using &lt;code&gt;na.strings&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_default_NA &amp;lt;- read.csv(&amp;quot;data/iris_weird_NA_edit.txt&amp;quot;,
                           stringsAsFactors = FALSE,
                           na.strings = c(&amp;quot;NULL&amp;quot;, &amp;quot;9999&amp;quot;, &amp;quot;MISSING&amp;quot;))

dat_default_NA$Sepal.Width&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.5 3.0 3.2 3.1  NA 3.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_default_NA$Petal.Length&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.4 1.4  NA 1.5 1.4  NA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;readrread_csv&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;readr::read_csv&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next function I want to talk about is &lt;code&gt;read_csv&lt;/code&gt; from the &lt;a href=&#34;https://readr.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;readr&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;This function is really helpful, &lt;strong&gt;and it’s pretty much my go-to function to read in flat files into R&lt;/strong&gt;. It has good and well-reasoned defaults (no &lt;code&gt;stringsAsFactors = FALSE&lt;/code&gt;!), and reads in the data as a &lt;a href=&#34;https://tibble.tidyverse.org/&#34;&gt;&lt;code&gt;tibble&lt;/code&gt;&lt;/a&gt; as opposed to a &lt;code&gt;data.frame&lt;/code&gt;. This makes printing the data to your console a lot better.&lt;/p&gt;
&lt;p&gt;Rather than looking at the boring iris data, we’ll instead read some data from the internet. Yes, if you give one of the &lt;code&gt;read&lt;/code&gt; functions (even &lt;code&gt;read.table&lt;/code&gt;!) a url with a csv/txt file, it will be able to read that into R (conditional on you having a connection to the internet).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;u1 &amp;lt;- &amp;quot;https://raw.githubusercontent.com/fivethirtyeight/data&amp;quot;

u2 &amp;lt;- &amp;quot;/master/college-majors/all-ages.csv&amp;quot;

(u &amp;lt;- paste0(u1, u2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/all-ages.csv&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;college_data &amp;lt;- read_csv(u) # informative parsing printing&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   Major_code = col_double(),
##   Major = col_character(),
##   Major_category = col_character(),
##   Total = col_double(),
##   Employed = col_double(),
##   Employed_full_time_year_round = col_double(),
##   Unemployed = col_double(),
##   Unemployment_rate = col_double(),
##   Median = col_double(),
##   P25th = col_double(),
##   P75th = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;college_data # nice printing of data, don&amp;#39;t need head() or tail()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 173 x 11
##    Major_code Major Major_category  Total Employed Employed_full_t~
##         &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1       1100 GENE~ Agriculture &amp;amp;~ 128148    90245            74078
##  2       1101 AGRI~ Agriculture &amp;amp;~  95326    76865            64240
##  3       1102 AGRI~ Agriculture &amp;amp;~  33955    26321            22810
##  4       1103 ANIM~ Agriculture &amp;amp;~ 103549    81177            64937
##  5       1104 FOOD~ Agriculture &amp;amp;~  24280    17281            12722
##  6       1105 PLAN~ Agriculture &amp;amp;~  79409    63043            51077
##  7       1106 SOIL~ Agriculture &amp;amp;~   6586     4926             4042
##  8       1199 MISC~ Agriculture &amp;amp;~   8549     6392             5074
##  9       1301 ENVI~ Biology &amp;amp; Lif~ 106106    87602            65238
## 10       1302 FORE~ Agriculture &amp;amp;~  69447    48228            39613
## # ... with 163 more rows, and 5 more variables: Unemployed &amp;lt;dbl&amp;gt;,
## #   Unemployment_rate &amp;lt;dbl&amp;gt;, Median &amp;lt;dbl&amp;gt;, P25th &amp;lt;dbl&amp;gt;, P75th &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;read_csv&lt;/code&gt; has a few arguments I should highlight:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;na&lt;/code&gt;&lt;/strong&gt;: Character vector of strings to interpret as missing values. (this is like &lt;code&gt;na.strings&lt;/code&gt; in &lt;code&gt;read.csv&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;skip&lt;/code&gt;&lt;/strong&gt;: Number of lines to skip before reading data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;n_max&lt;/code&gt;&lt;/strong&gt;: Maximum number of records to read.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;col_types&lt;/code&gt;&lt;/strong&gt;: allows you to specify the column types for your data. &lt;strong&gt;I typically leave this as is, and let the parser do its job&lt;/strong&gt;, but it can be helpful if you’re trying to coerce a certain column to certain data type&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data.tablefread&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;data.table::fread&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, sometimes you might be dealing with some really nasty data that is large and unwieldy. &lt;code&gt;read_csv&lt;/code&gt; is good for maybe 80-90% of data files, but sometimes we need something more powerful.&lt;/p&gt;
&lt;p&gt;There is where the &lt;code&gt;fread&lt;/code&gt; function from the &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/&#34;&gt;&lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package comes in handy (further reading: &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Convenience-features-of-fread&#34;&gt;Convenience features of fread&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;By &lt;a href=&#34;https://jozef.io/r917-fread-comparisons/&#34;&gt;some measures&lt;/a&gt;, &lt;code&gt;fread&lt;/code&gt; can be about 6 times faster than &lt;code&gt;read.csv&lt;/code&gt; and about 2.5 times faster than &lt;code&gt;read_csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One of the best parts of &lt;code&gt;fread&lt;/code&gt; is that you do not necessarily have to specify the delimiter in your data.&lt;/p&gt;
&lt;p&gt;For example, the pipe delimited data from above is read in easily. In the code below I set &lt;code&gt;verbose = TRUE&lt;/code&gt; to show the internal output from &lt;code&gt;fread&lt;/code&gt;. In general, I’d recommend you leave this as FALSE, unless you’re in serious debug mode.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pipe_dat &amp;lt;- fread(&amp;quot;data/iris_pipe_delim_edit.txt&amp;quot;,
                  skip = 3, # homework: what happens if you don&amp;#39;t specify skip?
                  verbose = TRUE, # default is FALSE, which I recommend
                  col.names = names(iris))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## omp_get_num_procs()==8
## R_DATATABLE_NUM_PROCS_PERCENT==&amp;quot;&amp;quot; (default 50)
## R_DATATABLE_NUM_THREADS==&amp;quot;&amp;quot;
## omp_get_thread_limit()==2147483647
## omp_get_max_threads()==8
## OMP_THREAD_LIMIT==&amp;quot;&amp;quot;
## OMP_NUM_THREADS==&amp;quot;&amp;quot;
## data.table is using 4 threads. This is set on startup, and by setDTthreads(). See ?setDTthreads.
## RestoreAfterFork==true
## Input contains no \n. Taking this to be a filename to open
## [01] Check arguments
##   Using 4 threads (omp_get_max_threads()=8, nth=4)
##   NAstrings = [&amp;lt;&amp;lt;NA&amp;gt;&amp;gt;]
##   None of the NAstrings look like numbers.
##   skip num lines = 3
##   show progress = 0
##   0/1 column will be read as integer
## [02] Opening the file
##   Opening file data/iris_pipe_delim_edit.txt
##   File opened, size = 356 bytes.
##   Memory mapped ok
## [03] Detect and skip BOM
## [04] Arrange mmap to be \0 terminated
##   \n has been found in the input and different lines can end with different line endings (e.g. mixed \n and \r\n in one file). This is common and ideal.
## [05] Skipping initial rows if needed
##   Skipped to line 4 in the file  Positioned on line 4 starting: &amp;lt;&amp;lt;5.1|3.5|1.4|0.2|&amp;quot;setosa&amp;quot;&amp;gt;&amp;gt;
## [06] Detect separator, quoting rule, and ncolumns
##   Detecting sep automatically ...
##   sep=&amp;#39;|&amp;#39;  with 6 lines of 5 fields using quote rule 0
##   Detected 5 columns on line 4. This line is either column names or first data row. Line starts as: &amp;lt;&amp;lt;5.1|3.5|1.4|0.2|&amp;quot;setosa&amp;quot;&amp;gt;&amp;gt;
##   Quote rule picked = 0
##   fill=false and the most number of columns found is 5
## [07] Detect column types, good nrow estimate and whether first row is column names
##   Number of sampling jump points = 1 because (150 bytes from row 1 to eof) / (2 * 150 jump0size) == 0
##   Type codes (jump 000)    : 7777A  Quote rule 0
##   &amp;#39;header&amp;#39; determined to be false because there are some number columns and those columns do not have a string field at the top of them
##   All rows were sampled since file is small so we know nrow=6 exactly
## [08] Assign column names
## [09] Apply user overrides on column types
##   After 0 type and 0 drop user overrides : 7777A
## [10] Allocate memory for the datatable
##   Allocating 5 column slots (5 - 0 dropped) with 6 rows
## [11] Read the data
##   jumps=[0..1), chunk_size=1048576, total_size=150
## Read 6 rows x 5 columns from 356 bytes file in 00:00.000 wall clock time
## [12] Finalizing the datatable
##   Type counts:
##          4 : float64   &amp;#39;7&amp;#39;
##          1 : string    &amp;#39;A&amp;#39;
## =============================
##    0.000s (  0%) Memory map 0.000GB file
##    0.000s (  0%) sep=&amp;#39;|&amp;#39; ncol=5 and header detection
##    0.000s (  0%) Column type detection using 6 sample rows
##    0.000s (  0%) Allocation of 6 rows x 5 cols (0.000GB) of which 6 (100%) rows used
##    0.000s (  0%) Reading 1 chunks (0 swept) of 1.000MB (each chunk 6 rows) using 1 threads
##    +    0.000s (  0%) Parse to row-major thread buffers (grown 0 times)
##    +    0.000s (  0%) Transpose
##    +    0.000s (  0%) Waiting
##    0.000s (  0%) Rereading 0 columns due to out-of-sample type exceptions
##    0.000s        Total&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pipe_dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1:          5.1         3.5          1.4         0.2  setosa
## 2:          4.9         3.0          1.4         0.2  setosa
## 3:          4.7         3.2          1.3         0.2  setosa
## 4:          4.6         3.1          1.5         0.2  setosa
## 5:          5.0         3.6          1.4         0.2  setosa
## 6:          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a demonstration of how much faster &lt;code&gt;fread&lt;/code&gt; is than &lt;code&gt;read.csv&lt;/code&gt; and &lt;code&gt;read_csv&lt;/code&gt; using a subset of the flights data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;u1 &amp;lt;- &amp;quot;https://github.com/roberthryniewicz/datasets/&amp;quot;
u2 &amp;lt;- &amp;quot;blob/master/airline-dataset/flights/flights.csv?raw=true&amp;quot;
(uu &amp;lt;- paste0(u1, u2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;https://github.com/roberthryniewicz/datasets/blob/master/airline-dataset/flights/flights.csv?raw=true&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(dat_base &amp;lt;- read.csv(uu)) # timing for read.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   15.45    0.15   17.36&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(dat_readr &amp;lt;- read_csv(uu)) # timing for read_csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   .default = col_double(),
##   UniqueCarrier = col_character(),
##   TailNum = col_character(),
##   Origin = col_character(),
##   Dest = col_character(),
##   CancellationCode = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See spec(...) for full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##    0.67    0.18    3.29&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(dat_fread &amp;lt;- fread(uu)) # timing for fread&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##    0.38    0.07    1.89&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(dat_fread) # rows by columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 100000     29&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I would strongly encourage you to spend some time playing around with &lt;code&gt;fread&lt;/code&gt;, and thoroughly investigate its arguments (it’s got a lot!).&lt;/p&gt;
&lt;p&gt;Considering all the benefits of &lt;code&gt;fread&lt;/code&gt;, I’m actually surprised I don’t use it more.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;readxlread_excel&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;readxl::read_excel&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maybe you’ve been unlucky enough to have to do some analysis using an excel file. This used to be a tedious task to get the data into R. Now, we can use the &lt;code&gt;read_excel&lt;/code&gt; function from the &lt;a href=&#34;https://readxl.tidyverse.org/index.html&#34;&gt;&lt;strong&gt;&lt;code&gt;readxl&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Excel files will typically have multiple sheets. The &lt;a href=&#34;https://community.tableau.com/servlet/JiveServlet/downloadBody/1236-102-2-15278/Sample%20-%20Superstore.xls&#34;&gt;excel example&lt;/a&gt; we’re looking at today has three separate sheets.&lt;/p&gt;
&lt;p&gt;Reading these in is straightforward, using the &lt;code&gt;read_excel&lt;/code&gt; function and the &lt;code&gt;sheet&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;orders &amp;lt;- read_excel(&amp;quot;data/Superstore.xls&amp;quot;,
                     sheet = &amp;quot;Orders&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L2236 / R2236C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L5276 / R5276C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L8800 / R8800C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9148 / R9148C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9149 / R9149C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9150 / R9150C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9388 / R9388C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9389 / R9389C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9390 / R9390C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9391 / R9391C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9743 / R9743C12: &amp;#39;05408&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(orders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 21
##   `Row ID` `Order ID` `Order Date`        `Ship Date`         `Ship Mode`
##      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;dttm&amp;gt;              &amp;lt;dttm&amp;gt;              &amp;lt;chr&amp;gt;      
## 1     9989 CA-2017-1~ 2017-11-17 00:00:00 2017-11-21 00:00:00 Standard C~
## 2     9990 CA-2014-1~ 2014-01-21 00:00:00 2014-01-23 00:00:00 Second Cla~
## 3     9991 CA-2017-1~ 2017-02-26 00:00:00 2017-03-03 00:00:00 Standard C~
## 4     9992 CA-2017-1~ 2017-02-26 00:00:00 2017-03-03 00:00:00 Standard C~
## 5     9993 CA-2017-1~ 2017-02-26 00:00:00 2017-03-03 00:00:00 Standard C~
## 6     9994 CA-2017-1~ 2017-05-04 00:00:00 2017-05-09 00:00:00 Second Cla~
## # ... with 16 more variables: `Customer ID` &amp;lt;chr&amp;gt;, `Customer Name` &amp;lt;chr&amp;gt;,
## #   Segment &amp;lt;chr&amp;gt;, Country &amp;lt;chr&amp;gt;, City &amp;lt;chr&amp;gt;, State &amp;lt;chr&amp;gt;, `Postal
## #   Code` &amp;lt;dbl&amp;gt;, Region &amp;lt;chr&amp;gt;, `Product ID` &amp;lt;chr&amp;gt;, Category &amp;lt;chr&amp;gt;,
## #   `Sub-Category` &amp;lt;chr&amp;gt;, `Product Name` &amp;lt;chr&amp;gt;, Sales &amp;lt;dbl&amp;gt;,
## #   Quantity &amp;lt;dbl&amp;gt;, Discount &amp;lt;dbl&amp;gt;, Profit &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;returns &amp;lt;- read_excel(&amp;quot;data/Superstore.xls&amp;quot;,
                      sheet = &amp;quot;Returns&amp;quot;)

tail(returns)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   Returned `Order ID`    
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;         
## 1 Yes      US-2016-140172
## 2 Yes      CA-2015-101910
## 3 Yes      CA-2017-156958
## 4 Yes      CA-2016-105585
## 5 Yes      CA-2016-148796
## 6 Yes      CA-2015-149636&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;people &amp;lt;- read_excel(&amp;quot;data/Superstore.xls&amp;quot;,
                     sheet = &amp;quot;People&amp;quot;)

tail(people)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   Person            Region 
##   &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;  
## 1 Anna Andreadi     West   
## 2 Chuck Magee       East   
## 3 Kelly Williams    Central
## 4 Cassandra Brandow South&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A few things to note about &lt;code&gt;read_excel&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VERY IMPORTANT&lt;/strong&gt; Sometimes the function fails if you have the file open. Make sure the excel file is closed before trying to read it into R!&lt;/li&gt;
&lt;li&gt;The function will inform you of parsing issues/column type coercion.&lt;/li&gt;
&lt;li&gt;By default, the function will return a &lt;code&gt;tibble&lt;/code&gt; and not a &lt;code&gt;data.frame&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;In this post, we’ve looked at a few different ways of getting data into R from flat files. For nice flat files, it’s pretty straightforward to get your data into R. If your data isn’t so nice, you can generally be successful using the &lt;code&gt;fread&lt;/code&gt; or &lt;code&gt;read_csv&lt;/code&gt; functions, but you’ll need to be very aware of the structure of your data, as well as the arguments for whatever function decide to use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Further Reading&lt;/h2&gt;
&lt;p&gt;Here are the links I’ve referenced.&lt;/p&gt;
&lt;div id=&#34;articles&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Articles&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/&#34;&gt;stringsAsFactors: An unauthorized biography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Convenience-features-of-fread&#34;&gt;Convenience features of fread&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jozef.io/r917-fread-comparisons/&#34;&gt;How data.table’s fread can save you a lot of time and memory, and take input from shell commands&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;package-sites&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Package Sites&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://readr.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;readr&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://tibble.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;tibble&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table/&#34;&gt;&lt;strong&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://readxl.tidyverse.org/index.html&#34;&gt;&lt;strong&gt;&lt;code&gt;readxl&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;function-documentation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Function Documentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html&#34;&gt;&lt;code&gt;read.table&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://readr.tidyverse.org/reference/read_delim.html&#34;&gt;&lt;code&gt;readr::read_delim&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/fread.html&#34;&gt;&lt;code&gt;data.table::fread&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://readxl.tidyverse.org/reference/read_excel.html&#34;&gt;&lt;code&gt;readxl::read_excel&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session Information&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.0 (2019-04-26)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18362)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.1252 
## [2] LC_CTYPE=English_United States.1252   
## [3] LC_MONETARY=English_United States.1252
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] readxl_1.3.1      data.table_1.12.2 readr_1.3.1      
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.1       knitr_1.23       magrittr_1.5     hms_0.4.2       
##  [5] R6_2.4.0         rlang_0.4.0      fansi_0.4.0      stringr_1.4.0   
##  [9] tools_3.6.0      xfun_0.8         utf8_1.1.4       cli_1.1.0       
## [13] htmltools_0.3.6  assertthat_0.2.1 yaml_2.2.0       digest_0.6.19   
## [17] tibble_2.1.3     crayon_1.3.4     bookdown_0.11    vctrs_0.1.0     
## [21] zeallot_0.1.0    evaluate_0.14    rmarkdown_1.13   blogdown_0.13   
## [25] stringi_1.4.3    compiler_3.6.0   pillar_1.4.1     cellranger_1.1.0
## [29] backports_1.1.4  pkgconfig_2.0.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
      <content:encoded><p>This post was originally meant for the R Users Group at my organization. I thought it would be worthwhile to have it on my blog as well, in case anyone out there is searching for a tutorial on reading data into R.</p>
<p>There are a lot of different ways to get data into R, and this post highlights a few of the common ways of doing that. This post assumes you have some flat file of data (e.g. csv, txt, excel) you’re trying to read into R. Maybe I’ll write a follow-up post where the data is in a less common format.</p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Today, we’re going to be taking a quick ride through a few ways to get data from flat files (txt, csv, excel) into R.</p>
<p>Here are links to the documentation for each of the functions discussed.</p>
<ul>
<li><a href="https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html"><code>read.table</code></a></li>
<li><a href="https://readr.tidyverse.org/reference/read_delim.html"><code>readr::read_delim</code></a></li>
<li><a href="https://rdatatable.gitlab.io/data.table/reference/fread.html"><code>data.table::fread</code></a></li>
<li><a href="https://readxl.tidyverse.org/reference/read_excel.html"><code>readxl::read_excel</code></a></li>
</ul>
<p>You can find all the code and data <a href="https://github.com/bgstieber/wfaa_rug_2020_01_24">on my GitHub</a>. If you <a href="https://help.github.com/en/desktop/contributing-to-projects/cloning-a-repository-from-github-to-github-desktop">clone that repository</a> you should be able to run all of this on your own machine.</p>
</div>
<div id="examples" class="section level2 tabset tabset-pills">
<h2>Examples</h2>
<div id="read.csv---basics" class="section level3">
<h3>read.csv - basics</h3>
<p>The first function you probably used to read data into R was <a href="https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html"><code>read.csv</code></a>.</p>
<p>Let’s suppose you get a basic flat file</p>
<pre><code>&quot;Sepal.Length&quot;,&quot;Sepal.Width&quot;,&quot;Petal.Length&quot;,&quot;Petal.Width&quot;,&quot;Species&quot;
5.1,3.5,1.4,0.2,&quot;setosa&quot;
4.9,3,1.4,0.2,&quot;setosa&quot;
4.7,3.2,1.3,0.2,&quot;setosa&quot;
4.6,3.1,1.5,0.2,&quot;setosa&quot;
5,3.6,1.4,0.2,&quot;setosa&quot;
5.4,3.9,1.7,0.4,&quot;setosa&quot;</code></pre>
<p>Using <code>read.csv</code> results in:</p>
<pre class="r"><code>dat &lt;- read.csv(&quot;data/iris.txt&quot;)
tail(dat)</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
<p>But wait, something weird happened with <code>Species</code>:</p>
<pre class="r"><code>dat$Species</code></pre>
<pre><code>## [1] setosa setosa setosa setosa setosa setosa
## Levels: setosa</code></pre>
<p><code>read.csv</code> has an argument called <code>stringsAsFactors</code>, and its default is TRUE. This means that any string/character type columns you have in your data will be converted to factors (further reading: <a href="https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/">stringsAsFactors: An unauthorized biography</a>). This is generally not what we want.</p>
<p>So, in the event that I use <code>read.csv</code> (I typically prefer <code>readr::read_csv</code> or <code>data.table::fread</code>, discussed below), I set <code>stringsAsFactors = FALSE</code>.</p>
<pre class="r"><code>dat2 &lt;- read.csv(&quot;data/iris.txt&quot;, stringsAsFactors = FALSE)
dat2$Species</code></pre>
<pre><code>## [1] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot;</code></pre>
<p><code>read.csv</code> works really well without specifying many arguments when your data is nice. What happens if your data is a little messier?</p>
</div>
<div id="read.table---column-headers-are-separated-weird-delimiter" class="section level3">
<h3>read.table - Column headers are separated, weird delimiter</h3>
<p>Here’s the data:</p>
<pre><code>&quot;Sepal.Length&quot;|&quot;Sepal.Width&quot;|&quot;Petal.Length&quot;|&quot;Petal.Width&quot;|&quot;Species&quot;
-----------------------------------------------------------------
----------------------------------------------------------------- 
5.1|3.5|1.4|0.2|&quot;setosa&quot;
4.9|3|1.4|0.2|&quot;setosa&quot;
4.7|3.2|1.3|0.2|&quot;setosa&quot;
4.6|3.1|1.5|0.2|&quot;setosa&quot;
5|3.6|1.4|0.2|&quot;setosa&quot;
5.4|3.9|1.7|0.4|&quot;setosa&quot;</code></pre>
<p>Two things are weird with this data</p>
<ol style="list-style-type: decimal">
<li>Column headers are separated from the data with two rows <code>---</code>
<ul>
<li>Use <code>skip</code> argument</li>
</ul></li>
<li>A pipe (<code>|</code>) delimiter is used
<ul>
<li>Use <code>sep = "|"</code> argument</li>
</ul></li>
</ol>
<p>These two oddities require that we use <code>read.table</code> instead of <code>read.csv</code>.</p>
<pre class="r"><code># first, get column names from first row of data
column_names &lt;- read.table(&quot;data/iris_pipe_delim_edit.txt&quot;,
                           sep = &quot;|&quot;, # pipe delim
                           nrows = 1, # only read first row
                           stringsAsFactors = FALSE,
                           header = FALSE) # no headers
# convert to a character vector
(column_names &lt;- as.character(column_names))</code></pre>
<pre><code>## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot; 
## [5] &quot;Species&quot;</code></pre>
<pre class="r"><code># then, read in remaining rows, using `skip` argument
full_data &lt;- read.table(&quot;data/iris_pipe_delim_edit.txt&quot;,
                        sep = &quot;|&quot;,
                        skip = 3, # skip first 3 rows
                        stringsAsFactors = FALSE,
                        col.names = column_names) # specify column names

tail(full_data)</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
</div>
<div id="read.csv---missing-values-are-coded-strangely" class="section level3">
<h3>read.csv - Missing Values are Coded Strangely</h3>
<p>What if our data has weird missing value codes, maybe due to human input.</p>
<pre><code>&quot;Sepal.Length&quot;,&quot;Sepal.Width&quot;,&quot;Petal.Length&quot;,&quot;Petal.Width&quot;,&quot;Species&quot;
5.1,3.5,1.4,0.2,&quot;setosa&quot;
4.9,3,1.4,0.2,&quot;setosa&quot;
4.7,3.2,NULL,0.2,&quot;setosa&quot;
4.6,3.1,1.5,0.2,&quot;setosa&quot;
5,MISSING,1.4,0.2,&quot;setosa&quot;
5.4,3.9,9999,0.4,&quot;setosa&quot;</code></pre>
<p>By default, when you’re reading in data with the <code>read.table</code>/<code>read.csv</code> family of functions, R treats any columns containing the string <code>"NA"</code> as an <code>NA</code> value. Sometimes we have missing values that take other values, like <code>999</code>, <code>""</code>, and <code>NULL</code>. Using the <code>na.strings</code> argument can help us with this.</p>
<pre class="r"><code>dat_default_NA &lt;- read.csv(&quot;data/iris_weird_NA_edit.txt&quot;, 
                           stringsAsFactors = FALSE)

dat_default_NA$Sepal.Width</code></pre>
<pre><code>## [1] &quot;3.5&quot;     &quot;3&quot;       &quot;3.2&quot;     &quot;3.1&quot;     &quot;MISSING&quot; &quot;3.9&quot;</code></pre>
<pre class="r"><code>dat_default_NA$Petal.Length</code></pre>
<pre><code>## [1] &quot;1.4&quot;  &quot;1.4&quot;  &quot;NULL&quot; &quot;1.5&quot;  &quot;1.4&quot;  &quot;9999&quot;</code></pre>
<p>Since <code>R</code> found character values in the <code>Sepal.Width</code> and <code>Petal.Length</code> columns, it treats those as characters. We know this is wrong, and can fix it using <code>na.strings</code>.</p>
<pre class="r"><code>dat_default_NA &lt;- read.csv(&quot;data/iris_weird_NA_edit.txt&quot;,
                           stringsAsFactors = FALSE,
                           na.strings = c(&quot;NULL&quot;, &quot;9999&quot;, &quot;MISSING&quot;))

dat_default_NA$Sepal.Width</code></pre>
<pre><code>## [1] 3.5 3.0 3.2 3.1  NA 3.9</code></pre>
<pre class="r"><code>dat_default_NA$Petal.Length</code></pre>
<pre><code>## [1] 1.4 1.4  NA 1.5 1.4  NA</code></pre>
</div>
<div id="readrread_csv" class="section level3">
<h3>readr::read_csv</h3>
<pre class="r"><code>library(readr)</code></pre>
<p>The next function I want to talk about is <code>read_csv</code> from the <a href="https://readr.tidyverse.org/"><strong><code>readr</code></strong></a> package.</p>
<p>This function is really helpful, <strong>and it’s pretty much my go-to function to read in flat files into R</strong>. It has good and well-reasoned defaults (no <code>stringsAsFactors = FALSE</code>!), and reads in the data as a <a href="https://tibble.tidyverse.org/"><code>tibble</code></a> as opposed to a <code>data.frame</code>. This makes printing the data to your console a lot better.</p>
<p>Rather than looking at the boring iris data, we’ll instead read some data from the internet. Yes, if you give one of the <code>read</code> functions (even <code>read.table</code>!) a url with a csv/txt file, it will be able to read that into R (conditional on you having a connection to the internet).</p>
<pre class="r"><code>u1 &lt;- &quot;https://raw.githubusercontent.com/fivethirtyeight/data&quot;

u2 &lt;- &quot;/master/college-majors/all-ages.csv&quot;

(u &lt;- paste0(u1, u2))</code></pre>
<pre><code>## [1] &quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/all-ages.csv&quot;</code></pre>
<pre class="r"><code>college_data &lt;- read_csv(u) # informative parsing printing</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   Major_code = col_double(),
##   Major = col_character(),
##   Major_category = col_character(),
##   Total = col_double(),
##   Employed = col_double(),
##   Employed_full_time_year_round = col_double(),
##   Unemployed = col_double(),
##   Unemployment_rate = col_double(),
##   Median = col_double(),
##   P25th = col_double(),
##   P75th = col_double()
## )</code></pre>
<pre class="r"><code>college_data # nice printing of data, don&#39;t need head() or tail()</code></pre>
<pre><code>## # A tibble: 173 x 11
##    Major_code Major Major_category  Total Employed Employed_full_t~
##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;
##  1       1100 GENE~ Agriculture &amp;~ 128148    90245            74078
##  2       1101 AGRI~ Agriculture &amp;~  95326    76865            64240
##  3       1102 AGRI~ Agriculture &amp;~  33955    26321            22810
##  4       1103 ANIM~ Agriculture &amp;~ 103549    81177            64937
##  5       1104 FOOD~ Agriculture &amp;~  24280    17281            12722
##  6       1105 PLAN~ Agriculture &amp;~  79409    63043            51077
##  7       1106 SOIL~ Agriculture &amp;~   6586     4926             4042
##  8       1199 MISC~ Agriculture &amp;~   8549     6392             5074
##  9       1301 ENVI~ Biology &amp; Lif~ 106106    87602            65238
## 10       1302 FORE~ Agriculture &amp;~  69447    48228            39613
## # ... with 163 more rows, and 5 more variables: Unemployed &lt;dbl&gt;,
## #   Unemployment_rate &lt;dbl&gt;, Median &lt;dbl&gt;, P25th &lt;dbl&gt;, P75th &lt;dbl&gt;</code></pre>
<p><code>read_csv</code> has a few arguments I should highlight:</p>
<ul>
<li><strong><code>na</code></strong>: Character vector of strings to interpret as missing values. (this is like <code>na.strings</code> in <code>read.csv</code>)</li>
<li><strong><code>skip</code></strong>: Number of lines to skip before reading data.</li>
<li><strong><code>n_max</code></strong>: Maximum number of records to read.</li>
<li><strong><code>col_types</code></strong>: allows you to specify the column types for your data. <strong>I typically leave this as is, and let the parser do its job</strong>, but it can be helpful if you’re trying to coerce a certain column to certain data type</li>
</ul>
</div>
<div id="data.tablefread" class="section level3">
<h3>data.table::fread</h3>
<pre class="r"><code>library(data.table)</code></pre>
<p>Now, sometimes you might be dealing with some really nasty data that is large and unwieldy. <code>read_csv</code> is good for maybe 80-90% of data files, but sometimes we need something more powerful.</p>
<p>There is where the <code>fread</code> function from the <a href="https://rdatatable.gitlab.io/data.table/"><strong><code>data.table</code></strong></a> package comes in handy (further reading: <a href="https://github.com/Rdatatable/data.table/wiki/Convenience-features-of-fread">Convenience features of fread</a>).</p>
<p>By <a href="https://jozef.io/r917-fread-comparisons/">some measures</a>, <code>fread</code> can be about 6 times faster than <code>read.csv</code> and about 2.5 times faster than <code>read_csv</code>.</p>
<p>One of the best parts of <code>fread</code> is that you do not necessarily have to specify the delimiter in your data.</p>
<p>For example, the pipe delimited data from above is read in easily. In the code below I set <code>verbose = TRUE</code> to show the internal output from <code>fread</code>. In general, I’d recommend you leave this as FALSE, unless you’re in serious debug mode.</p>
<pre class="r"><code>pipe_dat &lt;- fread(&quot;data/iris_pipe_delim_edit.txt&quot;,
                  skip = 3, # homework: what happens if you don&#39;t specify skip?
                  verbose = TRUE, # default is FALSE, which I recommend
                  col.names = names(iris))</code></pre>
<pre><code>## omp_get_num_procs()==8
## R_DATATABLE_NUM_PROCS_PERCENT==&quot;&quot; (default 50)
## R_DATATABLE_NUM_THREADS==&quot;&quot;
## omp_get_thread_limit()==2147483647
## omp_get_max_threads()==8
## OMP_THREAD_LIMIT==&quot;&quot;
## OMP_NUM_THREADS==&quot;&quot;
## data.table is using 4 threads. This is set on startup, and by setDTthreads(). See ?setDTthreads.
## RestoreAfterFork==true
## Input contains no \n. Taking this to be a filename to open
## [01] Check arguments
##   Using 4 threads (omp_get_max_threads()=8, nth=4)
##   NAstrings = [&lt;&lt;NA&gt;&gt;]
##   None of the NAstrings look like numbers.
##   skip num lines = 3
##   show progress = 0
##   0/1 column will be read as integer
## [02] Opening the file
##   Opening file data/iris_pipe_delim_edit.txt
##   File opened, size = 356 bytes.
##   Memory mapped ok
## [03] Detect and skip BOM
## [04] Arrange mmap to be \0 terminated
##   \n has been found in the input and different lines can end with different line endings (e.g. mixed \n and \r\n in one file). This is common and ideal.
## [05] Skipping initial rows if needed
##   Skipped to line 4 in the file  Positioned on line 4 starting: &lt;&lt;5.1|3.5|1.4|0.2|&quot;setosa&quot;&gt;&gt;
## [06] Detect separator, quoting rule, and ncolumns
##   Detecting sep automatically ...
##   sep=&#39;|&#39;  with 6 lines of 5 fields using quote rule 0
##   Detected 5 columns on line 4. This line is either column names or first data row. Line starts as: &lt;&lt;5.1|3.5|1.4|0.2|&quot;setosa&quot;&gt;&gt;
##   Quote rule picked = 0
##   fill=false and the most number of columns found is 5
## [07] Detect column types, good nrow estimate and whether first row is column names
##   Number of sampling jump points = 1 because (150 bytes from row 1 to eof) / (2 * 150 jump0size) == 0
##   Type codes (jump 000)    : 7777A  Quote rule 0
##   &#39;header&#39; determined to be false because there are some number columns and those columns do not have a string field at the top of them
##   All rows were sampled since file is small so we know nrow=6 exactly
## [08] Assign column names
## [09] Apply user overrides on column types
##   After 0 type and 0 drop user overrides : 7777A
## [10] Allocate memory for the datatable
##   Allocating 5 column slots (5 - 0 dropped) with 6 rows
## [11] Read the data
##   jumps=[0..1), chunk_size=1048576, total_size=150
## Read 6 rows x 5 columns from 356 bytes file in 00:00.000 wall clock time
## [12] Finalizing the datatable
##   Type counts:
##          4 : float64   &#39;7&#39;
##          1 : string    &#39;A&#39;
## =============================
##    0.000s (  0%) Memory map 0.000GB file
##    0.000s (  0%) sep=&#39;|&#39; ncol=5 and header detection
##    0.000s (  0%) Column type detection using 6 sample rows
##    0.000s (  0%) Allocation of 6 rows x 5 cols (0.000GB) of which 6 (100%) rows used
##    0.000s (  0%) Reading 1 chunks (0 swept) of 1.000MB (each chunk 6 rows) using 1 threads
##    +    0.000s (  0%) Parse to row-major thread buffers (grown 0 times)
##    +    0.000s (  0%) Transpose
##    +    0.000s (  0%) Waiting
##    0.000s (  0%) Rereading 0 columns due to out-of-sample type exceptions
##    0.000s        Total</code></pre>
<pre class="r"><code>pipe_dat</code></pre>
<pre><code>##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1:          5.1         3.5          1.4         0.2  setosa
## 2:          4.9         3.0          1.4         0.2  setosa
## 3:          4.7         3.2          1.3         0.2  setosa
## 4:          4.6         3.1          1.5         0.2  setosa
## 5:          5.0         3.6          1.4         0.2  setosa
## 6:          5.4         3.9          1.7         0.4  setosa</code></pre>
<p>Here’s a demonstration of how much faster <code>fread</code> is than <code>read.csv</code> and <code>read_csv</code> using a subset of the flights data set.</p>
<pre class="r"><code>u1 &lt;- &quot;https://github.com/roberthryniewicz/datasets/&quot;
u2 &lt;- &quot;blob/master/airline-dataset/flights/flights.csv?raw=true&quot;
(uu &lt;- paste0(u1, u2))</code></pre>
<pre><code>## [1] &quot;https://github.com/roberthryniewicz/datasets/blob/master/airline-dataset/flights/flights.csv?raw=true&quot;</code></pre>
<pre class="r"><code>system.time(dat_base &lt;- read.csv(uu)) # timing for read.csv</code></pre>
<pre><code>##    user  system elapsed 
##   15.45    0.15   17.36</code></pre>
<pre class="r"><code>system.time(dat_readr &lt;- read_csv(uu)) # timing for read_csv</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_double(),
##   UniqueCarrier = col_character(),
##   TailNum = col_character(),
##   Origin = col_character(),
##   Dest = col_character(),
##   CancellationCode = col_character()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<pre><code>##    user  system elapsed 
##    0.67    0.18    3.29</code></pre>
<pre class="r"><code>system.time(dat_fread &lt;- fread(uu)) # timing for fread</code></pre>
<pre><code>##    user  system elapsed 
##    0.38    0.07    1.89</code></pre>
<pre class="r"><code>dim(dat_fread) # rows by columns</code></pre>
<pre><code>## [1] 100000     29</code></pre>
<p>I would strongly encourage you to spend some time playing around with <code>fread</code>, and thoroughly investigate its arguments (it’s got a lot!).</p>
<p>Considering all the benefits of <code>fread</code>, I’m actually surprised I don’t use it more.</p>
</div>
<div id="readxlread_excel" class="section level3">
<h3>readxl::read_excel</h3>
<pre class="r"><code>library(readxl)</code></pre>
<p>Maybe you’ve been unlucky enough to have to do some analysis using an excel file. This used to be a tedious task to get the data into R. Now, we can use the <code>read_excel</code> function from the <a href="https://readxl.tidyverse.org/index.html"><strong><code>readxl</code></strong></a> package.</p>
<p>Excel files will typically have multiple sheets. The <a href="https://community.tableau.com/servlet/JiveServlet/downloadBody/1236-102-2-15278/Sample%20-%20Superstore.xls">excel example</a> we’re looking at today has three separate sheets.</p>
<p>Reading these in is straightforward, using the <code>read_excel</code> function and the <code>sheet</code> argument.</p>
<pre class="r"><code>orders &lt;- read_excel(&quot;data/Superstore.xls&quot;,
                     sheet = &quot;Orders&quot;)</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L2236 / R2236C12: &#39;05408&#39;</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L5276 / R5276C12: &#39;05408&#39;</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L8800 / R8800C12: &#39;05408&#39;</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9148 / R9148C12: &#39;05408&#39;</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9149 / R9149C12: &#39;05408&#39;</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9150 / R9150C12: &#39;05408&#39;</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9388 / R9388C12: &#39;05408&#39;</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9389 / R9389C12: &#39;05408&#39;</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9390 / R9390C12: &#39;05408&#39;</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9391 / R9391C12: &#39;05408&#39;</code></pre>
<pre><code>## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =
## sheet, : Coercing text to numeric in L9743 / R9743C12: &#39;05408&#39;</code></pre>
<pre class="r"><code>tail(orders)</code></pre>
<pre><code>## # A tibble: 6 x 21
##   `Row ID` `Order ID` `Order Date`        `Ship Date`         `Ship Mode`
##      &lt;dbl&gt; &lt;chr&gt;      &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;      
## 1     9989 CA-2017-1~ 2017-11-17 00:00:00 2017-11-21 00:00:00 Standard C~
## 2     9990 CA-2014-1~ 2014-01-21 00:00:00 2014-01-23 00:00:00 Second Cla~
## 3     9991 CA-2017-1~ 2017-02-26 00:00:00 2017-03-03 00:00:00 Standard C~
## 4     9992 CA-2017-1~ 2017-02-26 00:00:00 2017-03-03 00:00:00 Standard C~
## 5     9993 CA-2017-1~ 2017-02-26 00:00:00 2017-03-03 00:00:00 Standard C~
## 6     9994 CA-2017-1~ 2017-05-04 00:00:00 2017-05-09 00:00:00 Second Cla~
## # ... with 16 more variables: `Customer ID` &lt;chr&gt;, `Customer Name` &lt;chr&gt;,
## #   Segment &lt;chr&gt;, Country &lt;chr&gt;, City &lt;chr&gt;, State &lt;chr&gt;, `Postal
## #   Code` &lt;dbl&gt;, Region &lt;chr&gt;, `Product ID` &lt;chr&gt;, Category &lt;chr&gt;,
## #   `Sub-Category` &lt;chr&gt;, `Product Name` &lt;chr&gt;, Sales &lt;dbl&gt;,
## #   Quantity &lt;dbl&gt;, Discount &lt;dbl&gt;, Profit &lt;dbl&gt;</code></pre>
<pre class="r"><code>returns &lt;- read_excel(&quot;data/Superstore.xls&quot;,
                      sheet = &quot;Returns&quot;)

tail(returns)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   Returned `Order ID`    
##   &lt;chr&gt;    &lt;chr&gt;         
## 1 Yes      US-2016-140172
## 2 Yes      CA-2015-101910
## 3 Yes      CA-2017-156958
## 4 Yes      CA-2016-105585
## 5 Yes      CA-2016-148796
## 6 Yes      CA-2015-149636</code></pre>
<pre class="r"><code>people &lt;- read_excel(&quot;data/Superstore.xls&quot;,
                     sheet = &quot;People&quot;)

tail(people)</code></pre>
<pre><code>## # A tibble: 4 x 2
##   Person            Region 
##   &lt;chr&gt;             &lt;chr&gt;  
## 1 Anna Andreadi     West   
## 2 Chuck Magee       East   
## 3 Kelly Williams    Central
## 4 Cassandra Brandow South</code></pre>
<p>A few things to note about <code>read_excel</code></p>
<ul>
<li><strong>VERY IMPORTANT</strong> Sometimes the function fails if you have the file open. Make sure the excel file is closed before trying to read it into R!</li>
<li>The function will inform you of parsing issues/column type coercion.</li>
<li>By default, the function will return a <code>tibble</code> and not a <code>data.frame</code>.</li>
</ul>
</div>
</div>
<div id="wrapping-up" class="section level2">
<h2>Wrapping Up</h2>
<p>In this post, we’ve looked at a few different ways of getting data into R from flat files. For nice flat files, it’s pretty straightforward to get your data into R. If your data isn’t so nice, you can generally be successful using the <code>fread</code> or <code>read_csv</code> functions, but you’ll need to be very aware of the structure of your data, as well as the arguments for whatever function decide to use.</p>
</div>
<div id="further-reading" class="section level2">
<h2>Further Reading</h2>
<p>Here are the links I’ve referenced.</p>
<div id="articles" class="section level4">
<h4>Articles</h4>
<ul>
<li><a href="https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/">stringsAsFactors: An unauthorized biography</a></li>
<li><a href="https://github.com/Rdatatable/data.table/wiki/Convenience-features-of-fread">Convenience features of fread</a></li>
<li><a href="https://jozef.io/r917-fread-comparisons/">How data.table’s fread can save you a lot of time and memory, and take input from shell commands</a></li>
</ul>
</div>
<div id="package-sites" class="section level4">
<h4>Package Sites</h4>
<ul>
<li><a href="https://readr.tidyverse.org/"><strong><code>readr</code></strong></a></li>
<li><a href="https://tibble.tidyverse.org/"><strong><code>tibble</code></strong></a></li>
<li><a href="https://rdatatable.gitlab.io/data.table/"><strong><code>data.table</code></strong></a></li>
<li><a href="https://readxl.tidyverse.org/index.html"><strong><code>readxl</code></strong></a></li>
</ul>
</div>
<div id="function-documentation" class="section level4">
<h4>Function Documentation</h4>
<ul>
<li><a href="https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html"><code>read.table</code></a></li>
<li><a href="https://readr.tidyverse.org/reference/read_delim.html"><code>readr::read_delim</code></a></li>
<li><a href="https://rdatatable.gitlab.io/data.table/reference/fread.html"><code>data.table::fread</code></a></li>
<li><a href="https://readxl.tidyverse.org/reference/read_excel.html"><code>readxl::read_excel</code></a></li>
</ul>
</div>
</div>
<div id="session-information" class="section level2">
<h2>Session Information</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.6.0 (2019-04-26)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18362)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.1252 
## [2] LC_CTYPE=English_United States.1252   
## [3] LC_MONETARY=English_United States.1252
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] readxl_1.3.1      data.table_1.12.2 readr_1.3.1      
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.1       knitr_1.23       magrittr_1.5     hms_0.4.2       
##  [5] R6_2.4.0         rlang_0.4.0      fansi_0.4.0      stringr_1.4.0   
##  [9] tools_3.6.0      xfun_0.8         utf8_1.1.4       cli_1.1.0       
## [13] htmltools_0.3.6  assertthat_0.2.1 yaml_2.2.0       digest_0.6.19   
## [17] tibble_2.1.3     crayon_1.3.4     bookdown_0.11    vctrs_0.1.0     
## [21] zeallot_0.1.0    evaluate_0.14    rmarkdown_1.13   blogdown_0.13   
## [25] stringi_1.4.3    compiler_3.6.0   pillar_1.4.1     cellranger_1.1.0
## [29] backports_1.1.4  pkgconfig_2.0.2</code></pre>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>Quantifying Home Field Advantage in the NFL Using Linear Models in R</title>
      <link>/post/quantifying-home-field-advantage-in-the-nfl-using-linear-models-in-r/</link>
      <pubDate>January 17, 2020</pubDate>
      
      <guid>/post/quantifying-home-field-advantage-in-the-nfl-using-linear-models-in-r/</guid>
      <description>&lt;p&gt;If you pay attention to NFL football, you’re probably used to hearing that &lt;a href=&#34;https://en.wikipedia.org/wiki/Home_advantage&#34;&gt;homefield advantage&lt;/a&gt; is worth about 3 points. I’ve always been interested in this number, and how it was derived. So, using &lt;a href=&#34;https://github.com/fivethirtyeight/data/tree/master/nfl-elo&#34;&gt;some data from FiveThirtyEight&lt;/a&gt;, along with some linear modeling in R, I attempted to quantify home field advantage. My analysis shows that home field advantage (how much we expect the home team to win by, if the teams are evenly matched) is about 2.59 points.&lt;/p&gt;
&lt;p&gt;Here are the packages we’ll need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(data.table)
library(ggridges)
library(scales)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find my code for this analysis &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/tree/master/quantifying-nfl-homefield-advantage&#34;&gt;on my GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;FiveThirtyEight has a data set with game-by-game Elo ratings and forecasts dating back to 1920. Elo ratings are simple measures of strength based on game-by-game results. More details on Elo ratings can be found &lt;a href=&#34;https://fivethirtyeight.com/features/nfl-elo-ratings-are-back/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It’s pretty easy to get this data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_link &amp;lt;- &amp;quot;https://projects.fivethirtyeight.com/nfl-api/nfl_elo.csv&amp;quot;

nfl_data &amp;lt;- fread(data_link, verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the first few rows and columns of the data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;date&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;season&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;neutral&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;playoff&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;team1&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;team2&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;elo1_pre&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;elo2_pre&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1920-09-26&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1920&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RII&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;STP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1503.947&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1300.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1920-10-03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1920&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DAY&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COL&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1493.002&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1504.908&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1920-10-03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1920&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RII&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MUN&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1516.108&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1478.004&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1920-10-03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1920&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CHI&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MUT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1368.333&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1300.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1920-10-03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1920&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CBD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PTQ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1504.688&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1300.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1920-10-03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1920&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BFF&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WBU&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1478.004&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1300.000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The full description of the data can be found on &lt;a href=&#34;https://github.com/fivethirtyeight/data/tree/master/nfl-elo&#34;&gt;FiveThirtyEight’s GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We’re interested in a few variables in this data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;variable&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;definition&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;elo1_pre&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Home team’s Elo rating before the game&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;elo2_pre&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Away team’s Elo rating before the game&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;qbelo1_pre&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Home team’s quarterback-adjusted base rating before the game&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;qbelo2_pre&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Away team’s quarterback-adjusted base rating before the game&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;score1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Home team’s score&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;score2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Away team’s score&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To quantify home field advantage, we can look at the home vs away score differential for all games &lt;strong&gt;not played at a neutral site&lt;/strong&gt;. We excluded playoff games from this analysis.&lt;/p&gt;
&lt;p&gt;Here’s the summary of that score differential:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;measure&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Min.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-57.00000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1st Qu.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-7.00000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Median&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3.00000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mean&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2.59056&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3rd Qu.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12.00000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Max.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;59.00000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The median margin of victory is 3. Since this number is positive, it implies that there &lt;em&gt;is&lt;/em&gt; a noticeable home field advantage.&lt;/p&gt;
&lt;p&gt;You might be wondering, has home field advantage been changing over time?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-17-quantifying-home-field-advantage-in-the-nfl-using-linear-models-in-r.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you look at the most recent decades, you’ll notice that the distribution has been becoming &lt;a href=&#34;https://en.wikipedia.org/wiki/Multimodal_distribution&#34;&gt;bimodal&lt;/a&gt;, meaning there are two “peaks” in the distribution. The peaks belong to margins of victory of three points (a field goal) for the home and away teams:&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-9&#34;&gt;Table 1: &lt;/span&gt;Two most frequently occurring outcomes for each decade&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Decade&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Home - Away&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Count of Games&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;% of Games&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2010&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;203&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8.02%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2010&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;165&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;6.52%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2000&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;225&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8.87%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2000&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;173&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;6.82%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1990&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;196&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8.419%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1990&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;181&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;7.775%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1980&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;159&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;7.472%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1980&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;139&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;6.532%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1970&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;102&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5.280%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1970&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;98&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5.072%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1960&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;84&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5.214%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1960&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;72&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.469%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1950&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;37&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5.096%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1950&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.821%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It’s not just good enough to take the average or median of all home vs away score differentials. Each NFL game is different, and by just blindly taking a summary statistic, we are assuming that the teams playing in each game are evenly matched. In my opinion, this assumption is &lt;strong&gt;invalid&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can use linear models to get closer to understanding home field advantage, by adjusting for the differences between the two teams. But before we get too deep into that, let’s take a closer look at linear regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-regression-basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Regression Basics&lt;/h2&gt;
&lt;p&gt;A lot of people are familiar with linear models, having performed “line of best fit” calculations sometime in high school. Most people cringe when they see the &lt;span class=&#34;math inline&#34;&gt;\(y = mx + b\)&lt;/span&gt; formula, but statisticians and data scientists feel their hearts warm and get very excited after glancing at that formula.&lt;/p&gt;
&lt;p&gt;Linear models are incredibly powerful tools of statistical analysis. Most of the time, we spend a lot of energy interpreting the &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; in the equation above. This gives us insight into how much we expect &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to change (&lt;strong&gt;on average&lt;/strong&gt;) when &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; changes by some amount.&lt;/p&gt;
&lt;p&gt;To illustrate, let’s use the mtcars data set to predict a car’s miles per gallon using its weight in pounds.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-17-quantifying-home-field-advantage-in-the-nfl-using-linear-models-in-r.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The linear model’s formula is displayed in the upper right hand corner of the plot. The coefficient for &lt;span class=&#34;math inline&#34;&gt;\(wt\)&lt;/span&gt; is -5.3, this is the &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; in the &lt;span class=&#34;math inline&#34;&gt;\(y = mx + b\)&lt;/span&gt; equation. The coefficient is negative, meaning that as the weight of the car increases, we expect its fuel efficiency to get worse. But what about &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, the intercept of the line?&lt;/p&gt;
&lt;p&gt;The intercept of the best fit line is our “predicted” value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; equals 0. So, when the weight of a car is 0 pounds, we expect it to get 37.29 miles per gallon. This doesn’t make any sense! A car that weighs 0 pounds doesn’t get any miles per gallon, it doesn’t even exist!&lt;/p&gt;
&lt;p&gt;In a lot of linear models, the intercept isn’t really worth interpreting. However, we can use the intercept to understand home field advantage using our NFL data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpreting-the-intercept-in-a-model-of-the-nfl&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpreting the Intercept in a Model of the NFL&lt;/h2&gt;
&lt;p&gt;We first fit a model trying to predict the home vs away score differential using the home vs away pre game Elo differential and the home vs away pre game QB Elo differential.&lt;/p&gt;
&lt;p&gt;When the Elo differentials are equal to zero, it means the teams are effectively even matched (our best guess for the power rankings of the respective teams are basically equal). This gets us closer to understanding true home field advantage than taking a summary statistic would.&lt;/p&gt;
&lt;p&gt;Our model will look like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \text{Home - Away Score} = \beta_{0} + \beta_{1} *\text{Elo Difference} + \beta_{2} * \text{QB Elo Difference}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{2}\)&lt;/span&gt; above are interesting, we’re most interested in &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;, since this quantifies the home field advantage for evenly matched teams.&lt;/p&gt;
&lt;p&gt;Enough talking, let’s fit the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;score_diff_model &amp;lt;- lm(home_away_score_diff ~ home_away_elo_diff+home_away_qb_elo_diff,
                       data = model_data)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.586&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;home_away_elo_diff&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.037&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;home_away_qb_elo_diff&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.021&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Touchdown!&lt;/p&gt;
&lt;p&gt;If we look at the intercept, the value is about 2.59. This means that if the two teams are basically evenly matched (i.e. &lt;span class=&#34;math inline&#34;&gt;\(x=0\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(y = mx+b\)&lt;/span&gt;), we can expect the home team to win by about 2.59 points (&lt;strong&gt;on average&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;Taking it one step further, here are the home field advantages for each decade.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-17-quantifying-home-field-advantage-in-the-nfl-using-linear-models-in-r.en_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;We can learn some pretty cool things about our data if we pay close attention to the output of our linear models. I think a lot of people forget to pay attention to the intercepts in their linear models. This makes sense most of the time, because the intercept doesn’t really mean much in many of our models (e.g. if we predict a person’s height using their weight, the intercept is meaningless).&lt;/p&gt;
&lt;p&gt;However, in some cases the intercept is really important. In this example using NFL data, we were able to use the intercept to quantify home field advantage for evenly matched teams. Hopefully this post will give a data point to bring up at a nice dinner party where you and your acquaintances are discussing what home field advantage &lt;em&gt;really&lt;/em&gt; means.&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded><p>If you pay attention to NFL football, you’re probably used to hearing that <a href="https://en.wikipedia.org/wiki/Home_advantage">homefield advantage</a> is worth about 3 points. I’ve always been interested in this number, and how it was derived. So, using <a href="https://github.com/fivethirtyeight/data/tree/master/nfl-elo">some data from FiveThirtyEight</a>, along with some linear modeling in R, I attempted to quantify home field advantage. My analysis shows that home field advantage (how much we expect the home team to win by, if the teams are evenly matched) is about 2.59 points.</p>
<p>Here are the packages we’ll need:</p>
<pre class="r"><code>library(tidyverse)
library(data.table)
library(ggridges)
library(scales)</code></pre>
<p>You can find my code for this analysis <a href="https://github.com/bgstieber/files_for_blog/tree/master/quantifying-nfl-homefield-advantage">on my GitHub</a>.</p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>FiveThirtyEight has a data set with game-by-game Elo ratings and forecasts dating back to 1920. Elo ratings are simple measures of strength based on game-by-game results. More details on Elo ratings can be found <a href="https://fivethirtyeight.com/features/nfl-elo-ratings-are-back/">here</a>.</p>
<p>It’s pretty easy to get this data.</p>
<pre class="r"><code>data_link &lt;- &quot;https://projects.fivethirtyeight.com/nfl-api/nfl_elo.csv&quot;

nfl_data &lt;- fread(data_link, verbose = FALSE)</code></pre>
<p>Here are the first few rows and columns of the data:</p>
<table>
<thead>
<tr class="header">
<th align="left">date</th>
<th align="left">season</th>
<th align="left">neutral</th>
<th align="left">playoff</th>
<th align="left">team1</th>
<th align="left">team2</th>
<th align="left">elo1_pre</th>
<th align="left">elo2_pre</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1920-09-26</td>
<td align="left">1920</td>
<td align="left">0</td>
<td align="left"></td>
<td align="left">RII</td>
<td align="left">STP</td>
<td align="left">1503.947</td>
<td align="left">1300.000</td>
</tr>
<tr class="even">
<td align="left">1920-10-03</td>
<td align="left">1920</td>
<td align="left">0</td>
<td align="left"></td>
<td align="left">DAY</td>
<td align="left">COL</td>
<td align="left">1493.002</td>
<td align="left">1504.908</td>
</tr>
<tr class="odd">
<td align="left">1920-10-03</td>
<td align="left">1920</td>
<td align="left">0</td>
<td align="left"></td>
<td align="left">RII</td>
<td align="left">MUN</td>
<td align="left">1516.108</td>
<td align="left">1478.004</td>
</tr>
<tr class="even">
<td align="left">1920-10-03</td>
<td align="left">1920</td>
<td align="left">0</td>
<td align="left"></td>
<td align="left">CHI</td>
<td align="left">MUT</td>
<td align="left">1368.333</td>
<td align="left">1300.000</td>
</tr>
<tr class="odd">
<td align="left">1920-10-03</td>
<td align="left">1920</td>
<td align="left">0</td>
<td align="left"></td>
<td align="left">CBD</td>
<td align="left">PTQ</td>
<td align="left">1504.688</td>
<td align="left">1300.000</td>
</tr>
<tr class="even">
<td align="left">1920-10-03</td>
<td align="left">1920</td>
<td align="left">0</td>
<td align="left"></td>
<td align="left">BFF</td>
<td align="left">WBU</td>
<td align="left">1478.004</td>
<td align="left">1300.000</td>
</tr>
</tbody>
</table>
<p>The full description of the data can be found on <a href="https://github.com/fivethirtyeight/data/tree/master/nfl-elo">FiveThirtyEight’s GitHub</a>.</p>
<p>We’re interested in a few variables in this data:</p>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="left">definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">elo1_pre</td>
<td align="left">Home team’s Elo rating before the game</td>
</tr>
<tr class="even">
<td align="left">elo2_pre</td>
<td align="left">Away team’s Elo rating before the game</td>
</tr>
<tr class="odd">
<td align="left">qbelo1_pre</td>
<td align="left">Home team’s quarterback-adjusted base rating before the game</td>
</tr>
<tr class="even">
<td align="left">qbelo2_pre</td>
<td align="left">Away team’s quarterback-adjusted base rating before the game</td>
</tr>
<tr class="odd">
<td align="left">score1</td>
<td align="left">Home team’s score</td>
</tr>
<tr class="even">
<td align="left">score2</td>
<td align="left">Away team’s score</td>
</tr>
</tbody>
</table>
<p>To quantify home field advantage, we can look at the home vs away score differential for all games <strong>not played at a neutral site</strong>. We excluded playoff games from this analysis.</p>
<p>Here’s the summary of that score differential:</p>
<table>
<thead>
<tr class="header">
<th align="left">measure</th>
<th align="left">value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Min.</td>
<td align="left">-57.00000</td>
</tr>
<tr class="even">
<td align="left">1st Qu.</td>
<td align="left">-7.00000</td>
</tr>
<tr class="odd">
<td align="left">Median</td>
<td align="left">3.00000</td>
</tr>
<tr class="even">
<td align="left">Mean</td>
<td align="left">2.59056</td>
</tr>
<tr class="odd">
<td align="left">3rd Qu.</td>
<td align="left">12.00000</td>
</tr>
<tr class="even">
<td align="left">Max.</td>
<td align="left">59.00000</td>
</tr>
</tbody>
</table>
<p>The median margin of victory is 3. Since this number is positive, it implies that there <em>is</em> a noticeable home field advantage.</p>
<p>You might be wondering, has home field advantage been changing over time?</p>
<p><img src="/post/2020-01-17-quantifying-home-field-advantage-in-the-nfl-using-linear-models-in-r.en_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>If you look at the most recent decades, you’ll notice that the distribution has been becoming <a href="https://en.wikipedia.org/wiki/Multimodal_distribution">bimodal</a>, meaning there are two “peaks” in the distribution. The peaks belong to margins of victory of three points (a field goal) for the home and away teams:</p>
<table>
<caption><span id="tab:unnamed-chunk-9">Table 1: </span>Two most frequently occurring outcomes for each decade</caption>
<thead>
<tr class="header">
<th align="left">Decade</th>
<th align="left">Home - Away</th>
<th align="left">Count of Games</th>
<th align="left">% of Games</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2010</td>
<td align="left">3</td>
<td align="left">203</td>
<td align="left">8.02%</td>
</tr>
<tr class="even">
<td align="left">2010</td>
<td align="left">-3</td>
<td align="left">165</td>
<td align="left">6.52%</td>
</tr>
<tr class="odd">
<td align="left">2000</td>
<td align="left">3</td>
<td align="left">225</td>
<td align="left">8.87%</td>
</tr>
<tr class="even">
<td align="left">2000</td>
<td align="left">-3</td>
<td align="left">173</td>
<td align="left">6.82%</td>
</tr>
<tr class="odd">
<td align="left">1990</td>
<td align="left">3</td>
<td align="left">196</td>
<td align="left">8.419%</td>
</tr>
<tr class="even">
<td align="left">1990</td>
<td align="left">-3</td>
<td align="left">181</td>
<td align="left">7.775%</td>
</tr>
<tr class="odd">
<td align="left">1980</td>
<td align="left">3</td>
<td align="left">159</td>
<td align="left">7.472%</td>
</tr>
<tr class="even">
<td align="left">1980</td>
<td align="left">-3</td>
<td align="left">139</td>
<td align="left">6.532%</td>
</tr>
<tr class="odd">
<td align="left">1970</td>
<td align="left">3</td>
<td align="left">102</td>
<td align="left">5.280%</td>
</tr>
<tr class="even">
<td align="left">1970</td>
<td align="left">-3</td>
<td align="left">98</td>
<td align="left">5.072%</td>
</tr>
<tr class="odd">
<td align="left">1960</td>
<td align="left">3</td>
<td align="left">84</td>
<td align="left">5.214%</td>
</tr>
<tr class="even">
<td align="left">1960</td>
<td align="left">0</td>
<td align="left">72</td>
<td align="left">4.469%</td>
</tr>
<tr class="odd">
<td align="left">1950</td>
<td align="left">3</td>
<td align="left">37</td>
<td align="left">5.096%</td>
</tr>
<tr class="even">
<td align="left">1950</td>
<td align="left">-4</td>
<td align="left">35</td>
<td align="left">4.821%</td>
</tr>
</tbody>
</table>
<p>It’s not just good enough to take the average or median of all home vs away score differentials. Each NFL game is different, and by just blindly taking a summary statistic, we are assuming that the teams playing in each game are evenly matched. In my opinion, this assumption is <strong>invalid</strong>.</p>
<p>We can use linear models to get closer to understanding home field advantage, by adjusting for the differences between the two teams. But before we get too deep into that, let’s take a closer look at linear regression.</p>
</div>
<div id="linear-regression-basics" class="section level2">
<h2>Linear Regression Basics</h2>
<p>A lot of people are familiar with linear models, having performed “line of best fit” calculations sometime in high school. Most people cringe when they see the <span class="math inline">\(y = mx + b\)</span> formula, but statisticians and data scientists feel their hearts warm and get very excited after glancing at that formula.</p>
<p>Linear models are incredibly powerful tools of statistical analysis. Most of the time, we spend a lot of energy interpreting the <span class="math inline">\(m\)</span> in the equation above. This gives us insight into how much we expect <span class="math inline">\(y\)</span> to change (<strong>on average</strong>) when <span class="math inline">\(x\)</span> changes by some amount.</p>
<p>To illustrate, let’s use the mtcars data set to predict a car’s miles per gallon using its weight in pounds.</p>
<p><img src="/post/2020-01-17-quantifying-home-field-advantage-in-the-nfl-using-linear-models-in-r.en_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>The linear model’s formula is displayed in the upper right hand corner of the plot. The coefficient for <span class="math inline">\(wt\)</span> is -5.3, this is the <span class="math inline">\(m\)</span> in the <span class="math inline">\(y = mx + b\)</span> equation. The coefficient is negative, meaning that as the weight of the car increases, we expect its fuel efficiency to get worse. But what about <span class="math inline">\(b\)</span>, the intercept of the line?</p>
<p>The intercept of the best fit line is our “predicted” value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> equals 0. So, when the weight of a car is 0 pounds, we expect it to get 37.29 miles per gallon. This doesn’t make any sense! A car that weighs 0 pounds doesn’t get any miles per gallon, it doesn’t even exist!</p>
<p>In a lot of linear models, the intercept isn’t really worth interpreting. However, we can use the intercept to understand home field advantage using our NFL data.</p>
</div>
<div id="interpreting-the-intercept-in-a-model-of-the-nfl" class="section level2">
<h2>Interpreting the Intercept in a Model of the NFL</h2>
<p>We first fit a model trying to predict the home vs away score differential using the home vs away pre game Elo differential and the home vs away pre game QB Elo differential.</p>
<p>When the Elo differentials are equal to zero, it means the teams are effectively even matched (our best guess for the power rankings of the respective teams are basically equal). This gets us closer to understanding true home field advantage than taking a summary statistic would.</p>
<p>Our model will look like:</p>
<p><span class="math display">\[ \text{Home - Away Score} = \beta_{0} + \beta_{1} *\text{Elo Difference} + \beta_{2} * \text{QB Elo Difference}\]</span></p>
<p>While <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{2}\)</span> above are interesting, we’re most interested in <span class="math inline">\(\beta_{0}\)</span>, since this quantifies the home field advantage for evenly matched teams.</p>
<p>Enough talking, let’s fit the model:</p>
<pre class="r"><code>score_diff_model &lt;- lm(home_away_score_diff ~ home_away_elo_diff+home_away_qb_elo_diff,
                       data = model_data)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">2.586</td>
</tr>
<tr class="even">
<td align="left">home_away_elo_diff</td>
<td align="right">0.037</td>
</tr>
<tr class="odd">
<td align="left">home_away_qb_elo_diff</td>
<td align="right">0.021</td>
</tr>
</tbody>
</table>
<p>Touchdown!</p>
<p>If we look at the intercept, the value is about 2.59. This means that if the two teams are basically evenly matched (i.e. <span class="math inline">\(x=0\)</span> in <span class="math inline">\(y = mx+b\)</span>), we can expect the home team to win by about 2.59 points (<strong>on average</strong>).</p>
<p>Taking it one step further, here are the home field advantages for each decade.</p>
<p><img src="/post/2020-01-17-quantifying-home-field-advantage-in-the-nfl-using-linear-models-in-r.en_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="wrapping-up" class="section level2">
<h2>Wrapping Up</h2>
<p>We can learn some pretty cool things about our data if we pay close attention to the output of our linear models. I think a lot of people forget to pay attention to the intercepts in their linear models. This makes sense most of the time, because the intercept doesn’t really mean much in many of our models (e.g. if we predict a person’s height using their weight, the intercept is meaningless).</p>
<p>However, in some cases the intercept is really important. In this example using NFL data, we were able to use the intercept to quantify home field advantage for evenly matched teams. Hopefully this post will give a data point to bring up at a nice dinner party where you and your acquaintances are discussing what home field advantage <em>really</em> means.</p>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>7 Tips for Delivering a Great Data Science Presentation</title>
      <link>/post/7-tips-for-delivering-a-great-data-science-presentation/</link>
      <pubDate>November 3, 2019</pubDate>
      
      <guid>/post/7-tips-for-delivering-a-great-data-science-presentation/</guid>
      <description>&lt;p&gt;Delivering a great data science presentation can seem daunting. By no means am I a communications expert, but I have presented my fair share of talks to a diverse group of audiences. Through my experience, I’ve developed a few easy-to-remember tips to hopefully make your next data science presentation your best yet. These are tips that have worked for me, and I hope they’re helpful!&lt;/p&gt;
&lt;p&gt;Without further ado, here are &lt;strong&gt;seven tips for delivering a great data science presentation&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;practice-practice-practice&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Practice, practice, practice&lt;/h3&gt;
&lt;p&gt;This is the most important tip I can give to anyone. Practicing your presentation (in front of colleagues, friends, family, or the mirror) is one of the best ways to make sure you’ll feel comfortable on the day of your talk. In my opinion, it’s the best way to “stress-test” your talk and make sure you’re prepared.&lt;/p&gt;
&lt;p&gt;By practicing multiple times, you can find portions of your presentation that can be edited, enhanced, or eliminated. By presenting to others and taking their feedback seriously, you can prepare yourself for your real audience and the questions they might have.&lt;/p&gt;
&lt;p&gt;A corollary of &lt;strong&gt;practice, practice, practice&lt;/strong&gt;, is &lt;strong&gt;perfect practice makes perfect&lt;/strong&gt;. In your practice talks, try to simulate the environment of your talk as close as possible. It may help to deliver your practice talk while standing, dress up for your practice talk, or even practice in a public space if you expect your room to be noisy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;talk-about-what-you-know-but-dont-be-afraid-to-branch-out-a-little&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Talk about what you know, but don’t be afraid to branch out a little&lt;/h3&gt;
&lt;p&gt;Another way to feel more comfortable is to make sure you’re comfortable with the material you’re presenting. I’ve been to a few talks where it was clear that the presenter wasn’t entirely confident with the subject of their talk. Data science is a broad field, so it’s nearly impossible to be an expert in &lt;em&gt;everything&lt;/em&gt;. Even though you &lt;em&gt;probably&lt;/em&gt; can’t be an expert in &lt;em&gt;everything&lt;/em&gt;, you can be knowledgeable about &lt;em&gt;something&lt;/em&gt;. Talk about that something!&lt;/p&gt;
&lt;p&gt;That being said, delivering a presentation &lt;em&gt;can&lt;/em&gt; be an excellent way to familiarize yourself with a topic you haven’t explored in depth. Using the pressure of delivering a presentation may help you learn something you’ve been struggling to learn.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;look-yourself-in-the-mirror-and-say-im-not-an-impostor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. Look yourself in the mirror and say “I’m not an impostor”&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.uwhealth.org/health-wellness/tips-to-overcome-imposter-syndrome/52943&#34;&gt;Imposter syndrome&lt;/a&gt; is real and it sucks.&lt;/p&gt;
&lt;p&gt;One of my favorite posters growing up was this one:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.pinimg.com/736x/79/65/16/796516bfc4186f6e7762eae6617fe30a--baseball-mom-baseball-stuff.jpg&#34; height = &#34;400px&#34; width = &#34;300px&#34;&gt;&lt;/p&gt;
&lt;p&gt;We’re all at different levels of developing our skills. I’ve found that some of my biggest data science aha moments come from having conversations with people outside of the field. Sometimes, knowing too much &lt;em&gt;can&lt;/em&gt; be a bad thing, especially when it introduces rigidity or bias to a problem-solving strategy.&lt;/p&gt;
&lt;p&gt;No matter what level you’re at, &lt;strong&gt;I guarantee that you can provide useful information to your audience&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;put-yourself-in-the-shoes-of-an-audience-member&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4. Put yourself in the shoes of an audience member&lt;/h3&gt;
&lt;p&gt;If you were attending your talk, what would you want to learn? how would you expect the presentation to be delivered? how much information do you think you can absorb? You should also remind yourself that most people in the audience are coming to your presentation to have a positive experience.&lt;/p&gt;
&lt;p&gt;By taking an empathetic approach to understand the preferences of your audience, you can better prepare your talk and make it as effective as possible.&lt;/p&gt;
&lt;p&gt;One of my favorite (and most critical) ways to evaluate my own presentation is to ask myself “so what?” or “why should I care?” for each slide in my deck. If the slide can’t pass that test, I either remove it or edit it until it does. It’s probably an extreme approach, but I like doing this because it ensures that I’ve taken every step I can to reduce any irrelevant sections from my talk.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;play-your-game-not-anyone-elses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5. Play your game, not anyone else’s&lt;/h3&gt;
&lt;p&gt;One of the most notable data presentations is &lt;a href=&#34;https://www.youtube.com/watch?v=hVimVzgtD6w&#34;&gt;this one&lt;/a&gt; from Hans Rosling. He’s engaging, dynamic, and entertaining. His presentation style seems effortless. He makes data fun!&lt;/p&gt;
&lt;p&gt;Although I’d like to give presentations like Hans Rosling, I know I can’t right now. The best I can do is to present in the style that best fits me. This doesn’t mean I’ve resigned to delivering flat, boring, and dry talks. Instead, I stay realistic and try to do the best that I can do, not the best Hans Rosling could do.&lt;/p&gt;
&lt;p&gt;Speaking of being realistic…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-realistic-expectations-strive-to-meet-them-and-hold-yourself-accountable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;6. Set realistic expectations, strive to meet them, and hold yourself accountable&lt;/h3&gt;
&lt;p&gt;Another way to say this is to &lt;strong&gt;set yourself up for success, not disappointment&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Here’s an example.&lt;/p&gt;
&lt;p&gt;For my first presentation at a professional conference, the conference organizers provided ways to promote your session through social media and email. I thought this was really cool, but I had no interest taking part in these promotional activities. I had already committed to delivering my first talk at a professional conference, which took a lot of time to prepare for. I set a goal of delivering the best talk I could, and promoting my session was not part of my plan to achieve this goal. I stayed focused on my primary goal, and avoided getting distracted by anything else.&lt;/p&gt;
&lt;p&gt;Here are a few different ways of turning unrealistic goals into realistic and attainable ones.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Unrealistic&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Realistic&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Deliver the best damn talk anyone has ever seen&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Deliver a strong talk, providing useful information to those in attendance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Make people fall out of their chairs laughing&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Have a few punch lines that make at least one person laugh/chuckle/smirk&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Make my twenty minute talk last exactly twenty minutes&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Make my twenty minute talk last about twenty minutes, without sacrificing material or interfering with others’ presentations&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;BLOW MINDS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Provide a new and accessible way of framing or thinking about a problem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Be the next Hans Rosling&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Deliver an engaging presentation, without seeming inauthentic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;take-audience-feedback-and-questions-seriously-and-use-them-to-learn-and-improve&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;7. Take audience feedback and questions seriously, and use them to learn and improve&lt;/h3&gt;
&lt;p&gt;This tip doesn’t apply to delivering your presentation, it’s about your &lt;em&gt;next&lt;/em&gt; presentation.&lt;/p&gt;
&lt;p&gt;After you’ve finished your talk, do an analysis of your presentation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What went well?&lt;/li&gt;
&lt;li&gt;What could have been improved?&lt;/li&gt;
&lt;li&gt;How engaged was your audience?
&lt;ul&gt;
&lt;li&gt;An indicator I use to judge engagement are how many people laugh at one of my lame jokes and how many people ask questions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;What questions were asked?
&lt;ul&gt;
&lt;li&gt;You can use these questions to understand how well you conveyed your points and how engaging your presentation was.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Did you receive any feedback or constructive criticism on your content, delivery, and presentation style?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can learn a lot just by paying attention to how your audience received you and your own perception of your performance.&lt;/p&gt;
&lt;p&gt;Just like how most machine learning algorithms get better at minimizing a loss function through iteration after iteration, you can improve your presentation through actively learning and iterating on your presentation style.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wrapping up&lt;/h1&gt;
&lt;p&gt;Just to recap, here are my tips for a great data science presentation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Practice, practice, practice&lt;/li&gt;
&lt;li&gt;Talk about what you know&lt;/li&gt;
&lt;li&gt;Look yourself in the mirror and say “I’m not an impostor”&lt;/li&gt;
&lt;li&gt;Put yourself in the shoes of an audience member&lt;/li&gt;
&lt;li&gt;Play your game, not anyone else’s&lt;/li&gt;
&lt;li&gt;Set realistic expectations, strive to meet them, and hold yourself accountable&lt;/li&gt;
&lt;li&gt;Take audience feedback and questions seriously, and use them to learn and improve&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Do you have any tips for giving a great talk? If so, go ahead and leave them in the comments below!&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded><p>Delivering a great data science presentation can seem daunting. By no means am I a communications expert, but I have presented my fair share of talks to a diverse group of audiences. Through my experience, I’ve developed a few easy-to-remember tips to hopefully make your next data science presentation your best yet. These are tips that have worked for me, and I hope they’re helpful!</p>
<p>Without further ado, here are <strong>seven tips for delivering a great data science presentation</strong>.</p>
<div id="practice-practice-practice" class="section level3">
<h3>1. Practice, practice, practice</h3>
<p>This is the most important tip I can give to anyone. Practicing your presentation (in front of colleagues, friends, family, or the mirror) is one of the best ways to make sure you’ll feel comfortable on the day of your talk. In my opinion, it’s the best way to “stress-test” your talk and make sure you’re prepared.</p>
<p>By practicing multiple times, you can find portions of your presentation that can be edited, enhanced, or eliminated. By presenting to others and taking their feedback seriously, you can prepare yourself for your real audience and the questions they might have.</p>
<p>A corollary of <strong>practice, practice, practice</strong>, is <strong>perfect practice makes perfect</strong>. In your practice talks, try to simulate the environment of your talk as close as possible. It may help to deliver your practice talk while standing, dress up for your practice talk, or even practice in a public space if you expect your room to be noisy.</p>
</div>
<div id="talk-about-what-you-know-but-dont-be-afraid-to-branch-out-a-little" class="section level3">
<h3>2. Talk about what you know, but don’t be afraid to branch out a little</h3>
<p>Another way to feel more comfortable is to make sure you’re comfortable with the material you’re presenting. I’ve been to a few talks where it was clear that the presenter wasn’t entirely confident with the subject of their talk. Data science is a broad field, so it’s nearly impossible to be an expert in <em>everything</em>. Even though you <em>probably</em> can’t be an expert in <em>everything</em>, you can be knowledgeable about <em>something</em>. Talk about that something!</p>
<p>That being said, delivering a presentation <em>can</em> be an excellent way to familiarize yourself with a topic you haven’t explored in depth. Using the pressure of delivering a presentation may help you learn something you’ve been struggling to learn.</p>
</div>
<div id="look-yourself-in-the-mirror-and-say-im-not-an-impostor" class="section level3">
<h3>3. Look yourself in the mirror and say “I’m not an impostor”</h3>
<p><a href="https://www.uwhealth.org/health-wellness/tips-to-overcome-imposter-syndrome/52943">Imposter syndrome</a> is real and it sucks.</p>
<p>One of my favorite posters growing up was this one:</p>
<p><img src="https://i.pinimg.com/736x/79/65/16/796516bfc4186f6e7762eae6617fe30a--baseball-mom-baseball-stuff.jpg" height = "400px" width = "300px"></p>
<p>We’re all at different levels of developing our skills. I’ve found that some of my biggest data science aha moments come from having conversations with people outside of the field. Sometimes, knowing too much <em>can</em> be a bad thing, especially when it introduces rigidity or bias to a problem-solving strategy.</p>
<p>No matter what level you’re at, <strong>I guarantee that you can provide useful information to your audience</strong>.</p>
</div>
<div id="put-yourself-in-the-shoes-of-an-audience-member" class="section level3">
<h3>4. Put yourself in the shoes of an audience member</h3>
<p>If you were attending your talk, what would you want to learn? how would you expect the presentation to be delivered? how much information do you think you can absorb? You should also remind yourself that most people in the audience are coming to your presentation to have a positive experience.</p>
<p>By taking an empathetic approach to understand the preferences of your audience, you can better prepare your talk and make it as effective as possible.</p>
<p>One of my favorite (and most critical) ways to evaluate my own presentation is to ask myself “so what?” or “why should I care?” for each slide in my deck. If the slide can’t pass that test, I either remove it or edit it until it does. It’s probably an extreme approach, but I like doing this because it ensures that I’ve taken every step I can to reduce any irrelevant sections from my talk.</p>
</div>
<div id="play-your-game-not-anyone-elses" class="section level3">
<h3>5. Play your game, not anyone else’s</h3>
<p>One of the most notable data presentations is <a href="https://www.youtube.com/watch?v=hVimVzgtD6w">this one</a> from Hans Rosling. He’s engaging, dynamic, and entertaining. His presentation style seems effortless. He makes data fun!</p>
<p>Although I’d like to give presentations like Hans Rosling, I know I can’t right now. The best I can do is to present in the style that best fits me. This doesn’t mean I’ve resigned to delivering flat, boring, and dry talks. Instead, I stay realistic and try to do the best that I can do, not the best Hans Rosling could do.</p>
<p>Speaking of being realistic…</p>
</div>
<div id="set-realistic-expectations-strive-to-meet-them-and-hold-yourself-accountable" class="section level3">
<h3>6. Set realistic expectations, strive to meet them, and hold yourself accountable</h3>
<p>Another way to say this is to <strong>set yourself up for success, not disappointment</strong>.</p>
<p>Here’s an example.</p>
<p>For my first presentation at a professional conference, the conference organizers provided ways to promote your session through social media and email. I thought this was really cool, but I had no interest taking part in these promotional activities. I had already committed to delivering my first talk at a professional conference, which took a lot of time to prepare for. I set a goal of delivering the best talk I could, and promoting my session was not part of my plan to achieve this goal. I stayed focused on my primary goal, and avoided getting distracted by anything else.</p>
<p>Here are a few different ways of turning unrealistic goals into realistic and attainable ones.</p>
<table>
<thead>
<tr class="header">
<th align="left">Unrealistic</th>
<th align="left">Realistic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Deliver the best damn talk anyone has ever seen</td>
<td align="left">Deliver a strong talk, providing useful information to those in attendance</td>
</tr>
<tr class="even">
<td align="left">Make people fall out of their chairs laughing</td>
<td align="left">Have a few punch lines that make at least one person laugh/chuckle/smirk</td>
</tr>
<tr class="odd">
<td align="left">Make my twenty minute talk last exactly twenty minutes</td>
<td align="left">Make my twenty minute talk last about twenty minutes, without sacrificing material or interfering with others’ presentations</td>
</tr>
<tr class="even">
<td align="left">BLOW MINDS</td>
<td align="left">Provide a new and accessible way of framing or thinking about a problem</td>
</tr>
<tr class="odd">
<td align="left">Be the next Hans Rosling</td>
<td align="left">Deliver an engaging presentation, without seeming inauthentic</td>
</tr>
</tbody>
</table>
</div>
<div id="take-audience-feedback-and-questions-seriously-and-use-them-to-learn-and-improve" class="section level3">
<h3>7. Take audience feedback and questions seriously, and use them to learn and improve</h3>
<p>This tip doesn’t apply to delivering your presentation, it’s about your <em>next</em> presentation.</p>
<p>After you’ve finished your talk, do an analysis of your presentation.</p>
<ul>
<li>What went well?</li>
<li>What could have been improved?</li>
<li>How engaged was your audience?
<ul>
<li>An indicator I use to judge engagement are how many people laugh at one of my lame jokes and how many people ask questions</li>
</ul></li>
<li>What questions were asked?
<ul>
<li>You can use these questions to understand how well you conveyed your points and how engaging your presentation was.</li>
</ul></li>
<li>Did you receive any feedback or constructive criticism on your content, delivery, and presentation style?</li>
</ul>
<p>You can learn a lot just by paying attention to how your audience received you and your own perception of your performance.</p>
<p>Just like how most machine learning algorithms get better at minimizing a loss function through iteration after iteration, you can improve your presentation through actively learning and iterating on your presentation style.</p>
</div>
<div id="wrapping-up" class="section level1">
<h1>Wrapping up</h1>
<p>Just to recap, here are my tips for a great data science presentation:</p>
<ol style="list-style-type: decimal">
<li>Practice, practice, practice</li>
<li>Talk about what you know</li>
<li>Look yourself in the mirror and say “I’m not an impostor”</li>
<li>Put yourself in the shoes of an audience member</li>
<li>Play your game, not anyone else’s</li>
<li>Set realistic expectations, strive to meet them, and hold yourself accountable</li>
<li>Take audience feedback and questions seriously, and use them to learn and improve</li>
</ol>
<p>Do you have any tips for giving a great talk? If so, go ahead and leave them in the comments below!</p>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>Roulette Wheels for Multi-Armed Bandits: A Simulation in R</title>
      <link>/post/roulette-wheels-for-multi-armed-bandits-a-simulation-in-r/</link>
      <pubDate>October 29, 2019</pubDate>
      
      <guid>/post/roulette-wheels-for-multi-armed-bandits-a-simulation-in-r/</guid>
      <description>&lt;p&gt;One of my favorite &lt;a href=&#34;https://jamesmccaffrey.wordpress.com/&#34;&gt;data science blogs&lt;/a&gt; comes from James McCaffrey, a software engineer and researcher at Microsoft. He recently wrote a &lt;a href=&#34;https://jamesmccaffrey.wordpress.com/2019/10/28/roulette-wheel-selection-for-multi-armed-bandit-problems/&#34;&gt;blog post&lt;/a&gt; on a method for allocating turns in a multi-armed bandit problem.&lt;/p&gt;
&lt;p&gt;I really liked his post, and decided to take a look at the algorithm he described and code up a function to do the simulation in R.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; this is strictly an implementation of Dr. McCaffrey’s ideas from his blog post, and should not be taken as my own.&lt;/p&gt;
&lt;p&gt;You can find the .Rmd file for this post &lt;a href=&#34;https://github.com/bgstieber/bgstieber.github.io/blob/master/post/2019-10-29-roulette-wheels-for-multi-armed-bandits-a-simulation-in-r.en.Rmd&#34;&gt;on my GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;The basic idea of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Multi-armed_bandit&#34;&gt;multi-armed bandit&lt;/a&gt; is that you have a fixed number of resources (e.g. money at a casino) and you have a number of competing places where you can allocate those resources (e.g. four slot machines at the casino). These allocations occur sequentially, so in the casino example, we choose a slot machine, observe the success or failure from our play, and then make the next allocation decision. Since we’re data scientists at a casino, hopefully we’re using the information we’re gathering to make better gambling decisions (is that an oxymoron?).&lt;/p&gt;
&lt;p&gt;We want to choose the best place to allocate our resources, and maximize our reward for each allocation. However, we should shy away from a greedy strategy (just play the winner), because it doesn’t allow us to explore our other options.&lt;/p&gt;
&lt;p&gt;There are &lt;a href=&#34;https://en.wikipedia.org/wiki/Multi-armed_bandit&#34;&gt;different strategies&lt;/a&gt; for choosing where to allocate your next resource. One of the more popular choices is Thompson sampling, which usually involves sampling from a Beta distribution, and using the results of that sampling to determine your next allocation (out of scope for this blog post!).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code-roulette_wheel&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Code: &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/blob/master/miscellaneous/roulette_wheel.R&#34;&gt;&lt;code&gt;roulette_wheel&lt;/code&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The following function implements the roulette wheel allocation, for a flexible number of slot machines.&lt;/p&gt;
&lt;p&gt;The function starts by generating a warm start with the data. We need to gather information about our different slot machines, so we allocate a small number of resources to each one to collect information. After we do this, we start the real allocation. We pick a winner based on how its cumulative probability compares to a draw from a random uniform distribution.&lt;/p&gt;
&lt;p&gt;So, if our observed success probabilities are&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;machine&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;observed_prob&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cumulative_prob&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;selection_range&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.0-0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.2-0.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.5-1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And our draw from the random uniform was 0.7, we’d pick the third arm (0.7 falls between 0.5 and 1). This selection criteria is the main point of Dr. McCaffrey’s algorithm. For a better and more thorough explanation, I’d suggest reading his &lt;a href=&#34;https://jamesmccaffrey.wordpress.com/2019/10/28/roulette-wheel-selection-for-multi-armed-bandit-problems/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We then continue this process (playing a slot machine, observing the outcome, recalculating observed probabilities, and picking the next slot machine) until we run out of coins.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And here’s the code&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;roulette_wheel &amp;lt;- function(coins = 40, 
                           starts = 5,
                           true_prob = c(0.3, 0.5, 0.7)){
  # must have enough coins to generate initial empirical distribution
  if (coins &amp;lt; (length(true_prob) * starts)){
    stop(&amp;quot;To generate a starting distribution, each machine must be&amp;quot;,
         &amp;quot; played &amp;quot;,
         starts,
         &amp;quot; times - not enough coins to do so.&amp;quot;)
  }
  # allocate first (&amp;quot;warm up&amp;quot;)
  SS &amp;lt;- sapply(true_prob, FUN = function(x) sum(rbinom(starts, 1, x)))
  FF &amp;lt;- starts - SS
  # calculate metrics used for play allocation
  probs &amp;lt;- SS / (SS + FF)
  probs_normalized &amp;lt;- probs / sum(probs)
  cumu_probs_normalized &amp;lt;- cumsum(probs_normalized)
  # update number of coins
  coins &amp;lt;- coins - (length(true_prob) * starts)
  # create simulation data.frame
  sim_df &amp;lt;- data.frame(machine = seq_along(true_prob),
                       true_probabilities = true_prob,
                       observed_probs = probs,
                       successes = SS,
                       failures = FF,
                       plays = SS + FF,
                       machine_played = NA,
                       coins_left = coins)
  # initialize before while loop
  sim_list &amp;lt;- vector(&amp;#39;list&amp;#39;, length = coins)
  i &amp;lt;- 1
  # play until we run out of original coins
  while(coins &amp;gt; 0){
    # which machine to play?
    update_index &amp;lt;- findInterval(runif(1), c(0, cumu_probs_normalized))
    # play machine
    flip &amp;lt;- rbinom(1, 1, true_prob[update_index])
    # update successes and failure for machine that was played
    SS[update_index] &amp;lt;- SS[update_index] + flip
    FF[update_index] &amp;lt;- FF[update_index] + (1-flip)
    # update metrics used for play allocation
    probs &amp;lt;- SS / (SS + FF)
    probs_normalized &amp;lt;- probs / sum(probs)
    cumu_probs_normalized &amp;lt;- cumsum(probs_normalized)
    # update number of coins
    coins &amp;lt;- coins - 1    
    # update simulation data.frame (very inefficient)
    sim_list[[i]] &amp;lt;- data.frame(machine = seq_along(true_prob),
                                true_probabilities = true_prob,
                                observed_probs = probs,
                                successes = SS,
                                failures = FF,
                                plays = SS + FF,
                                machine_played = seq_along(true_prob) == update_index,
                                coins_left = coins)
    i &amp;lt;- i + 1
  }
  # show success:failure ratio
  message(&amp;quot;Success to failure ratio was &amp;quot;,
          round(sum(SS) / sum(FF), 2),
          &amp;quot;\n&amp;quot;,
          paste0(&amp;quot;(&amp;quot;, 
                 paste0(SS, collapse = &amp;quot;+&amp;quot;), 
                 &amp;quot;)/(&amp;quot;, 
                 paste0(FF, collapse = &amp;quot;+&amp;quot;), &amp;quot;)&amp;quot;))
  # return data frame of values from experiment
  rbind(sim_df, do.call(&amp;#39;rbind&amp;#39;, sim_list))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Analysis&lt;/h1&gt;
&lt;p&gt;I’ll show a brief example of what we can do with the data generated from this function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
rw1 &amp;lt;- roulette_wheel(coins = 5000, 
                      starts = 10, 
                      true_prob = c(0.1, 0.25, 0.5, 0.65))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Success to failure ratio was 1.06
## (15+228+835+1490)/(213+662+826+731)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-5&#34;&gt;Table 1: &lt;/span&gt;Final simulation result&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;machine&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;true_probabilities&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;observed_probs&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;successes&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;failures&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;plays&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;machine_played&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;coins_left&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0658&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;213&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;228&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2562&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;228&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;662&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;890&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5027&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;835&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;826&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1661&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.65&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6709&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1490&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;731&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2221&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s look at how the observed probabilities changed over time:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-29-roulette-wheels-for-multi-armed-bandits-a-simulation-in-r.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And how did our plays for each machine accumulate through time?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-29-roulette-wheels-for-multi-armed-bandits-a-simulation-in-r.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boring!&lt;/p&gt;
&lt;p&gt;Maybe if we run a smaller number of simulations, we might get a better sense of variation in our number of plays.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
rw2 &amp;lt;- roulette_wheel(coins = 100, 
                      starts = 5, 
                      true_prob = c(0.1, 0.3, 0.65))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Success to failure ratio was 0.82
## (1+16+28)/(11+26+18)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-9&#34;&gt;Table 2: &lt;/span&gt;Final simulation result&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;machine&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;true_probabilities&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;observed_probs&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;successes&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;failures&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;plays&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;machine_played&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;coins_left&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0833&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3810&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.65&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6087&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-29-roulette-wheels-for-multi-armed-bandits-a-simulation-in-r.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That shows our allocations a little bit better than the previous visualization.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This was a fun exercise for me, and it reminded me of a &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/blob/master/miscellaneous/MultiArmedBandits.pdf&#34;&gt;presentation&lt;/a&gt; I did in graduate school about a very similar topic. I also wrote a &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/blob/master/miscellaneous/roulette_wheel.py&#34;&gt;roulette wheel function&lt;/a&gt; in Python, and was moderately successful at that (it runs faster than my R function, but I’m less confident in how “pythonic” it is).&lt;/p&gt;
&lt;p&gt;My biggest concern with this implementation is the potential situation in which our warm start results in all failures for a given slot machine. If the machine fails across the warm start, it will not be selected for the rest of the simulation. To offset this, you could add a little “jitter” (technical term: epsilon) to the observed probabilities at each iteration. Another option would be to generate a second random uniform variable, and if that value is very small, you that pull a random lever, rather than the one determined by the simulation.&lt;/p&gt;
&lt;p&gt;Finally, I’d be interested in comparing the statistical properties of this algorithm and others that are used in sequential allocation problems…if I have the time.&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded><p>One of my favorite <a href="https://jamesmccaffrey.wordpress.com/">data science blogs</a> comes from James McCaffrey, a software engineer and researcher at Microsoft. He recently wrote a <a href="https://jamesmccaffrey.wordpress.com/2019/10/28/roulette-wheel-selection-for-multi-armed-bandit-problems/">blog post</a> on a method for allocating turns in a multi-armed bandit problem.</p>
<p>I really liked his post, and decided to take a look at the algorithm he described and code up a function to do the simulation in R.</p>
<p><strong>Note:</strong> this is strictly an implementation of Dr. McCaffrey’s ideas from his blog post, and should not be taken as my own.</p>
<p>You can find the .Rmd file for this post <a href="https://github.com/bgstieber/bgstieber.github.io/blob/master/post/2019-10-29-roulette-wheels-for-multi-armed-bandits-a-simulation-in-r.en.Rmd">on my GitHub</a>.</p>
<div id="background" class="section level1">
<h1>Background</h1>
<p>The basic idea of a <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandit</a> is that you have a fixed number of resources (e.g. money at a casino) and you have a number of competing places where you can allocate those resources (e.g. four slot machines at the casino). These allocations occur sequentially, so in the casino example, we choose a slot machine, observe the success or failure from our play, and then make the next allocation decision. Since we’re data scientists at a casino, hopefully we’re using the information we’re gathering to make better gambling decisions (is that an oxymoron?).</p>
<p>We want to choose the best place to allocate our resources, and maximize our reward for each allocation. However, we should shy away from a greedy strategy (just play the winner), because it doesn’t allow us to explore our other options.</p>
<p>There are <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">different strategies</a> for choosing where to allocate your next resource. One of the more popular choices is Thompson sampling, which usually involves sampling from a Beta distribution, and using the results of that sampling to determine your next allocation (out of scope for this blog post!).</p>
</div>
<div id="code-roulette_wheel" class="section level1">
<h1>Code: <a href="https://github.com/bgstieber/files_for_blog/blob/master/miscellaneous/roulette_wheel.R"><code>roulette_wheel</code></a></h1>
<p>The following function implements the roulette wheel allocation, for a flexible number of slot machines.</p>
<p>The function starts by generating a warm start with the data. We need to gather information about our different slot machines, so we allocate a small number of resources to each one to collect information. After we do this, we start the real allocation. We pick a winner based on how its cumulative probability compares to a draw from a random uniform distribution.</p>
<p>So, if our observed success probabilities are</p>
<table>
<thead>
<tr class="header">
<th align="right">machine</th>
<th align="right">observed_prob</th>
<th align="right">cumulative_prob</th>
<th align="left">selection_range</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.2</td>
<td align="right">0.2</td>
<td align="left">0.0-0.2</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.3</td>
<td align="right">0.5</td>
<td align="left">0.2-0.5</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">0.5</td>
<td align="right">1.0</td>
<td align="left">0.5-1.0</td>
</tr>
</tbody>
</table>
<p>And our draw from the random uniform was 0.7, we’d pick the third arm (0.7 falls between 0.5 and 1). This selection criteria is the main point of Dr. McCaffrey’s algorithm. For a better and more thorough explanation, I’d suggest reading his <a href="https://jamesmccaffrey.wordpress.com/2019/10/28/roulette-wheel-selection-for-multi-armed-bandit-problems/">blog post</a>.</p>
<p>We then continue this process (playing a slot machine, observing the outcome, recalculating observed probabilities, and picking the next slot machine) until we run out of coins.</p>
<p><strong>And here’s the code</strong></p>
<pre class="r"><code>roulette_wheel &lt;- function(coins = 40, 
                           starts = 5,
                           true_prob = c(0.3, 0.5, 0.7)){
  # must have enough coins to generate initial empirical distribution
  if (coins &lt; (length(true_prob) * starts)){
    stop(&quot;To generate a starting distribution, each machine must be&quot;,
         &quot; played &quot;,
         starts,
         &quot; times - not enough coins to do so.&quot;)
  }
  # allocate first (&quot;warm up&quot;)
  SS &lt;- sapply(true_prob, FUN = function(x) sum(rbinom(starts, 1, x)))
  FF &lt;- starts - SS
  # calculate metrics used for play allocation
  probs &lt;- SS / (SS + FF)
  probs_normalized &lt;- probs / sum(probs)
  cumu_probs_normalized &lt;- cumsum(probs_normalized)
  # update number of coins
  coins &lt;- coins - (length(true_prob) * starts)
  # create simulation data.frame
  sim_df &lt;- data.frame(machine = seq_along(true_prob),
                       true_probabilities = true_prob,
                       observed_probs = probs,
                       successes = SS,
                       failures = FF,
                       plays = SS + FF,
                       machine_played = NA,
                       coins_left = coins)
  # initialize before while loop
  sim_list &lt;- vector(&#39;list&#39;, length = coins)
  i &lt;- 1
  # play until we run out of original coins
  while(coins &gt; 0){
    # which machine to play?
    update_index &lt;- findInterval(runif(1), c(0, cumu_probs_normalized))
    # play machine
    flip &lt;- rbinom(1, 1, true_prob[update_index])
    # update successes and failure for machine that was played
    SS[update_index] &lt;- SS[update_index] + flip
    FF[update_index] &lt;- FF[update_index] + (1-flip)
    # update metrics used for play allocation
    probs &lt;- SS / (SS + FF)
    probs_normalized &lt;- probs / sum(probs)
    cumu_probs_normalized &lt;- cumsum(probs_normalized)
    # update number of coins
    coins &lt;- coins - 1    
    # update simulation data.frame (very inefficient)
    sim_list[[i]] &lt;- data.frame(machine = seq_along(true_prob),
                                true_probabilities = true_prob,
                                observed_probs = probs,
                                successes = SS,
                                failures = FF,
                                plays = SS + FF,
                                machine_played = seq_along(true_prob) == update_index,
                                coins_left = coins)
    i &lt;- i + 1
  }
  # show success:failure ratio
  message(&quot;Success to failure ratio was &quot;,
          round(sum(SS) / sum(FF), 2),
          &quot;\n&quot;,
          paste0(&quot;(&quot;, 
                 paste0(SS, collapse = &quot;+&quot;), 
                 &quot;)/(&quot;, 
                 paste0(FF, collapse = &quot;+&quot;), &quot;)&quot;))
  # return data frame of values from experiment
  rbind(sim_df, do.call(&#39;rbind&#39;, sim_list))
}</code></pre>
</div>
<div id="data-analysis" class="section level1">
<h1>Data Analysis</h1>
<p>I’ll show a brief example of what we can do with the data generated from this function.</p>
<pre class="r"><code>set.seed(123)
rw1 &lt;- roulette_wheel(coins = 5000, 
                      starts = 10, 
                      true_prob = c(0.1, 0.25, 0.5, 0.65))</code></pre>
<pre><code>## Success to failure ratio was 1.06
## (15+228+835+1490)/(213+662+826+731)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-5">Table 1: </span>Final simulation result</caption>
<thead>
<tr class="header">
<th align="right">machine</th>
<th align="right">true_probabilities</th>
<th align="right">observed_probs</th>
<th align="right">successes</th>
<th align="right">failures</th>
<th align="right">plays</th>
<th align="left">machine_played</th>
<th align="right">coins_left</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.10</td>
<td align="right">0.0658</td>
<td align="right">15</td>
<td align="right">213</td>
<td align="right">228</td>
<td align="left">FALSE</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.25</td>
<td align="right">0.2562</td>
<td align="right">228</td>
<td align="right">662</td>
<td align="right">890</td>
<td align="left">FALSE</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">0.50</td>
<td align="right">0.5027</td>
<td align="right">835</td>
<td align="right">826</td>
<td align="right">1661</td>
<td align="left">FALSE</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.65</td>
<td align="right">0.6709</td>
<td align="right">1490</td>
<td align="right">731</td>
<td align="right">2221</td>
<td align="left">TRUE</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>Let’s look at how the observed probabilities changed over time:</p>
<p><img src="/post/2019-10-29-roulette-wheels-for-multi-armed-bandits-a-simulation-in-r.en_files/figure-html/unnamed-chunk-6-1.png" width="768" /></p>
<p>And how did our plays for each machine accumulate through time?</p>
<p><img src="/post/2019-10-29-roulette-wheels-for-multi-armed-bandits-a-simulation-in-r.en_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Boring!</p>
<p>Maybe if we run a smaller number of simulations, we might get a better sense of variation in our number of plays.</p>
<pre class="r"><code>set.seed(1)
rw2 &lt;- roulette_wheel(coins = 100, 
                      starts = 5, 
                      true_prob = c(0.1, 0.3, 0.65))</code></pre>
<pre><code>## Success to failure ratio was 0.82
## (1+16+28)/(11+26+18)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-9">Table 2: </span>Final simulation result</caption>
<thead>
<tr class="header">
<th align="right">machine</th>
<th align="right">true_probabilities</th>
<th align="right">observed_probs</th>
<th align="right">successes</th>
<th align="right">failures</th>
<th align="right">plays</th>
<th align="left">machine_played</th>
<th align="right">coins_left</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.10</td>
<td align="right">0.0833</td>
<td align="right">1</td>
<td align="right">11</td>
<td align="right">12</td>
<td align="left">FALSE</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.30</td>
<td align="right">0.3810</td>
<td align="right">16</td>
<td align="right">26</td>
<td align="right">42</td>
<td align="left">FALSE</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">0.65</td>
<td align="right">0.6087</td>
<td align="right">28</td>
<td align="right">18</td>
<td align="right">46</td>
<td align="left">TRUE</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><img src="/post/2019-10-29-roulette-wheels-for-multi-armed-bandits-a-simulation-in-r.en_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>That shows our allocations a little bit better than the previous visualization.</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>This was a fun exercise for me, and it reminded me of a <a href="https://github.com/bgstieber/files_for_blog/blob/master/miscellaneous/MultiArmedBandits.pdf">presentation</a> I did in graduate school about a very similar topic. I also wrote a <a href="https://github.com/bgstieber/files_for_blog/blob/master/miscellaneous/roulette_wheel.py">roulette wheel function</a> in Python, and was moderately successful at that (it runs faster than my R function, but I’m less confident in how “pythonic” it is).</p>
<p>My biggest concern with this implementation is the potential situation in which our warm start results in all failures for a given slot machine. If the machine fails across the warm start, it will not be selected for the rest of the simulation. To offset this, you could add a little “jitter” (technical term: epsilon) to the observed probabilities at each iteration. Another option would be to generate a second random uniform variable, and if that value is very small, you that pull a random lever, rather than the one determined by the simulation.</p>
<p>Finally, I’d be interested in comparing the statistical properties of this algorithm and others that are used in sequential allocation problems…if I have the time.</p>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>Recommending Songs Using Cosine Similarity in R</title>
      <link>/post/recommending-songs-using-cosine-similarity-in-r/</link>
      <pubDate>December 31, 2018</pubDate>
      
      <guid>/post/recommending-songs-using-cosine-similarity-in-r/</guid>
      <description>&lt;p&gt;Recommendation engines have a huge impact on our online lives. The content we watch on Netflix, the products we purchase on Amazon, and even &lt;a href=&#34;https://www.redfin.com/blog/2013/09/the-end-of-search.html&#34;&gt;the homes we buy&lt;/a&gt; are all served up using these algorithms. In this post, I’ll run through one of the key metrics used in developing recommendation engines: &lt;strong&gt;cosine similarity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, I’ll give a brief overview of some vocabulary we’ll need to understand recommendation systems. Then, I’ll look at the math behind cosine similarity. Finally, I’m going to use cosine similarity to build a recommendation engine for songs in R.&lt;/p&gt;
&lt;div id=&#34;the-basics-recommendation-engine-vocabulary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Basics: Recommendation Engine Vocabulary&lt;/h2&gt;
&lt;p&gt;There are a few different flavors of recommendation engines. One type is &lt;strong&gt;collaborative filtering&lt;/strong&gt;, which relies on the behavior of users to understand and predict the similarity between items. There are two subtypes of collaborative filtering: &lt;strong&gt;user-user&lt;/strong&gt; and &lt;strong&gt;item-item&lt;/strong&gt;. In a nutshell, user-user engines will look for similar users to you, and suggest things that these users have liked (&lt;em&gt;users like you also bought X&lt;/em&gt;). Item-item recommendation engines generate suggestions based on the similarity of items instead of the similarity of users (&lt;em&gt;you bought X and Y, maybe you’d like Z too&lt;/em&gt;). Converting an engine from user-user to item-item can reduce the computational cost of generating recommendations.&lt;/p&gt;
&lt;p&gt;Another type of recommendation engine is &lt;strong&gt;content-based&lt;/strong&gt;. Rather than using the behavior of other users or the similarity between ratings, content-based systems employ information about the items themselves (e.g. genre, starring actors, or when the movie was released). Then, a user’s behavior is examined to generate a user profile, which tries to find content similar to what’s been consumed before based on the characteristics of the content.&lt;/p&gt;
&lt;p&gt;Cosine similarity is helpful for building both types of recommender systems, as it provides a way of measuring how similar users, items, or content is. In this post, we’ll be using it to generate song recommendations based on how often users listen to different songs.&lt;/p&gt;
&lt;p&gt;The only package we’ll need for this post is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-math-cosine-similarity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Math: Cosine Similarity&lt;/h2&gt;
&lt;p&gt;Cosine similarity is built on the geometric definition of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dot_product&#34;&gt;&lt;strong&gt;dot product&lt;/strong&gt;&lt;/a&gt; of two vectors:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{dot product}(a, b) =a \cdot b = a^{T}b = \sum_{i=1}^{n} a_i b_i \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You may be wondering what &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; actually represent. If we’re trying to recommend certain products, &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; might be the collection of ratings for two products based on the input from &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; customers. For example, if &lt;span class=&#34;math inline&#34;&gt;\(a =[5, 0, 1]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b = [0, 1, 2]\)&lt;/span&gt;, the first customer rated &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; a 5 and did not rate &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, the second customer did not rate &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and gave &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; a 1, and the third customer rated &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; a 1 and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; a 2.&lt;/p&gt;
&lt;p&gt;With that out of the way, we can layer in geometric information&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a \cdot b = \Vert a \Vert \Vert b \Vert \text{cos}(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the angle between &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Vert x \Vert\)&lt;/span&gt; is the magnitude/length/norm of a vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. From the above expression, we can arrive at cosine similarity:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{cosine similarity} = \text{cos}(\theta) = \frac{a \cdot b}{\Vert a \Vert \Vert b \Vert}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;R&lt;/code&gt; this is defined as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cosine_sim &amp;lt;- function(a, b) crossprod(a,b)/sqrt(crossprod(a)*crossprod(b))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK, OK, OK, you’ve seen the formula, and I even wrote an &lt;code&gt;R&lt;/code&gt; function, but where’s the intuition? What does it all mean?&lt;/p&gt;
&lt;p&gt;What I like to focus on in cosine similarity is the angle &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; tells us how far we’d have to move vector &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; so that it could rest on top of &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. This assumes we can only adjust the orientation of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, and have no ability to influence its magnitude. The easier it is to get &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; on top of &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, the smaller this angle will be, and the more similar &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; will be. Furthermore, the smaller &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is, the larger &lt;span class=&#34;math inline&#34;&gt;\(\text{cos}(\theta)\)&lt;/span&gt; will be. &lt;a href=&#34;http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/&#34;&gt;This blog post&lt;/a&gt; has a great image demonstrating cosine similarity for a few examples.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png&#34; alt=&#34;Image from a 2013 blog post by Christian S. Perone&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Image from a 2013 &lt;a href=&#34;http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/&#34;&gt;blog post&lt;/a&gt; by Christian S. Perone&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;For the data we’ll be looking at in this post, &lt;span class=&#34;math inline&#34;&gt;\(\text{cos}(\theta)\)&lt;/span&gt; will be somewhere between 0 and 1, since user play data is all non-negative. A value of 1 will indicate perfect similarity, and 0 will indicate the two vectors are unrelated. In other applications, there may be data which is positive &lt;em&gt;and&lt;/em&gt; negative. For these cases, &lt;span class=&#34;math inline&#34;&gt;\(\text{cos}(\theta)\)&lt;/span&gt; will be between -1 and 1, with -1 meaning &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; are perfectly dissimilar and 1 meaning &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; are perfectly similar.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data-songs-from-the-million-song-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Data: Songs from the Million Song Dataset&lt;/h2&gt;
&lt;p&gt;We use a subset of the data from the &lt;a href=&#34;https://labrosa.ee.columbia.edu/millionsong/&#34;&gt;Million Song Dataset&lt;/a&gt;. The data only has 10K songs, but that should be enough for this exercise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# read user play data and song data from the internet
play_data &amp;lt;- &amp;quot;https://static.turi.com/datasets/millionsong/10000.txt&amp;quot; %&amp;gt;%
  read_tsv(col_names = c(&amp;#39;user&amp;#39;, &amp;#39;song_id&amp;#39;, &amp;#39;plays&amp;#39;))

song_data &amp;lt;- &amp;#39;https://static.turi.com/datasets/millionsong/song_data.csv&amp;#39; %&amp;gt;%
  read_csv() %&amp;gt;%
  distinct(song_id, title, artist_name)
# join user and song data together
all_data &amp;lt;- play_data %&amp;gt;%
  group_by(user, song_id) %&amp;gt;%
  summarise(plays = sum(plays, na.rm = TRUE)) %&amp;gt;%
  inner_join(song_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the first few rows of the data. The important variable is &lt;code&gt;plays&lt;/code&gt;, which measures how many times a certain user has listened to a song. We’ll be using this variable to generate recommendations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(all_data, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;user&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;song_id&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;plays&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;title&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;artist_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;00003a4459f33b92906be11abe0e93efc423c0ff&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SOJJRVI12A6D4FBE49&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Only You (Illuminate Album Version)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;David Crowder*Band&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;00003a4459f33b92906be11abe0e93efc423c0ff&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SOKJWZB12A6D4F9487&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Do You Want To Know Love (Pray For Rain Album Version)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PFR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;00003a4459f33b92906be11abe0e93efc423c0ff&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SOMZHIH12A8AE45D00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;You’re A Wolf (Album)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Sea Wolf&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;00003a4459f33b92906be11abe0e93efc423c0ff&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SONFEUF12AAF3B47E3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Não É Proibido&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Marisa Monte&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are 76,353 users in this data set, so combining the number of users with songs makes the data a little too unwieldy for this toy example. I’m going to filter our dataset so that it’s only based on the 1,000 most-played songs. We use the &lt;code&gt;spread&lt;/code&gt; function to turn our data from being “tall” (one row per user per song) to being “wide” (one row per user, and one column per song).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_1k_songs &amp;lt;- all_data %&amp;gt;%
    group_by(song_id, title, artist_name) %&amp;gt;%
    summarise(sum_plays = sum(plays)) %&amp;gt;%
    ungroup() %&amp;gt;%
    top_n(1000, sum_plays) %&amp;gt;% 
    distinct(song_id)

all_data_top_1k &amp;lt;- all_data %&amp;gt;%
  inner_join(top_1k_songs)

top_1k_wide &amp;lt;- all_data_top_1k %&amp;gt;%
    ungroup() %&amp;gt;%
    distinct(user, song_id, plays) %&amp;gt;%
    spread(song_id, plays, fill = 0)

ratings &amp;lt;- as.matrix(top_1k_wide[,-1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This results in having play data for 70,345 users and 994 songs. 1.05% of user-song combinations have &lt;code&gt;plays&lt;/code&gt; greater than 0.&lt;/p&gt;
&lt;p&gt;Here’s a sample of what the &lt;code&gt;ratings&lt;/code&gt; matrix looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ratings[1:5, 1:3] # one row per user, one column per song&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      SOAAVUV12AB0186646 SOABHYV12A6D4F6D0F SOABJBU12A8C13F63F
## [1,]                  0                  0                  0
## [2,]                  0                  0                  0
## [3,]                  0                  0                  0
## [4,]                  0                  0                  0
## [5,]                  0                  0                  0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-result-making-song-recommendations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Result: Making Song Recommendations&lt;/h2&gt;
&lt;p&gt;I wrote a function called &lt;code&gt;calc_cos_sim&lt;/code&gt;, which will calculate the similarity between a chosen song and the other songs, and recommend 5 new songs for a user to listen to. From start to finish, this only took about 20 lines of code, indicating how easy it can be to spin up a recommendation engine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_cos_sim &amp;lt;- function(song_code, 
                         rating_mat = ratings,
                         songs = song_data,
                         return_n = 5) {
  # find our song
  song_col_index &amp;lt;- which(colnames(rating_mat) == song_code)
  # calculate cosine similarity for each song based on 
  # number of plays for users
  # apply(..., 2) iterates over the columns of a matrix
  cos_sims &amp;lt;- apply(rating_mat, 2,
                    FUN = function(y) 
                      cosine_sim(rating_mat[,song_col_index], y))
  # return results
  data_frame(song_id = names(cos_sims), cos_sim = cos_sims) %&amp;gt;%
    filter(song_id != song_code) %&amp;gt;% # remove self reference
    inner_join(songs) %&amp;gt;%
    arrange(desc(cos_sim)) %&amp;gt;%
    top_n(return_n, cos_sim) %&amp;gt;%
    select(song_id, title, artist_name, cos_sim)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the function above to calculate similarities and generate recommendations for a few songs.&lt;/p&gt;
&lt;p&gt;Let’s look at the hip-hop classic &lt;a href=&#34;https://www.youtube.com/watch?v=QFcv5Ma8u8k&#34;&gt;“Forgot about Dre”&lt;/a&gt; first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forgot_about_dre &amp;lt;- &amp;#39;SOPJLFV12A6701C797&amp;#39;
knitr::kable(calc_cos_sim(forgot_about_dre))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;song_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;title&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;artist_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cos_sim&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOZCWQA12A6701C798&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The Next Episode&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Dr. Dre / Snoop Dogg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3561683&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOHEMBB12A6701E907&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Superman&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Eminem / Dina Rae&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2507195&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOWGXOP12A6701E93A&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Without Me&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Eminem&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1596885&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOJTDUS12A6D4FBF0E&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;None Shall Pass (Main)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aesop Rock&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1591929&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOSKDTM12A6701C795&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;What’s The Difference&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Dr. Dre / Eminem / Alvin Joiner&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1390476&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Each song we recommended is a hip-hop song, which is a good start! Even on this reduced dataset, the engine is making &lt;em&gt;decent&lt;/em&gt; recommendations.&lt;/p&gt;
&lt;!--

```r
enter_sandman &lt;- &#39;SOCHYVZ12A6D4F5908&#39;
knitr::kable(calc_cos_sim(enter_sandman))
```



song_id              title                         artist_name                  cos_sim
-------------------  ----------------------------  ------------------------  ----------
SOLMIUU12A58A79C99   Another Day In Paradise       Phil Collins               0.2211738
SOYGHUM12AB018139C   Bad Company                   Five Finger Death Punch    0.2190192
SOZDGEW12A8C13E748   One                           Metallica                  0.2104946
SOUBXSF12A6701D23C   You Could Be Mine             Guns N&#39; Roses              0.1289000
SOIPYPB12A8C1360D4   My Immortal (Album Version)   Evanescence                0.1175952
SOIPYPB12A8C1360D4   My Immortal                   Evanescence                0.1175952
--&gt;
&lt;p&gt;The next song is &lt;a href=&#34;https://www.youtube.com/watch?v=vabnZ9-ex7o&#34;&gt;“Come As You Are” by Nirvana&lt;/a&gt;. Users who like this song probably listen to other grunge/rock songs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;come_as_you_are &amp;lt;- &amp;#39;SODEOCO12A6701E922&amp;#39;
knitr::kable(calc_cos_sim(come_as_you_are))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;song_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;title&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;artist_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cos_sim&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOCPMIK12A6701E96D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The Man Who Sold The World&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Nirvana&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3903533&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SONNNEH12AB01827DE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Lithium&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Nirvana&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3568732&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOLOFYI12A8C145F8D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Heart Shaped Box&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Nirvana&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1958162&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOVDLVT12A58A7B988&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Behind Blue Eyes&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Limp Bizkit&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1186160&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOWBYZF12A6D4F9424&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Fakty&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Horkyze Slyze&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0952245&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Alright, 2 for 2. One thing to be mindful of when looking at these results is that we’re not incorporating &lt;em&gt;any&lt;/em&gt; information about the songs themselves. Our engine isn’t built using any data about the artist, genre, or other musical characteristics. Additionally, we’re not considering any demographic information about the users, and it’s fairly easy to see how useful age, gender, and other user-level data could be in making recommendations. If we used this information in addition to our user play data, we’d have what is called a &lt;a href=&#34;https://www.math.uci.edu/icamp/courses/math77b/lecture_12w/pdfs/Chapter%2005%20-%20Hybrid%20recommendation%20approaches.pdf&#34;&gt;&lt;strong&gt;hybrid recommendation system&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we’ll recommend songs for our hard-partying friends that like the song &lt;a href=&#34;https://www.youtube.com/watch?v=XNtTEibFvlQ&#34;&gt;“Shots” by LMFAO featuring Lil Jon&lt;/a&gt; (&lt;strong&gt;that video is not for the faint of heart&lt;/strong&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shots &amp;lt;- &amp;#39;SOJYBJZ12AB01801D0&amp;#39;
knitr::kable(calc_cos_sim(shots))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;song_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;title&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;artist_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cos_sim&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOWEHOM12A6BD4E09E&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;16 Candles&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The Crests&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2551851&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOLQXDJ12AB0182E47&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;LMFAO&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1866648&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOSZJFV12AB01878CB&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Teach Me How To Dougie&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;California Swag District&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1387647&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOYGKNI12AB0187E6E&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;All I Do Is Win (feat. T-Pain_ Ludacris_ Snoop Dogg &amp;amp; Rick Ross)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DJ Khaled&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1173063&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOUSMXX12AB0185C24&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;OMG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Usher featuring will.i.am&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1012716&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Well, the “16 Candles” result is a little surprising, but this might give us some insight into the demographics of users that like “Shots”. The other four recommendations seem pretty solid, I guess.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Cosine similarity is simple to calculate and is fairly intuitive once some basic geometric concepts are understood. The simplicity of this metric makes it a great first-pass option for recommendation systems, and can be treated as a baseline with which to compare more computationally intensive and/or difficult to understand methods.&lt;/p&gt;
&lt;p&gt;I think that recommendation systems will continue to play a large role in our online lives. It can be helpful to understand the components underneath these systems, so that we treat them less as blackbox oracles and more as the imperfect prediction systems based on data they are.&lt;/p&gt;
&lt;p&gt;I hope you liked this brief excursion into the world of recommendation engines. Hopefully you can walk away knowing a little more about why Amazon, Netflix, and other platforms recommend the content they do.&lt;/p&gt;
&lt;div id=&#34;other-resources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other Resources&lt;/h3&gt;
&lt;p&gt;Here are a few great resources if you want to dive deeper into recommendation systems and cosine similarity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/&#34;&gt;Machine Learning :: Cosine Similarity for Vector Space Models (Part III)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://stefansavev.com/blog/cosine-similarity-all-posts/&#34;&gt;Series of blog posts about cosine similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cosine_similarity&#34;&gt;Wikipedia: Cosine Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://masongallo.github.io/machine/learning,/python/2016/07/29/cosine-similarity.html&#34;&gt;Implementing and Understanding Cosine Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/what-are-product-recommendation-engines-and-the-various-versions-of-them-9dcab4ee26d5&#34;&gt;What are Product Recommendation Engines? And the various versions of them?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hackernoon.com/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75&#34;&gt;Introduction to Collaborative Filtering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      <content:encoded><p>Recommendation engines have a huge impact on our online lives. The content we watch on Netflix, the products we purchase on Amazon, and even <a href="https://www.redfin.com/blog/2013/09/the-end-of-search.html">the homes we buy</a> are all served up using these algorithms. In this post, I’ll run through one of the key metrics used in developing recommendation engines: <strong>cosine similarity</strong>.</p>
<p>First, I’ll give a brief overview of some vocabulary we’ll need to understand recommendation systems. Then, I’ll look at the math behind cosine similarity. Finally, I’m going to use cosine similarity to build a recommendation engine for songs in R.</p>
<div id="the-basics-recommendation-engine-vocabulary" class="section level2">
<h2>The Basics: Recommendation Engine Vocabulary</h2>
<p>There are a few different flavors of recommendation engines. One type is <strong>collaborative filtering</strong>, which relies on the behavior of users to understand and predict the similarity between items. There are two subtypes of collaborative filtering: <strong>user-user</strong> and <strong>item-item</strong>. In a nutshell, user-user engines will look for similar users to you, and suggest things that these users have liked (<em>users like you also bought X</em>). Item-item recommendation engines generate suggestions based on the similarity of items instead of the similarity of users (<em>you bought X and Y, maybe you’d like Z too</em>). Converting an engine from user-user to item-item can reduce the computational cost of generating recommendations.</p>
<p>Another type of recommendation engine is <strong>content-based</strong>. Rather than using the behavior of other users or the similarity between ratings, content-based systems employ information about the items themselves (e.g. genre, starring actors, or when the movie was released). Then, a user’s behavior is examined to generate a user profile, which tries to find content similar to what’s been consumed before based on the characteristics of the content.</p>
<p>Cosine similarity is helpful for building both types of recommender systems, as it provides a way of measuring how similar users, items, or content is. In this post, we’ll be using it to generate song recommendations based on how often users listen to different songs.</p>
<p>The only package we’ll need for this post is:</p>
<pre class="r"><code>library(tidyverse)</code></pre>
</div>
<div id="the-math-cosine-similarity" class="section level2">
<h2>The Math: Cosine Similarity</h2>
<p>Cosine similarity is built on the geometric definition of the <a href="https://en.wikipedia.org/wiki/Dot_product"><strong>dot product</strong></a> of two vectors:</p>
<p><span class="math display">\[\text{dot product}(a, b) =a \cdot b = a^{T}b = \sum_{i=1}^{n} a_i b_i \]</span></p>
<p>You may be wondering what <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> actually represent. If we’re trying to recommend certain products, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> might be the collection of ratings for two products based on the input from <span class="math inline">\(n\)</span> customers. For example, if <span class="math inline">\(a =[5, 0, 1]\)</span> and <span class="math inline">\(b = [0, 1, 2]\)</span>, the first customer rated <span class="math inline">\(a\)</span> a 5 and did not rate <span class="math inline">\(b\)</span>, the second customer did not rate <span class="math inline">\(a\)</span> and gave <span class="math inline">\(b\)</span> a 1, and the third customer rated <span class="math inline">\(a\)</span> a 1 and <span class="math inline">\(b\)</span> a 2.</p>
<p>With that out of the way, we can layer in geometric information</p>
<p><span class="math display">\[a \cdot b = \Vert a \Vert \Vert b \Vert \text{cos}(\theta)\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and <span class="math inline">\(\Vert x \Vert\)</span> is the magnitude/length/norm of a vector <span class="math inline">\(x\)</span>. From the above expression, we can arrive at cosine similarity:</p>
<p><span class="math display">\[\text{cosine similarity} = \text{cos}(\theta) = \frac{a \cdot b}{\Vert a \Vert \Vert b \Vert}\]</span></p>
<p>In <code>R</code> this is defined as:</p>
<pre class="r"><code>cosine_sim &lt;- function(a, b) crossprod(a,b)/sqrt(crossprod(a)*crossprod(b))</code></pre>
<p>OK, OK, OK, you’ve seen the formula, and I even wrote an <code>R</code> function, but where’s the intuition? What does it all mean?</p>
<p>What I like to focus on in cosine similarity is the angle <span class="math inline">\(\theta\)</span>. <span class="math inline">\(\theta\)</span> tells us how far we’d have to move vector <span class="math inline">\(a\)</span> so that it could rest on top of <span class="math inline">\(b\)</span>. This assumes we can only adjust the orientation of <span class="math inline">\(a\)</span>, and have no ability to influence its magnitude. The easier it is to get <span class="math inline">\(a\)</span> on top of <span class="math inline">\(b\)</span>, the smaller this angle will be, and the more similar <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> will be. Furthermore, the smaller <span class="math inline">\(\theta\)</span> is, the larger <span class="math inline">\(\text{cos}(\theta)\)</span> will be. <a href="http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/">This blog post</a> has a great image demonstrating cosine similarity for a few examples.</p>
<p><br></p>
<div class="figure">
<img src="http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png" alt="Image from a 2013 blog post by Christian S. Perone" />
<p class="caption"><em>Image from a 2013 <a href="http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/">blog post</a> by Christian S. Perone</em></p>
</div>
<p><br></p>
<p>For the data we’ll be looking at in this post, <span class="math inline">\(\text{cos}(\theta)\)</span> will be somewhere between 0 and 1, since user play data is all non-negative. A value of 1 will indicate perfect similarity, and 0 will indicate the two vectors are unrelated. In other applications, there may be data which is positive <em>and</em> negative. For these cases, <span class="math inline">\(\text{cos}(\theta)\)</span> will be between -1 and 1, with -1 meaning <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are perfectly dissimilar and 1 meaning <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are perfectly similar.</p>
</div>
<div id="the-data-songs-from-the-million-song-dataset" class="section level2">
<h2>The Data: Songs from the Million Song Dataset</h2>
<p>We use a subset of the data from the <a href="https://labrosa.ee.columbia.edu/millionsong/">Million Song Dataset</a>. The data only has 10K songs, but that should be enough for this exercise.</p>
<pre class="r"><code># read user play data and song data from the internet
play_data &lt;- &quot;https://static.turi.com/datasets/millionsong/10000.txt&quot; %&gt;%
  read_tsv(col_names = c(&#39;user&#39;, &#39;song_id&#39;, &#39;plays&#39;))

song_data &lt;- &#39;https://static.turi.com/datasets/millionsong/song_data.csv&#39; %&gt;%
  read_csv() %&gt;%
  distinct(song_id, title, artist_name)
# join user and song data together
all_data &lt;- play_data %&gt;%
  group_by(user, song_id) %&gt;%
  summarise(plays = sum(plays, na.rm = TRUE)) %&gt;%
  inner_join(song_data)</code></pre>
<p>Here are the first few rows of the data. The important variable is <code>plays</code>, which measures how many times a certain user has listened to a song. We’ll be using this variable to generate recommendations.</p>
<pre class="r"><code>knitr::kable(head(all_data, 4))</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">user</th>
<th align="left">song_id</th>
<th align="right">plays</th>
<th align="left">title</th>
<th align="left">artist_name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">00003a4459f33b92906be11abe0e93efc423c0ff</td>
<td align="left">SOJJRVI12A6D4FBE49</td>
<td align="right">1</td>
<td align="left">Only You (Illuminate Album Version)</td>
<td align="left">David Crowder*Band</td>
</tr>
<tr class="even">
<td align="left">00003a4459f33b92906be11abe0e93efc423c0ff</td>
<td align="left">SOKJWZB12A6D4F9487</td>
<td align="right">4</td>
<td align="left">Do You Want To Know Love (Pray For Rain Album Version)</td>
<td align="left">PFR</td>
</tr>
<tr class="odd">
<td align="left">00003a4459f33b92906be11abe0e93efc423c0ff</td>
<td align="left">SOMZHIH12A8AE45D00</td>
<td align="right">3</td>
<td align="left">You’re A Wolf (Album)</td>
<td align="left">Sea Wolf</td>
</tr>
<tr class="even">
<td align="left">00003a4459f33b92906be11abe0e93efc423c0ff</td>
<td align="left">SONFEUF12AAF3B47E3</td>
<td align="right">3</td>
<td align="left">Não É Proibido</td>
<td align="left">Marisa Monte</td>
</tr>
</tbody>
</table>
<p>There are 76,353 users in this data set, so combining the number of users with songs makes the data a little too unwieldy for this toy example. I’m going to filter our dataset so that it’s only based on the 1,000 most-played songs. We use the <code>spread</code> function to turn our data from being “tall” (one row per user per song) to being “wide” (one row per user, and one column per song).</p>
<pre class="r"><code>top_1k_songs &lt;- all_data %&gt;%
    group_by(song_id, title, artist_name) %&gt;%
    summarise(sum_plays = sum(plays)) %&gt;%
    ungroup() %&gt;%
    top_n(1000, sum_plays) %&gt;% 
    distinct(song_id)

all_data_top_1k &lt;- all_data %&gt;%
  inner_join(top_1k_songs)

top_1k_wide &lt;- all_data_top_1k %&gt;%
    ungroup() %&gt;%
    distinct(user, song_id, plays) %&gt;%
    spread(song_id, plays, fill = 0)

ratings &lt;- as.matrix(top_1k_wide[,-1])</code></pre>
<p>This results in having play data for 70,345 users and 994 songs. 1.05% of user-song combinations have <code>plays</code> greater than 0.</p>
<p>Here’s a sample of what the <code>ratings</code> matrix looks like:</p>
<pre class="r"><code>ratings[1:5, 1:3] # one row per user, one column per song</code></pre>
<pre><code>##      SOAAVUV12AB0186646 SOABHYV12A6D4F6D0F SOABJBU12A8C13F63F
## [1,]                  0                  0                  0
## [2,]                  0                  0                  0
## [3,]                  0                  0                  0
## [4,]                  0                  0                  0
## [5,]                  0                  0                  0</code></pre>
</div>
<div id="the-result-making-song-recommendations" class="section level2">
<h2>The Result: Making Song Recommendations</h2>
<p>I wrote a function called <code>calc_cos_sim</code>, which will calculate the similarity between a chosen song and the other songs, and recommend 5 new songs for a user to listen to. From start to finish, this only took about 20 lines of code, indicating how easy it can be to spin up a recommendation engine.</p>
<pre class="r"><code>calc_cos_sim &lt;- function(song_code, 
                         rating_mat = ratings,
                         songs = song_data,
                         return_n = 5) {
  # find our song
  song_col_index &lt;- which(colnames(rating_mat) == song_code)
  # calculate cosine similarity for each song based on 
  # number of plays for users
  # apply(..., 2) iterates over the columns of a matrix
  cos_sims &lt;- apply(rating_mat, 2,
                    FUN = function(y) 
                      cosine_sim(rating_mat[,song_col_index], y))
  # return results
  data_frame(song_id = names(cos_sims), cos_sim = cos_sims) %&gt;%
    filter(song_id != song_code) %&gt;% # remove self reference
    inner_join(songs) %&gt;%
    arrange(desc(cos_sim)) %&gt;%
    top_n(return_n, cos_sim) %&gt;%
    select(song_id, title, artist_name, cos_sim)
}</code></pre>
<p>We can use the function above to calculate similarities and generate recommendations for a few songs.</p>
<p>Let’s look at the hip-hop classic <a href="https://www.youtube.com/watch?v=QFcv5Ma8u8k">“Forgot about Dre”</a> first.</p>
<pre class="r"><code>forgot_about_dre &lt;- &#39;SOPJLFV12A6701C797&#39;
knitr::kable(calc_cos_sim(forgot_about_dre))</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">song_id</th>
<th align="left">title</th>
<th align="left">artist_name</th>
<th align="right">cos_sim</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">SOZCWQA12A6701C798</td>
<td align="left">The Next Episode</td>
<td align="left">Dr. Dre / Snoop Dogg</td>
<td align="right">0.3561683</td>
</tr>
<tr class="even">
<td align="left">SOHEMBB12A6701E907</td>
<td align="left">Superman</td>
<td align="left">Eminem / Dina Rae</td>
<td align="right">0.2507195</td>
</tr>
<tr class="odd">
<td align="left">SOWGXOP12A6701E93A</td>
<td align="left">Without Me</td>
<td align="left">Eminem</td>
<td align="right">0.1596885</td>
</tr>
<tr class="even">
<td align="left">SOJTDUS12A6D4FBF0E</td>
<td align="left">None Shall Pass (Main)</td>
<td align="left">Aesop Rock</td>
<td align="right">0.1591929</td>
</tr>
<tr class="odd">
<td align="left">SOSKDTM12A6701C795</td>
<td align="left">What’s The Difference</td>
<td align="left">Dr. Dre / Eminem / Alvin Joiner</td>
<td align="right">0.1390476</td>
</tr>
</tbody>
</table>
<p>Each song we recommended is a hip-hop song, which is a good start! Even on this reduced dataset, the engine is making <em>decent</em> recommendations.</p>
<!--

```r
enter_sandman <- 'SOCHYVZ12A6D4F5908'
knitr::kable(calc_cos_sim(enter_sandman))
```



song_id              title                         artist_name                  cos_sim
-------------------  ----------------------------  ------------------------  ----------
SOLMIUU12A58A79C99   Another Day In Paradise       Phil Collins               0.2211738
SOYGHUM12AB018139C   Bad Company                   Five Finger Death Punch    0.2190192
SOZDGEW12A8C13E748   One                           Metallica                  0.2104946
SOUBXSF12A6701D23C   You Could Be Mine             Guns N' Roses              0.1289000
SOIPYPB12A8C1360D4   My Immortal (Album Version)   Evanescence                0.1175952
SOIPYPB12A8C1360D4   My Immortal                   Evanescence                0.1175952
-->
<p>The next song is <a href="https://www.youtube.com/watch?v=vabnZ9-ex7o">“Come As You Are” by Nirvana</a>. Users who like this song probably listen to other grunge/rock songs.</p>
<pre class="r"><code>come_as_you_are &lt;- &#39;SODEOCO12A6701E922&#39;
knitr::kable(calc_cos_sim(come_as_you_are))</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">song_id</th>
<th align="left">title</th>
<th align="left">artist_name</th>
<th align="right">cos_sim</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">SOCPMIK12A6701E96D</td>
<td align="left">The Man Who Sold The World</td>
<td align="left">Nirvana</td>
<td align="right">0.3903533</td>
</tr>
<tr class="even">
<td align="left">SONNNEH12AB01827DE</td>
<td align="left">Lithium</td>
<td align="left">Nirvana</td>
<td align="right">0.3568732</td>
</tr>
<tr class="odd">
<td align="left">SOLOFYI12A8C145F8D</td>
<td align="left">Heart Shaped Box</td>
<td align="left">Nirvana</td>
<td align="right">0.1958162</td>
</tr>
<tr class="even">
<td align="left">SOVDLVT12A58A7B988</td>
<td align="left">Behind Blue Eyes</td>
<td align="left">Limp Bizkit</td>
<td align="right">0.1186160</td>
</tr>
<tr class="odd">
<td align="left">SOWBYZF12A6D4F9424</td>
<td align="left">Fakty</td>
<td align="left">Horkyze Slyze</td>
<td align="right">0.0952245</td>
</tr>
</tbody>
</table>
<p>Alright, 2 for 2. One thing to be mindful of when looking at these results is that we’re not incorporating <em>any</em> information about the songs themselves. Our engine isn’t built using any data about the artist, genre, or other musical characteristics. Additionally, we’re not considering any demographic information about the users, and it’s fairly easy to see how useful age, gender, and other user-level data could be in making recommendations. If we used this information in addition to our user play data, we’d have what is called a <a href="https://www.math.uci.edu/icamp/courses/math77b/lecture_12w/pdfs/Chapter%2005%20-%20Hybrid%20recommendation%20approaches.pdf"><strong>hybrid recommendation system</strong></a>.</p>
<p>Finally, we’ll recommend songs for our hard-partying friends that like the song <a href="https://www.youtube.com/watch?v=XNtTEibFvlQ">“Shots” by LMFAO featuring Lil Jon</a> (<strong>that video is not for the faint of heart</strong>).</p>
<pre class="r"><code>shots &lt;- &#39;SOJYBJZ12AB01801D0&#39;
knitr::kable(calc_cos_sim(shots))</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">song_id</th>
<th align="left">title</th>
<th align="left">artist_name</th>
<th align="right">cos_sim</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">SOWEHOM12A6BD4E09E</td>
<td align="left">16 Candles</td>
<td align="left">The Crests</td>
<td align="right">0.2551851</td>
</tr>
<tr class="even">
<td align="left">SOLQXDJ12AB0182E47</td>
<td align="left">Yes</td>
<td align="left">LMFAO</td>
<td align="right">0.1866648</td>
</tr>
<tr class="odd">
<td align="left">SOSZJFV12AB01878CB</td>
<td align="left">Teach Me How To Dougie</td>
<td align="left">California Swag District</td>
<td align="right">0.1387647</td>
</tr>
<tr class="even">
<td align="left">SOYGKNI12AB0187E6E</td>
<td align="left">All I Do Is Win (feat. T-Pain_ Ludacris_ Snoop Dogg &amp; Rick Ross)</td>
<td align="left">DJ Khaled</td>
<td align="right">0.1173063</td>
</tr>
<tr class="odd">
<td align="left">SOUSMXX12AB0185C24</td>
<td align="left">OMG</td>
<td align="left">Usher featuring will.i.am</td>
<td align="right">0.1012716</td>
</tr>
</tbody>
</table>
<p>Well, the “16 Candles” result is a little surprising, but this might give us some insight into the demographics of users that like “Shots”. The other four recommendations seem pretty solid, I guess.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Cosine similarity is simple to calculate and is fairly intuitive once some basic geometric concepts are understood. The simplicity of this metric makes it a great first-pass option for recommendation systems, and can be treated as a baseline with which to compare more computationally intensive and/or difficult to understand methods.</p>
<p>I think that recommendation systems will continue to play a large role in our online lives. It can be helpful to understand the components underneath these systems, so that we treat them less as blackbox oracles and more as the imperfect prediction systems based on data they are.</p>
<p>I hope you liked this brief excursion into the world of recommendation engines. Hopefully you can walk away knowing a little more about why Amazon, Netflix, and other platforms recommend the content they do.</p>
<div id="other-resources" class="section level3">
<h3>Other Resources</h3>
<p>Here are a few great resources if you want to dive deeper into recommendation systems and cosine similarity.</p>
<ul>
<li><a href="http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/">Machine Learning :: Cosine Similarity for Vector Space Models (Part III)</a></li>
<li><a href="http://stefansavev.com/blog/cosine-similarity-all-posts/">Series of blog posts about cosine similarity</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cosine_similarity">Wikipedia: Cosine Similarity</a></li>
<li><a href="https://masongallo.github.io/machine/learning,/python/2016/07/29/cosine-similarity.html">Implementing and Understanding Cosine Similarity</a></li>
<li><a href="https://towardsdatascience.com/what-are-product-recommendation-engines-and-the-various-versions-of-them-9dcab4ee26d5">What are Product Recommendation Engines? And the various versions of them?</a></li>
<li><a href="https://hackernoon.com/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75">Introduction to Collaborative Filtering</a></li>
</ul>
</div>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>Using R to Create Custom Color Palettes for Tableau</title>
      <link>/post/using-r-to-create-custom-color-palettes-for-tableau/</link>
      <pubDate>October 31, 2018</pubDate>
      
      <guid>/post/using-r-to-create-custom-color-palettes-for-tableau/</guid>
      <description>&lt;p&gt;Have you ever wanted to define custom color palettes in Tableau, but didn’t know how? In this post, I’m going to walk through how we can use &lt;code&gt;R&lt;/code&gt; to programmatically generate custom palettes in Tableau. Creating custom color palettes for Tableau has never been easier!&lt;/p&gt;
&lt;p&gt;This is going to be a short post, with just a little bit of &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;p&gt;At the end of the post, you’ll see how to use &lt;code&gt;R&lt;/code&gt; to generate custom color palettes to add to Tableau. We’ll show how to add palettes from the viridis color palette and ColorBrewer to Tableau.&lt;/p&gt;
&lt;div id=&#34;defining-custom-color-palettes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Defining Custom Color Palettes&lt;/h2&gt;
&lt;p&gt;Tableau already has a &lt;a href=&#34;https://onlinehelp.tableau.com/current/pro/desktop/en-us/formatting_create_custom_colors.htm&#34;&gt;pretty good tutorial&lt;/a&gt; and &lt;a href=&#34;http://www.tableauexpert.co.in/2015/11/how-to-create-custom-color-palette-in.html&#34;&gt;this tutorial&lt;/a&gt; is pretty good too, but I thought I’d share some &lt;code&gt;R&lt;/code&gt; code that helps to make it easier to define custom palettes.&lt;/p&gt;
&lt;p&gt;The basics of defining custom palettes in Tableau is that you have to modify the &lt;code&gt;Preferences.tps&lt;/code&gt; file that comes with Tableau. This file can be found in your &lt;strong&gt;My Tableau Repository&lt;/strong&gt;. It’s an &lt;code&gt;XML&lt;/code&gt; file, which makes it pretty easy to hack around in the text editor of your choice (I prefer &lt;a href=&#34;https://www.sublimetext.com/&#34;&gt;Sublime Text&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If we wanted to define a custom palette, based on the &lt;a href=&#34;http://colorbrewer2.org/#type=qualitative&amp;amp;scheme=Set1&amp;amp;n=3&#34;&gt;three color Set1 palette&lt;/a&gt; from &lt;a href=&#34;http://colorbrewer2.org&#34;&gt;ColorBrewer&lt;/a&gt;, we would just add this to our &lt;code&gt;Preferences.tps&lt;/code&gt; file:&lt;/p&gt;
&lt;pre class=&#34;xml&#34;&gt;&lt;code&gt;&amp;lt;color-palette name=&amp;quot;Set1 3 Color Qual Palette&amp;quot; type=&amp;quot;regular&amp;quot;&amp;gt;
&amp;lt;color&amp;gt;#E41A1C&amp;lt;/color&amp;gt;
&amp;lt;color&amp;gt;#377EB8&amp;lt;/color&amp;gt;
&amp;lt;color&amp;gt;#4DAF4A&amp;lt;/color&amp;gt;
&amp;lt;/color-palette&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In it, we defined the &lt;code&gt;name&lt;/code&gt;, the &lt;code&gt;type&lt;/code&gt; (regular, ordered-diverging, or ordered-sequential), and the &lt;code&gt;color&lt;/code&gt;s (HEX codes).&lt;/p&gt;
&lt;p&gt;If you wanted to hand-edit this file, it might be tedious and you’d need to do a lot of copying-and-pasting.&lt;/p&gt;
&lt;p&gt;So, why not write a quick function in &lt;code&gt;R&lt;/code&gt; to generate this?&lt;/p&gt;
&lt;p&gt;Or, maybe you’d just like a pre-filled &lt;code&gt;Preferences.tps&lt;/code&gt; file with many useful palettes added already. If so, check out &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/blob/master/custom-tableau-color-palette/Preferences.tps&#34;&gt;my GitHub repository&lt;/a&gt; which has a fairly complete &lt;code&gt;Preferences.tps&lt;/code&gt; file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-create_tableau_palette-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;create_tableau_palette&lt;/code&gt; function&lt;/h2&gt;
&lt;p&gt;The following function takes three arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;palette_name&lt;/code&gt;&lt;/strong&gt;: this is what you want the name to be in your file. In the example above, it was &lt;em&gt;Set1 3 Color Qual Palette&lt;/em&gt;. Make sure you name it something descriptive enough to be found easily in Tableau.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;palette_colors&lt;/code&gt;&lt;/strong&gt;: this is a character vector of colors which will be added to the palette. You should use HEX codes (e.g. &lt;code&gt;&#34;#E41A1C&#34;&lt;/code&gt;, &lt;code&gt;&#34;#377EB8&#34;&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;palette_type&lt;/code&gt;&lt;/strong&gt;: this is one of the three palette types described above. In the previous example, it was &lt;em&gt;regular&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The function will then print the resulting color palette to the console, so you can copy and paste the results. It uses the &lt;code&gt;cat&lt;/code&gt; function, so it &lt;strong&gt;only&lt;/strong&gt; prints stuff to the console, it isn’t necessary to store the result in a variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_tableau_palette &amp;lt;- function(palette_name,
                                   palette_colors,
                                   palette_type) {
  # check palette type
  p_type = match.arg(palette_type,
                     choices = c(&amp;#39;ordered-diverging&amp;#39;,
                                 &amp;#39;ordered-sequential&amp;#39;,
                                 &amp;#39;regular&amp;#39;))
  # starting line
  line_start &amp;lt;- paste0(&amp;#39;&amp;lt;color-palette name=&amp;quot;&amp;#39;,
                       palette_name,
                       &amp;#39;&amp;quot; type=&amp;quot;&amp;#39;,
                       p_type,
                       &amp;#39;&amp;quot;&amp;gt;\n&amp;#39;)
  # define colors
  colors &amp;lt;- paste0(&amp;#39;&amp;lt;color&amp;gt;&amp;#39;,
                   palette_colors,
                   &amp;#39;&amp;lt;/color&amp;gt;\n&amp;#39;)
  # ending line
  line_end &amp;lt;- &amp;quot;&amp;lt;/color-palette&amp;gt;\n&amp;quot;
  # push together
  cat(paste0(c(line_start, colors, line_end)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# character vector of first four Set2 color values
brewer_4 &amp;lt;- RColorBrewer::brewer.pal(4, &amp;#39;Set2&amp;#39;)
# use the function
create_tableau_palette(palette_name = &amp;quot;Color Brewer Set2 4&amp;quot;,
                       palette_colors = brewer_4,
                       palette_type = &amp;#39;regular&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;color-palette name=&amp;quot;Color Brewer Set2 4&amp;quot; type=&amp;quot;regular&amp;quot;&amp;gt;
##  &amp;lt;color&amp;gt;#66C2A5&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#FC8D62&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#8DA0CB&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#E78AC3&amp;lt;/color&amp;gt;
##  &amp;lt;/color-palette&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You could take the result above (remove the &lt;code&gt;##&lt;/code&gt; that results from printing) and copy and paste it into the &lt;code&gt;Preferences.tps&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Of course, we could loop through different specifications to create many custom palettes rather quickly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-viridis-palettes-for-tableau&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generating Viridis Palettes for Tableau&lt;/h2&gt;
&lt;p&gt;Let’s use this function to generate custom Tableau color palettes for the popular &lt;a href=&#34;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&#34;&gt;viridis palette&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-31-using-r-to-create-custom-color-palettes-for-tableau_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’re using the &lt;a href=&#34;https://purrr.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package to do our “looping”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# need to store result for better printing, result is just a list of NULL
x&amp;lt;-purrr::map(4:7,
           ~create_tableau_palette(palette_name = paste(&amp;#39;Viridis&amp;#39;, .x),
                                   palette_colors = viridis::viridis(.x),
                                   palette_type = &amp;#39;ordered-sequential&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;color-palette name=&amp;quot;Viridis 4&amp;quot; type=&amp;quot;ordered-sequential&amp;quot;&amp;gt;
##  &amp;lt;color&amp;gt;#440154FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#31688EFF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#35B779FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#FDE725FF&amp;lt;/color&amp;gt;
##  &amp;lt;/color-palette&amp;gt;
## &amp;lt;color-palette name=&amp;quot;Viridis 5&amp;quot; type=&amp;quot;ordered-sequential&amp;quot;&amp;gt;
##  &amp;lt;color&amp;gt;#440154FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#3B528BFF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#21908CFF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#5DC863FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#FDE725FF&amp;lt;/color&amp;gt;
##  &amp;lt;/color-palette&amp;gt;
## &amp;lt;color-palette name=&amp;quot;Viridis 6&amp;quot; type=&amp;quot;ordered-sequential&amp;quot;&amp;gt;
##  &amp;lt;color&amp;gt;#440154FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#414487FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#2A788EFF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#22A884FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#7AD151FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#FDE725FF&amp;lt;/color&amp;gt;
##  &amp;lt;/color-palette&amp;gt;
## &amp;lt;color-palette name=&amp;quot;Viridis 7&amp;quot; type=&amp;quot;ordered-sequential&amp;quot;&amp;gt;
##  &amp;lt;color&amp;gt;#440154FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#443A83FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#31688EFF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#21908CFF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#35B779FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#8FD744FF&amp;lt;/color&amp;gt;
##  &amp;lt;color&amp;gt;#FDE725FF&amp;lt;/color&amp;gt;
##  &amp;lt;/color-palette&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then you can copy and paste that right into your &lt;code&gt;Preferences.tps&lt;/code&gt; file! You’ll need to remove those &lt;code&gt;##&lt;/code&gt; symbols, but that shouldn’t be an issue if you’re using this function in your own &lt;code&gt;R&lt;/code&gt; session. After you’ve added that to your file, restart Tableau, and then you should find the new palettes in your color choices.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;This was a short post illustrating one way to use &lt;code&gt;R&lt;/code&gt; to generate custom palettes for Tableau. I really like Tableau as a way to build interactive dashboards, but I have found the default color palettes to be somewhat lacking (or maybe I just have high color palette standards). Hopefully this post will show you how easy it is to add new palettes to Tableau without having to do too much tedious copying-and-pasting.&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded><p>Have you ever wanted to define custom color palettes in Tableau, but didn’t know how? In this post, I’m going to walk through how we can use <code>R</code> to programmatically generate custom palettes in Tableau. Creating custom color palettes for Tableau has never been easier!</p>
<p>This is going to be a short post, with just a little bit of <code>R</code> code.</p>
<p>At the end of the post, you’ll see how to use <code>R</code> to generate custom color palettes to add to Tableau. We’ll show how to add palettes from the viridis color palette and ColorBrewer to Tableau.</p>
<div id="defining-custom-color-palettes" class="section level2">
<h2>Defining Custom Color Palettes</h2>
<p>Tableau already has a <a href="https://onlinehelp.tableau.com/current/pro/desktop/en-us/formatting_create_custom_colors.htm">pretty good tutorial</a> and <a href="http://www.tableauexpert.co.in/2015/11/how-to-create-custom-color-palette-in.html">this tutorial</a> is pretty good too, but I thought I’d share some <code>R</code> code that helps to make it easier to define custom palettes.</p>
<p>The basics of defining custom palettes in Tableau is that you have to modify the <code>Preferences.tps</code> file that comes with Tableau. This file can be found in your <strong>My Tableau Repository</strong>. It’s an <code>XML</code> file, which makes it pretty easy to hack around in the text editor of your choice (I prefer <a href="https://www.sublimetext.com/">Sublime Text</a>).</p>
<p>If we wanted to define a custom palette, based on the <a href="http://colorbrewer2.org/#type=qualitative&amp;scheme=Set1&amp;n=3">three color Set1 palette</a> from <a href="http://colorbrewer2.org">ColorBrewer</a>, we would just add this to our <code>Preferences.tps</code> file:</p>
<pre class="xml"><code>&lt;color-palette name=&quot;Set1 3 Color Qual Palette&quot; type=&quot;regular&quot;&gt;
&lt;color&gt;#E41A1C&lt;/color&gt;
&lt;color&gt;#377EB8&lt;/color&gt;
&lt;color&gt;#4DAF4A&lt;/color&gt;
&lt;/color-palette&gt;</code></pre>
<p>In it, we defined the <code>name</code>, the <code>type</code> (regular, ordered-diverging, or ordered-sequential), and the <code>color</code>s (HEX codes).</p>
<p>If you wanted to hand-edit this file, it might be tedious and you’d need to do a lot of copying-and-pasting.</p>
<p>So, why not write a quick function in <code>R</code> to generate this?</p>
<p>Or, maybe you’d just like a pre-filled <code>Preferences.tps</code> file with many useful palettes added already. If so, check out <a href="https://github.com/bgstieber/files_for_blog/blob/master/custom-tableau-color-palette/Preferences.tps">my GitHub repository</a> which has a fairly complete <code>Preferences.tps</code> file.</p>
</div>
<div id="the-create_tableau_palette-function" class="section level2">
<h2>The <code>create_tableau_palette</code> function</h2>
<p>The following function takes three arguments:</p>
<ul>
<li><strong><code>palette_name</code></strong>: this is what you want the name to be in your file. In the example above, it was <em>Set1 3 Color Qual Palette</em>. Make sure you name it something descriptive enough to be found easily in Tableau.</li>
<li><strong><code>palette_colors</code></strong>: this is a character vector of colors which will be added to the palette. You should use HEX codes (e.g. <code>"#E41A1C"</code>, <code>"#377EB8"</code>)</li>
<li><strong><code>palette_type</code></strong>: this is one of the three palette types described above. In the previous example, it was <em>regular</em>.</li>
</ul>
<p>The function will then print the resulting color palette to the console, so you can copy and paste the results. It uses the <code>cat</code> function, so it <strong>only</strong> prints stuff to the console, it isn’t necessary to store the result in a variable.</p>
<pre class="r"><code>create_tableau_palette &lt;- function(palette_name,
                                   palette_colors,
                                   palette_type) {
  # check palette type
  p_type = match.arg(palette_type,
                     choices = c(&#39;ordered-diverging&#39;,
                                 &#39;ordered-sequential&#39;,
                                 &#39;regular&#39;))
  # starting line
  line_start &lt;- paste0(&#39;&lt;color-palette name=&quot;&#39;,
                       palette_name,
                       &#39;&quot; type=&quot;&#39;,
                       p_type,
                       &#39;&quot;&gt;\n&#39;)
  # define colors
  colors &lt;- paste0(&#39;&lt;color&gt;&#39;,
                   palette_colors,
                   &#39;&lt;/color&gt;\n&#39;)
  # ending line
  line_end &lt;- &quot;&lt;/color-palette&gt;\n&quot;
  # push together
  cat(paste0(c(line_start, colors, line_end)))
}</code></pre>
<p>Here’s an example:</p>
<pre class="r"><code># character vector of first four Set2 color values
brewer_4 &lt;- RColorBrewer::brewer.pal(4, &#39;Set2&#39;)
# use the function
create_tableau_palette(palette_name = &quot;Color Brewer Set2 4&quot;,
                       palette_colors = brewer_4,
                       palette_type = &#39;regular&#39;)</code></pre>
<pre><code>## &lt;color-palette name=&quot;Color Brewer Set2 4&quot; type=&quot;regular&quot;&gt;
##  &lt;color&gt;#66C2A5&lt;/color&gt;
##  &lt;color&gt;#FC8D62&lt;/color&gt;
##  &lt;color&gt;#8DA0CB&lt;/color&gt;
##  &lt;color&gt;#E78AC3&lt;/color&gt;
##  &lt;/color-palette&gt;</code></pre>
<p>You could take the result above (remove the <code>##</code> that results from printing) and copy and paste it into the <code>Preferences.tps</code> file.</p>
<p>Of course, we could loop through different specifications to create many custom palettes rather quickly.</p>
</div>
<div id="generating-viridis-palettes-for-tableau" class="section level2">
<h2>Generating Viridis Palettes for Tableau</h2>
<p>Let’s use this function to generate custom Tableau color palettes for the popular <a href="https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html">viridis palette</a>.</p>
<p><img src="/post/2018-10-31-using-r-to-create-custom-color-palettes-for-tableau_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>We’re using the <a href="https://purrr.tidyverse.org/"><strong><code>purrr</code></strong></a> package to do our “looping”.</p>
<pre class="r"><code># need to store result for better printing, result is just a list of NULL
x&lt;-purrr::map(4:7,
           ~create_tableau_palette(palette_name = paste(&#39;Viridis&#39;, .x),
                                   palette_colors = viridis::viridis(.x),
                                   palette_type = &#39;ordered-sequential&#39;))</code></pre>
<pre><code>## &lt;color-palette name=&quot;Viridis 4&quot; type=&quot;ordered-sequential&quot;&gt;
##  &lt;color&gt;#440154FF&lt;/color&gt;
##  &lt;color&gt;#31688EFF&lt;/color&gt;
##  &lt;color&gt;#35B779FF&lt;/color&gt;
##  &lt;color&gt;#FDE725FF&lt;/color&gt;
##  &lt;/color-palette&gt;
## &lt;color-palette name=&quot;Viridis 5&quot; type=&quot;ordered-sequential&quot;&gt;
##  &lt;color&gt;#440154FF&lt;/color&gt;
##  &lt;color&gt;#3B528BFF&lt;/color&gt;
##  &lt;color&gt;#21908CFF&lt;/color&gt;
##  &lt;color&gt;#5DC863FF&lt;/color&gt;
##  &lt;color&gt;#FDE725FF&lt;/color&gt;
##  &lt;/color-palette&gt;
## &lt;color-palette name=&quot;Viridis 6&quot; type=&quot;ordered-sequential&quot;&gt;
##  &lt;color&gt;#440154FF&lt;/color&gt;
##  &lt;color&gt;#414487FF&lt;/color&gt;
##  &lt;color&gt;#2A788EFF&lt;/color&gt;
##  &lt;color&gt;#22A884FF&lt;/color&gt;
##  &lt;color&gt;#7AD151FF&lt;/color&gt;
##  &lt;color&gt;#FDE725FF&lt;/color&gt;
##  &lt;/color-palette&gt;
## &lt;color-palette name=&quot;Viridis 7&quot; type=&quot;ordered-sequential&quot;&gt;
##  &lt;color&gt;#440154FF&lt;/color&gt;
##  &lt;color&gt;#443A83FF&lt;/color&gt;
##  &lt;color&gt;#31688EFF&lt;/color&gt;
##  &lt;color&gt;#21908CFF&lt;/color&gt;
##  &lt;color&gt;#35B779FF&lt;/color&gt;
##  &lt;color&gt;#8FD744FF&lt;/color&gt;
##  &lt;color&gt;#FDE725FF&lt;/color&gt;
##  &lt;/color-palette&gt;</code></pre>
<p>And then you can copy and paste that right into your <code>Preferences.tps</code> file! You’ll need to remove those <code>##</code> symbols, but that shouldn’t be an issue if you’re using this function in your own <code>R</code> session. After you’ve added that to your file, restart Tableau, and then you should find the new palettes in your color choices.</p>
</div>
<div id="wrapping-up" class="section level2">
<h2>Wrapping Up</h2>
<p>This was a short post illustrating one way to use <code>R</code> to generate custom palettes for Tableau. I really like Tableau as a way to build interactive dashboards, but I have found the default color palettes to be somewhat lacking (or maybe I just have high color palette standards). Hopefully this post will show you how easy it is to add new palettes to Tableau without having to do too much tedious copying-and-pasting.</p>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>Iterating on a 2016 Election Analysis</title>
      <link>/post/iterating-on-a-2016-election-analysis/</link>
      <pubDate>October 3, 2018</pubDate>
      
      <guid>/post/iterating-on-a-2016-election-analysis/</guid>
      <description>&lt;p&gt;Jake Low wrote a &lt;a href=&#34;https://beta.observablehq.com/@jake-low/how-well-does-population-density-predict-u-s-voting-outcome&#34;&gt;really interesting piece&lt;/a&gt; that presented a few data visualizations that went beyond the typical &lt;a href=&#34;https://www.nytimes.com/interactive/2018/upshot/election-2016-voting-precinct-maps.html&#34;&gt;2016 election maps&lt;/a&gt; we’ve all gotten used to seeing.&lt;/p&gt;
&lt;p&gt;I liked a lot of things about Jake’s post, here are three I was particularly fond of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;His color palette choices
&lt;ul&gt;
&lt;li&gt;Each color palette that was used had solid perceptual properties and made sense for the data being visualized (i.e. diverging versus sequential)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;He made residuals from a model interesting by visualizing &lt;em&gt;and&lt;/em&gt; interpreting them&lt;/li&gt;
&lt;li&gt;He explained the usage of a log-scale transformation in an intuitive way, putting it in terms of the data set being used for the analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, I’m going to replicate Jake’s analysis and then extend it a bit, by fitting a model which is a little more complicated than the one is his post.&lt;/p&gt;
&lt;p&gt;In Jake’s post, he used d3.js to do most of the work. As usual, I’ll be using &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Since this is &lt;code&gt;R&lt;/code&gt;, here are the packages I’ll need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidycensus)
library(ggmap)
library(scales)
library(maps)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;getting-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting the Data&lt;/h2&gt;
&lt;p&gt;For this analysis, we’ll need a few different data sets, all measured at the county level:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2016 election results&lt;/li&gt;
&lt;li&gt;Population density&lt;/li&gt;
&lt;li&gt;Educational information (how many people over 25 have some college or associate’s degree)&lt;/li&gt;
&lt;li&gt;Median income&lt;/li&gt;
&lt;li&gt;2012 election results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I got election results from &lt;a href=&#34;https://github.com/tonmcg/County_Level_Election_Results_12-16&#34;&gt;this GitHub repository&lt;/a&gt;. I plan on making a few heatmaps with this data, so I’m only including information from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Contiguous_United_States&#34;&gt;contiguous United States&lt;/a&gt;. There’s also an issue I don’t fully understand with Alaska vote reporting, where it seems as though county-level reporting doesn’t exist.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;github_raw &amp;lt;- &amp;quot;https://raw.githubusercontent.com/&amp;quot;
repo &amp;lt;- &amp;quot;tonmcg/County_Level_Election_Results_12-16/master/&amp;quot;
data_file &amp;lt;- &amp;quot;2016_US_County_Level_Presidential_Results.csv&amp;quot;
results_16 &amp;lt;- read_csv(paste0(github_raw, repo, data_file)) %&amp;gt;%
  filter(! state_abbr %in% c(&amp;#39;AK&amp;#39;, &amp;#39;HI&amp;#39;)) %&amp;gt;%
  select(-X1) %&amp;gt;%
  mutate(total_votes = votes_dem + votes_gop,
         trump_ratio_clinton = 
           (votes_gop/total_votes) / (votes_dem/total_votes),
         two_party_ratio = (votes_dem) / (votes_dem + votes_gop)) %&amp;gt;%
  mutate(log_trump_ratio = log(trump_ratio_clinton)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get some extra information about the counties I’ll use the &lt;a href=&#34;https://github.com/walkerke/tidycensus&#34;&gt;&lt;strong&gt;&lt;code&gt;tidycensus&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package. For each county, I’m pulling information about the population over 25, the number of people over 25 with some college or associate’s degree, and the median income.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_data &amp;lt;- get_acs(&amp;#39;county&amp;#39;, 
                   c(pop_25 = &amp;#39;B15003_001&amp;#39;, 
                     edu = &amp;#39;B16010_028&amp;#39;, 
                     inc = &amp;#39;B21004_001&amp;#39;)) %&amp;gt;%
  select(-moe) %&amp;gt;%
  spread(variable, estimate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I pulled in information about population density from the &lt;a href=&#34;https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?src=bkmk&#34;&gt;Census American FactFinder&lt;/a&gt;. Once I downloaded that data, I threw it into a &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/tree/master/election-map/Data&#34;&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recreating-jakes-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recreating Jake’s Analysis&lt;/h2&gt;
&lt;div id=&#34;election-and-population-density-maps&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Election and Population Density Maps&lt;/h4&gt;
&lt;p&gt;First, we’re going to make the 2016 election map.&lt;/p&gt;
&lt;p&gt;I’ll be making all of the maps in this post using &lt;code&gt;ggplot2&lt;/code&gt;. I’ve removed the code from this post, but if you look on my GitHub, you’ll notice some funky stuff going on with &lt;code&gt;scale_fill_gradientn&lt;/code&gt;. To make the map more visually salient, I’ve played around a bit with how the colors are scaled.&lt;/p&gt;
&lt;p&gt;Counties that are red voted more for Trump, and counties that are blue voted more for Clinton.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then we’ll take a look at population density. Again, I’ve hidden the code to make this map, but I’ve used &lt;code&gt;ggplot2&lt;/code&gt; to visualize the data, and I used one of the &lt;a href=&#34;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&#34;&gt;virdis&lt;/a&gt; color palettes (which is what Jake did as well).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you look between the two maps, you might get a sense of the correlation between population density and the 2016 election results. It’s reasonable to expect that we might be able to do a decent job at predicting election results just using population density.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predicting-2016-with-population-density&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Predicting 2016 with Population Density&lt;/h4&gt;
&lt;p&gt;Finally, we’re going to try to &lt;strong&gt;predict the 2016 election results using population density&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, let’s examine a scatter plot of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results_and_pop_density &amp;lt;- results_16 %&amp;gt;% 
  inner_join(pop_density, by = c(&amp;#39;combined_fips&amp;#39; = &amp;#39;FIPS&amp;#39;)) %&amp;gt;%
  mutate(two_party_ratio = (votes_dem) / (votes_dem + votes_gop))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’re going to fit a linear model which tries to predict the Two Party Vote Ratio &lt;span class=&#34;math inline&#34;&gt;\(\frac{N_{clinton}}{N_{clinton}+N_{trump}}\)&lt;/span&gt; using population density. Calculating the two party ratio in the way we have means that a county over 0.5 favored Clinton.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model1 &amp;lt;- lm(two_party_ratio ~ I(log(population_density)),
             data = results_and_pop_density)
# extract residuals
# negative values underestimate Trump (prediction is too high)
# positive values underestimate Clinton (prediction is too low)
results_and_pop_density$resid_model1 &amp;lt;- resid(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m hiding the code used to generate the map, but what we’re visualizing isn’t actually what the model predicted. Instead, &lt;strong&gt;we’re visualizing the residuals&lt;/strong&gt;, which tell us about how good or bad the model’s prediction is. Calculating residuals is straightforward (we just see how far off our prediction was from the actual observed value):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{residual} = \text{actual proportion} - \text{predicted proportion}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The larger the residual (in absolute value), the farther the model was from being right for that observation. Analyzing residuals and learning more about where your model is wrong can be one of the most fascinating parts of statistics and data science.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In my opinion, visualizing the residuals from this simple model tells an even more interesting story than the election map above.&lt;/p&gt;
&lt;p&gt;For example, look at the southwest corner of Texas, which had much higher rates of Clinton favorability than the model predicted. Additionally, this map also shows Trump’s appeal through the middle of the country. Much of the nation’s “breadbasket” is colored pink, indicating that these counties favored Trump much more than the model predicted.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;extending-jakes-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extending Jake’s Analysis&lt;/h2&gt;
&lt;p&gt;My first thought after Jake’s post was that the model he built was pretty simple. There are a few other variables it would be good to adjust for.&lt;/p&gt;
&lt;p&gt;What would the previous residual map look like if we controlled for factors like education, income, and age? How about if we add in the two party ratio from the 2012 election (which came from &lt;a href=&#34;https://github.com/tonmcg/US_County_Level_Election_Results_08-16&#34;&gt;this GitHub repository&lt;/a&gt;)?&lt;/p&gt;
&lt;p&gt;We’re also going to deviate from the linear model we fit before. I think it would make more sense to fit a &lt;a href=&#34;https://stats.idre.ucla.edu/r/dae/logit-regression/&#34;&gt;logistic regression&lt;/a&gt; instead of a linear model, since the thing we want to predict is a proportion (it lives on the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;), not necessarily a continuous value (which lives on the range &lt;span class=&#34;math inline&#34;&gt;\((-\infty, \infty)\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;I would hope that building a more robust model would make the residual map even more interesting. Let’s see what happens!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit logistic regression
model2 &amp;lt;- glm(cbind(votes_dem, votes_gop) ~ I(log(population_density))+
                I(log(inc))+edu_pct + state_abbr + I(log1p(two_party_2012)),
              data = results_pop_census,
              family = &amp;#39;binomial&amp;#39;,
              na.action = na.exclude)
# extract residuals
results_pop_census$resid_model2 &amp;lt;- resid(model2, type = &amp;#39;response&amp;#39;)
# join up to geographic data
results_pop_census_map &amp;lt;- county_map_with_fips %&amp;gt;%
  inner_join(results_pop_census, by = c(&amp;#39;fips&amp;#39; = &amp;#39;combined_fips&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A few things are &lt;strong&gt;noticeably different&lt;/strong&gt; between the two maps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The upper Midwest is considerably pinker than the previous map. Incorporating the 2012 election results demonstrates how conventional wisdom resulted in poor predictions for 2016.&lt;/li&gt;
&lt;li&gt;Adding variables to the model substantially improved the fit. Notice how the range for the residuals is now much smaller.&lt;/li&gt;
&lt;li&gt;Parts of the Northeast are now much pinker as well. Trump’s vow to bring back coal jobs resonated with this area of the country.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some things &lt;strong&gt;stayed the same&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The southwestern corner (i.e. border towns) of Texas are still green (Clinton performed better than the model would have predicted).&lt;/li&gt;
&lt;li&gt;The “breadbasket” or “flyover” part of the country is still pink (Trump performed better than the model would have predicted).&lt;/li&gt;
&lt;li&gt;Most coastal population centers are green (Clinton performed better than the model would have predicted).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also look at counties where the model was most wrong, to see if there are any interesting patterns at the highest level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_n(results_pop_census, 10, resid_model2) %&amp;gt;%
    bind_rows(top_n(results_pop_census, 10, -resid_model2)) %&amp;gt;%
    ggplot(aes(reorder(paste0(county_name, &amp;#39;, &amp;#39;, state_abbr), resid_model2), resid_model2))+
    geom_col(aes(fill = resid_model2), colour = &amp;#39;black&amp;#39;)+
    coord_flip()+
    scale_fill_gradientn(values = rescale(c(-.18, -.05, -.02, 0, .02, .05, .18)),
                         colours = brewer_pal(palette = &amp;#39;PiYG&amp;#39;)(7),
                         limits = c(-.18, .18),
                         name = &amp;#39;Prediction Error (pink underestimates Trump, green underestimates Clinton)&amp;#39;)+
    theme(legend.position = &amp;#39;none&amp;#39;)+
    xlab(&amp;#39;&amp;#39;)+
    ylab(&amp;#39;Prediction Error (pink underestimated Trump, green underestimated Clinton)&amp;#39;)+
    ggtitle(&amp;#39;Counties with the Highest Prediction Errors&amp;#39;,
            subtitle = &amp;#39;Top 10 over- and under-predictions selected&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we can see that three of the highest ten errors in favor of Clinton (the county performed better for Clinton than the model predicted) were from Texas. More analysis showed that five of the highest twenty five errors in favor of Clinton were in Texas. For Trump, Virginia, Kentucky, and Tennessee each had two counties in the top 10.&lt;/p&gt;
&lt;p&gt;Finally, we can look at how wrong our new-and-improved model was across states. There are a few ways to summarize residuals over states (we could even build another model that predicted state results), but I’ll opt for the simplest route, and calculate &lt;strong&gt;the median error for each state&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Simply calculating the median prediction error tells an interesting story about the 2016 election. We see the Midwest lighting up in pink, indicating areas where Trump out-performed the expectations of the model. We see areas out west where Clinton out-performed the expectations of the model. Finally, we see areas which are colored faintly, indicating that the model’s median prediction error was fairly close to 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I wanted to build on the great work of &lt;a href=&#34;https://beta.observablehq.com/@jake-low/how-well-does-population-density-predict-u-s-voting-outcome&#34;&gt;Jake Low&lt;/a&gt; and demonstrate how going a bit deeper with our model fitting can allow us to refine the data stories we tell. I also wanted to take his analysis (done in d3.js) and demonstrate how it could be replicated in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I think the key takeaway from this post is that investigating the residuals from a model can result in some revelatory findings from a data analysis. Our models will &lt;a href=&#34;https://en.wikipedia.org/wiki/All_models_are_wrong&#34;&gt;always be wrong&lt;/a&gt;, but understanding and telling a story about &lt;em&gt;what&lt;/em&gt; they got wrong can make us feel a little better about that fact.&lt;/p&gt;
&lt;p&gt;Thanks for reading through this post. If you want to look at the code for this analysis, you can find it &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/tree/master/election-map&#34;&gt;on my GitHub&lt;/a&gt;. Let me know what you thought about my post. Did any of the maps surprise you?&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded><p>Jake Low wrote a <a href="https://beta.observablehq.com/@jake-low/how-well-does-population-density-predict-u-s-voting-outcome">really interesting piece</a> that presented a few data visualizations that went beyond the typical <a href="https://www.nytimes.com/interactive/2018/upshot/election-2016-voting-precinct-maps.html">2016 election maps</a> we’ve all gotten used to seeing.</p>
<p>I liked a lot of things about Jake’s post, here are three I was particularly fond of:</p>
<ul>
<li>His color palette choices
<ul>
<li>Each color palette that was used had solid perceptual properties and made sense for the data being visualized (i.e. diverging versus sequential)</li>
</ul></li>
<li>He made residuals from a model interesting by visualizing <em>and</em> interpreting them</li>
<li>He explained the usage of a log-scale transformation in an intuitive way, putting it in terms of the data set being used for the analysis.</li>
</ul>
<p>In this post, I’m going to replicate Jake’s analysis and then extend it a bit, by fitting a model which is a little more complicated than the one is his post.</p>
<p>In Jake’s post, he used d3.js to do most of the work. As usual, I’ll be using <code>R</code>.</p>
<p>Since this is <code>R</code>, here are the packages I’ll need:</p>
<pre class="r"><code>library(tidyverse)
library(tidycensus)
library(ggmap)
library(scales)
library(maps)</code></pre>
<div id="getting-the-data" class="section level2">
<h2>Getting the Data</h2>
<p>For this analysis, we’ll need a few different data sets, all measured at the county level:</p>
<ul>
<li>2016 election results</li>
<li>Population density</li>
<li>Educational information (how many people over 25 have some college or associate’s degree)</li>
<li>Median income</li>
<li>2012 election results</li>
</ul>
<p>I got election results from <a href="https://github.com/tonmcg/County_Level_Election_Results_12-16">this GitHub repository</a>. I plan on making a few heatmaps with this data, so I’m only including information from the <a href="https://en.wikipedia.org/wiki/Contiguous_United_States">contiguous United States</a>. There’s also an issue I don’t fully understand with Alaska vote reporting, where it seems as though county-level reporting doesn’t exist.</p>
<pre class="r"><code>github_raw &lt;- &quot;https://raw.githubusercontent.com/&quot;
repo &lt;- &quot;tonmcg/County_Level_Election_Results_12-16/master/&quot;
data_file &lt;- &quot;2016_US_County_Level_Presidential_Results.csv&quot;
results_16 &lt;- read_csv(paste0(github_raw, repo, data_file)) %&gt;%
  filter(! state_abbr %in% c(&#39;AK&#39;, &#39;HI&#39;)) %&gt;%
  select(-X1) %&gt;%
  mutate(total_votes = votes_dem + votes_gop,
         trump_ratio_clinton = 
           (votes_gop/total_votes) / (votes_dem/total_votes),
         two_party_ratio = (votes_dem) / (votes_dem + votes_gop)) %&gt;%
  mutate(log_trump_ratio = log(trump_ratio_clinton)) </code></pre>
<p>To get some extra information about the counties I’ll use the <a href="https://github.com/walkerke/tidycensus"><strong><code>tidycensus</code></strong></a> package. For each county, I’m pulling information about the population over 25, the number of people over 25 with some college or associate’s degree, and the median income.</p>
<pre class="r"><code>census_data &lt;- get_acs(&#39;county&#39;, 
                   c(pop_25 = &#39;B15003_001&#39;, 
                     edu = &#39;B16010_028&#39;, 
                     inc = &#39;B21004_001&#39;)) %&gt;%
  select(-moe) %&gt;%
  spread(variable, estimate)</code></pre>
<p>Finally, I pulled in information about population density from the <a href="https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?src=bkmk">Census American FactFinder</a>. Once I downloaded that data, I threw it into a <a href="https://github.com/bgstieber/files_for_blog/tree/master/election-map/Data">GitHub repo</a>.</p>
</div>
<div id="recreating-jakes-analysis" class="section level2">
<h2>Recreating Jake’s Analysis</h2>
<div id="election-and-population-density-maps" class="section level4">
<h4>Election and Population Density Maps</h4>
<p>First, we’re going to make the 2016 election map.</p>
<p>I’ll be making all of the maps in this post using <code>ggplot2</code>. I’ve removed the code from this post, but if you look on my GitHub, you’ll notice some funky stuff going on with <code>scale_fill_gradientn</code>. To make the map more visually salient, I’ve played around a bit with how the colors are scaled.</p>
<p>Counties that are red voted more for Trump, and counties that are blue voted more for Clinton.</p>
<p><img src="/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-7-1.png" width="864" /></p>
<p>Then we’ll take a look at population density. Again, I’ve hidden the code to make this map, but I’ve used <code>ggplot2</code> to visualize the data, and I used one of the <a href="https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html">virdis</a> color palettes (which is what Jake did as well).</p>
<p><img src="/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-8-1.png" width="864" /></p>
<p>If you look between the two maps, you might get a sense of the correlation between population density and the 2016 election results. It’s reasonable to expect that we might be able to do a decent job at predicting election results just using population density.</p>
</div>
<div id="predicting-2016-with-population-density" class="section level4">
<h4>Predicting 2016 with Population Density</h4>
<p>Finally, we’re going to try to <strong>predict the 2016 election results using population density</strong>.</p>
<p>First, let’s examine a scatter plot of the data.</p>
<pre class="r"><code>results_and_pop_density &lt;- results_16 %&gt;% 
  inner_join(pop_density, by = c(&#39;combined_fips&#39; = &#39;FIPS&#39;)) %&gt;%
  mutate(two_party_ratio = (votes_dem) / (votes_dem + votes_gop))</code></pre>
<p><img src="/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-10-1.png" width="768" /></p>
<p>We’re going to fit a linear model which tries to predict the Two Party Vote Ratio <span class="math inline">\(\frac{N_{clinton}}{N_{clinton}+N_{trump}}\)</span> using population density. Calculating the two party ratio in the way we have means that a county over 0.5 favored Clinton.</p>
<pre class="r"><code>model1 &lt;- lm(two_party_ratio ~ I(log(population_density)),
             data = results_and_pop_density)
# extract residuals
# negative values underestimate Trump (prediction is too high)
# positive values underestimate Clinton (prediction is too low)
results_and_pop_density$resid_model1 &lt;- resid(model1)</code></pre>
<p>I’m hiding the code used to generate the map, but what we’re visualizing isn’t actually what the model predicted. Instead, <strong>we’re visualizing the residuals</strong>, which tell us about how good or bad the model’s prediction is. Calculating residuals is straightforward (we just see how far off our prediction was from the actual observed value):</p>
<p><span class="math display">\[\text{residual} = \text{actual proportion} - \text{predicted proportion}\]</span></p>
<p>The larger the residual (in absolute value), the farther the model was from being right for that observation. Analyzing residuals and learning more about where your model is wrong can be one of the most fascinating parts of statistics and data science.</p>
<p><img src="/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-12-1.png" width="864" /></p>
<p>In my opinion, visualizing the residuals from this simple model tells an even more interesting story than the election map above.</p>
<p>For example, look at the southwest corner of Texas, which had much higher rates of Clinton favorability than the model predicted. Additionally, this map also shows Trump’s appeal through the middle of the country. Much of the nation’s “breadbasket” is colored pink, indicating that these counties favored Trump much more than the model predicted.</p>
</div>
</div>
<div id="extending-jakes-analysis" class="section level2">
<h2>Extending Jake’s Analysis</h2>
<p>My first thought after Jake’s post was that the model he built was pretty simple. There are a few other variables it would be good to adjust for.</p>
<p>What would the previous residual map look like if we controlled for factors like education, income, and age? How about if we add in the two party ratio from the 2012 election (which came from <a href="https://github.com/tonmcg/US_County_Level_Election_Results_08-16">this GitHub repository</a>)?</p>
<p>We’re also going to deviate from the linear model we fit before. I think it would make more sense to fit a <a href="https://stats.idre.ucla.edu/r/dae/logit-regression/">logistic regression</a> instead of a linear model, since the thing we want to predict is a proportion (it lives on the range <span class="math inline">\([0,1]\)</span>), not necessarily a continuous value (which lives on the range <span class="math inline">\((-\infty, \infty)\)</span>).</p>
<p>I would hope that building a more robust model would make the residual map even more interesting. Let’s see what happens!</p>
<pre class="r"><code># fit logistic regression
model2 &lt;- glm(cbind(votes_dem, votes_gop) ~ I(log(population_density))+
                I(log(inc))+edu_pct + state_abbr + I(log1p(two_party_2012)),
              data = results_pop_census,
              family = &#39;binomial&#39;,
              na.action = na.exclude)
# extract residuals
results_pop_census$resid_model2 &lt;- resid(model2, type = &#39;response&#39;)
# join up to geographic data
results_pop_census_map &lt;- county_map_with_fips %&gt;%
  inner_join(results_pop_census, by = c(&#39;fips&#39; = &#39;combined_fips&#39;))</code></pre>
<p><img src="/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-16-1.png" width="864" /></p>
<p>A few things are <strong>noticeably different</strong> between the two maps:</p>
<ul>
<li>The upper Midwest is considerably pinker than the previous map. Incorporating the 2012 election results demonstrates how conventional wisdom resulted in poor predictions for 2016.</li>
<li>Adding variables to the model substantially improved the fit. Notice how the range for the residuals is now much smaller.</li>
<li>Parts of the Northeast are now much pinker as well. Trump’s vow to bring back coal jobs resonated with this area of the country.</li>
</ul>
<p>Some things <strong>stayed the same</strong>:</p>
<ul>
<li>The southwestern corner (i.e. border towns) of Texas are still green (Clinton performed better than the model would have predicted).</li>
<li>The “breadbasket” or “flyover” part of the country is still pink (Trump performed better than the model would have predicted).</li>
<li>Most coastal population centers are green (Clinton performed better than the model would have predicted).</li>
</ul>
<p>We can also look at counties where the model was most wrong, to see if there are any interesting patterns at the highest level.</p>
<pre class="r"><code>top_n(results_pop_census, 10, resid_model2) %&gt;%
    bind_rows(top_n(results_pop_census, 10, -resid_model2)) %&gt;%
    ggplot(aes(reorder(paste0(county_name, &#39;, &#39;, state_abbr), resid_model2), resid_model2))+
    geom_col(aes(fill = resid_model2), colour = &#39;black&#39;)+
    coord_flip()+
    scale_fill_gradientn(values = rescale(c(-.18, -.05, -.02, 0, .02, .05, .18)),
                         colours = brewer_pal(palette = &#39;PiYG&#39;)(7),
                         limits = c(-.18, .18),
                         name = &#39;Prediction Error (pink underestimates Trump, green underestimates Clinton)&#39;)+
    theme(legend.position = &#39;none&#39;)+
    xlab(&#39;&#39;)+
    ylab(&#39;Prediction Error (pink underestimated Trump, green underestimated Clinton)&#39;)+
    ggtitle(&#39;Counties with the Highest Prediction Errors&#39;,
            subtitle = &#39;Top 10 over- and under-predictions selected&#39;)</code></pre>
<p><img src="/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>In the above plot, we can see that three of the highest ten errors in favor of Clinton (the county performed better for Clinton than the model predicted) were from Texas. More analysis showed that five of the highest twenty five errors in favor of Clinton were in Texas. For Trump, Virginia, Kentucky, and Tennessee each had two counties in the top 10.</p>
<p>Finally, we can look at how wrong our new-and-improved model was across states. There are a few ways to summarize residuals over states (we could even build another model that predicted state results), but I’ll opt for the simplest route, and calculate <strong>the median error for each state</strong>.</p>
<p><img src="/post/2018-10-03-iterating-on-a-2016-election-analysis_files/figure-html/unnamed-chunk-19-1.png" width="864" /></p>
<p>Simply calculating the median prediction error tells an interesting story about the 2016 election. We see the Midwest lighting up in pink, indicating areas where Trump out-performed the expectations of the model. We see areas out west where Clinton out-performed the expectations of the model. Finally, we see areas which are colored faintly, indicating that the model’s median prediction error was fairly close to 0.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>I wanted to build on the great work of <a href="https://beta.observablehq.com/@jake-low/how-well-does-population-density-predict-u-s-voting-outcome">Jake Low</a> and demonstrate how going a bit deeper with our model fitting can allow us to refine the data stories we tell. I also wanted to take his analysis (done in d3.js) and demonstrate how it could be replicated in <code>R</code>.</p>
<p>I think the key takeaway from this post is that investigating the residuals from a model can result in some revelatory findings from a data analysis. Our models will <a href="https://en.wikipedia.org/wiki/All_models_are_wrong">always be wrong</a>, but understanding and telling a story about <em>what</em> they got wrong can make us feel a little better about that fact.</p>
<p>Thanks for reading through this post. If you want to look at the code for this analysis, you can find it <a href="https://github.com/bgstieber/files_for_blog/tree/master/election-map">on my GitHub</a>. Let me know what you thought about my post. Did any of the maps surprise you?</p>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>Everything I Know About Machine Learning I Learned from Making Soup</title>
      <link>/post/everything-i-know-about-machine-learning-i-learned-from-making-soup/</link>
      <pubDate>August 6, 2018</pubDate>
      
      <guid>/post/everything-i-know-about-machine-learning-i-learned-from-making-soup/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this post, I’m going to make the claim that we can simplify some parts of the machine learning process by using the analogy of making soup. I think this analogy can improve how a data scientist explains machine learning to a broad audience, and it provides a helpful framework throughout the model building process.&lt;/p&gt;
&lt;p&gt;Relying on some insight from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining&#34;&gt;CRISP-DM framework&lt;/a&gt;, my own experience as an amateur chef, and the well-known &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/iris&#34;&gt;iris data set&lt;/a&gt;, I’m going to explain why I think that the soup making and machine learning connection is a pretty decent first approximation you could use to understand the machine learning process.&lt;/p&gt;
&lt;p&gt;This post is pretty light on code, with just a few code chunks for illustrative purposes. These are the packages we’ll need.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(glmnet)
library(caret) # caret or carrot? :)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code for this post can be found on &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/tree/master/soup-machine-learning&#34;&gt;my GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some Background&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png#/media/File:CRISP-DM_Process_Diagram.png&#34;&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png&#34; alt=&#34;CRISP-DM Process Diagram.png&#34; height=&#34;400&#34; width=&#34;400&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;strong&gt;The CRISP-DM Framework (Kenneth Jensen)&lt;/strong&gt; &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/3.0&#34; title=&#34;Creative Commons Attribution-Share Alike 3.0&#34;&gt;CC BY-SA 3.0&lt;/a&gt;, &lt;a href=&#34;https://commons.wikimedia.org/w/index.php?curid=24930610&#34;&gt;Link&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;I recently gave a presentation on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining&#34;&gt;CRISP-DM framework&lt;/a&gt; to the various teams that make up the IT Department at my organization. While I was discussing the Modeling phase of CRISP-DM, I got some questions that come up when you talk about data science with software-minded audiences.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;When you’re building a model, what are you doing? Where are you spending your time? How long does that take?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The people in IT know that data, machine learning, and artificial intelligence are impacting their daily lives, from &lt;a href=&#34;https://www.kdnuggets.com/2017/08/deep-learning-train-chatbot-talk-like-me.html&#34;&gt;chat bots&lt;/a&gt; to &lt;a href=&#34;https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering&#34;&gt;spam email detection&lt;/a&gt; to the &lt;a href=&#34;http://fortune.com/facebook-machine-learning/&#34;&gt;curation of news feeds&lt;/a&gt;. While they have awareness of the &lt;em&gt;impact&lt;/em&gt; of data science, they may not have as much awareness of the &lt;em&gt;processes&lt;/em&gt; of data science. I think the responsibility of demystifying machine learning rests on data scientists, and it’s imperative to have comprehensible mental models that can be employed to describe and de-clutter the machine learning process.&lt;/p&gt;
&lt;p&gt;In my response to the questions, I thought I did a fairly good job of breaking down the three components of machine learning and the typical amount of iteration within each component by mirroring the CRISP-DM breakdown:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Problem type and associated modeling technique
&lt;ul&gt;
&lt;li&gt;Iteration level: low&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Parameter tuning
&lt;ul&gt;
&lt;li&gt;Iteration level: high&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Feature engineering and selection
&lt;ul&gt;
&lt;li&gt;Iteration level: high&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, I was in a room full of very smart people, trying to extemporaneously explain parameter tuning and feature engineering in a coherent way, so I probably could have done a better job.&lt;/p&gt;
&lt;p&gt;A few minutes after the meeting, I realized I could have used a simple analogy to explain the machine learning process.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Machine learning is like making a soup. First, you pick the type of soup you want to make. Second, you figure out the ingredients that are going to be in the soup and how they should be prepared. Third, you determine how you’re going to cook the soup. Finally, you taste the soup and iterate to make it taste better.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;making-a-soup-machine-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Making a Soup = Machine Learning&lt;/h1&gt;
&lt;p&gt;While I’m certainly not an expert chef, I think you can boil down making a soup into a few simple components.&lt;/p&gt;
&lt;iframe src=&#34;https://giphy.com/embed/xT9DPhWvvzbSI5vrYQ&#34; width=&#34;480&#34; height=&#34;270&#34; frameBorder=&#34;0&#34; class=&#34;giphy-embed&#34; allowFullScreen&gt;
&lt;/iframe&gt;
&lt;p&gt;
&lt;a href=&#34;https://giphy.com/gifs/cravetvcanada-seinfeld-xT9DPhWvvzbSI5vrYQ&#34;&gt;via GIPHY&lt;/a&gt;
&lt;/p&gt;
&lt;div id=&#34;picking-the-soup-selecting-a-modeling-technique&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Picking the Soup = Selecting a Modeling Technique&lt;/h2&gt;
&lt;p&gt;In this step, we’re just trying to figure out what we want to make. In the machine learning world, this is where we need to think carefully about the problem we’re trying to solve, and which of the many machine learning algorithms can be used to attack it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Soup Making:&lt;/strong&gt; what type of soup are we trying to make? are there external characteristics (season, weather, mood) we should consider? what soup will we enjoy? how difficult is this soup to make?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning:&lt;/strong&gt; what type of question(s) are we trying to answer? what type of model will allow us to answer this question? how is this model implemented? what are its assumptions?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ingredients-feature-engineering-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ingredients = Feature Engineering &amp;amp; Selection&lt;/h2&gt;
&lt;p&gt;Now that we’ve decided what we’re going to make, we need to head to the grocery store and pick up the ingredients. After that we’ll need to prepare the ingredients for cooking. Similarly, we’ll need to understand the variables and context for our data set, and create/transform/aggregate our variables to get them into a useful form for modeling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Soup Making:&lt;/strong&gt; what vegetables are needed and how should they be prepped? what type of protein will we be using? do we need some type of stock for the soup?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning:&lt;/strong&gt; what variables are needed for this model? do we need to standardize any of the variables? are there non-numeric variables? if so, how should those be handled?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cooking-methods-parameter-tuning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cooking Methods = Parameter Tuning&lt;/h2&gt;
&lt;p&gt;Once we’ve decided upon the type of soup and the ingredient, it comes time to make the soup. This step involves select the best combination of different cooking methods to make the optimal (i.e. most tasty) soup. When we perform parameter tuning, we’re trying to pick the best combination of values to make our machine learning algorithms reach optimal performance. These values are different from the variables we previously discussed, as the variables are the &lt;em&gt;inputs&lt;/em&gt; for our ML algorithms, while the tuning parameters describe the algorithm itself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Soup Making:&lt;/strong&gt; what heat are we cooking at and for how long? is the pot covered or uncovered? will the pot be on the stove top for the entirety of cooking or will we move it to the oven? how long will we let the soup simmer? how big of a batch are we making?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning:&lt;/strong&gt; what is our loss function? is there a learning rate? what’s the k in our k-fold cross validation? how many trees in our random forest? how much should we penalize complexity? what is the training/validation split? does an ensemble model outperform a single model?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building a Model&lt;/h1&gt;
&lt;p&gt;Let’s see this framework in action. I’m going to pick a straightforward classification task to demonstrate. I’m going to use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;iris dataset&lt;/a&gt; and try to predict whether a flower is from the setosa species.&lt;/p&gt;
&lt;p&gt;Here’s a quick look at the data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Sepal.Length&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Sepal.Width&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Petal.Length&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Petal.Width&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Species&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5.1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3.0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;versicolor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6.0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2.2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5.0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;virginica&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5.1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6.6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3.0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;versicolor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4.9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;virginica&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6.3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3.3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;versicolor&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-06-everything-i-know-about-machine-learning-i-learned-from-making-soup_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Just looking at the &lt;a href=&#34;https://www.itl.nist.gov/div898/handbook/eda/section3/scatterb.htm&#34;&gt;scatterplot matrix&lt;/a&gt; above, we can see that trying to classify flowers into the setosa species is a bit of a toy problem (the red points are well-separated from the blue and green). In fact, just by examining if the petal length of a flower is smaller than 2.45, we can determine if the flower is from the setosa species.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(&amp;#39;Petal Cut&amp;#39; = iris$Petal.Length &amp;gt;= 2.45,&amp;#39;Setosa&amp;#39; = iris$Species == &amp;#39;setosa&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Setosa
## Petal Cut FALSE TRUE
##     FALSE     0   50
##     TRUE    100    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The iris data is used as a &lt;a href=&#34;https://en.wikipedia.org/wiki/%22Hello,_World!%22_program&#34;&gt;“hello, world”&lt;/a&gt; in data science. It has nice applications across a broad spectrum of applications: &lt;a href=&#34;https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html&#34;&gt;clustering&lt;/a&gt;, &lt;a href=&#34;https://warwick.ac.uk/fac/sci/moac/people/students/peter_cock/r/iris_lm/&#34;&gt;regression&lt;/a&gt;, &lt;a href=&#34;http://michael.hahsler.net/SMU/EMIS7332/R/logistic_regression.html&#34;&gt;classification&lt;/a&gt;, and &lt;a href=&#34;https://bl.ocks.org/mbostock/4063663&#34;&gt;visualization&lt;/a&gt;. It’s worth getting &lt;a href=&#34;https://eagereyes.org/blog/2018/how-to-get-excited-about-standard-datasets&#34;&gt;excited&lt;/a&gt; about!&lt;/p&gt;
&lt;div id=&#34;picking-the-soup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Picking the Soup&lt;/h2&gt;
&lt;p&gt;As discussed earlier, the problem we’re trying to tackle is predicting if a flower is from the setosa species. Since we already know that there is something we’re trying to predict, we only have to explore supervised machine learning algorithms. We also know that we’re not interested in building a model which generates predictions on a continuous range. We only want the answer a &lt;em&gt;binary&lt;/em&gt; question: is this a setosa or not?&lt;/p&gt;
&lt;p&gt;This narrows the set of algorithms even further, and we only need to explore models which will either generate a classification for a flower, or will generate a probability of the flower’s species being setosa.&lt;/p&gt;
&lt;p&gt;For this post, I’m going to use &lt;a href=&#34;https://en.wikipedia.org/wiki/Elastic_net_regularization&#34;&gt;elastic net&lt;/a&gt; logistic regression. Elastic net fits a logistic regression while also penalizing the complexity of the model. The excellent &lt;a href=&#34;https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html&#34;&gt;&lt;strong&gt;&lt;code&gt;glmnet&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package allows us to build models in this fashion. I’m going to use ridge regression (elastic net with &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0\)&lt;/span&gt;), which penalizes the &lt;span class=&#34;math inline&#34;&gt;\(L_2\)&lt;/span&gt; norm of the coefficients. Fitting an elastic net with &lt;span class=&#34;math inline&#34;&gt;\(\alpha=1\)&lt;/span&gt; generates a LASSO model, which can also be useful for variable selection, but I’m using a different technique to do variable selection in this post, so I’m sticking with ridge here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ingredients&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ingredients&lt;/h2&gt;
&lt;p&gt;Feature engineering and selection is one of the most time-consuming parts of the machine learning process. To keep this post brief, I’m going to go through just a few feature engineering steps.&lt;/p&gt;
&lt;p&gt;First, we split the data into &lt;a href=&#34;https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets&#34;&gt;training and testing&lt;/a&gt; sets. After this, I extract only the numeric predictor variables for the model, and then add squared terms as well as interactions between each of the first order effects. These two steps transform the predictor matrix from four columns into fourteen. Finally, we use the &lt;code&gt;preProcess&lt;/code&gt; function from the &lt;a href=&#34;https://topepo.github.io/caret/index.html&#34;&gt;&lt;strong&gt;&lt;code&gt;caret&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package to center and scale each variable so that it has mean = 0 and variance = 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
# create training and testing split
training_index &amp;lt;- sample(nrow(iris), 0.7 * nrow(iris))
iris_train &amp;lt;- iris[training_index,]
iris_test &amp;lt;- iris[-training_index,]
# grab X data add squared term for each column
iris_X_train &amp;lt;- iris_train[,-5] %&amp;gt;% mutate_all(funs(&amp;#39;sq&amp;#39; = . ^ 2))
iris_X_test &amp;lt;- iris_test[,-5] %&amp;gt;% mutate_all(funs(&amp;#39;sq&amp;#39; = . ^ 2))
# all two way interactions with first order terms
iris_X_train &amp;lt;- 
  model.matrix(~-1 + . + (Sepal.Length+Sepal.Width+Petal.Length+Petal.Width) ^ 2,
               data = iris_X_train)
iris_X_test &amp;lt;-
  model.matrix(~-1 + . + (Sepal.Length+Sepal.Width+Petal.Length+Petal.Width) ^ 2,
               data = iris_X_test)
# fit center and scale processor on training data
preProc &amp;lt;- preProcess(iris_X_train)
iris_X_train_cs &amp;lt;- predict(preProc, iris_X_train)
iris_X_test_cs &amp;lt;- predict(preProc, iris_X_test)
# labels, convert to factor for glmnet
iris_y_train &amp;lt;- as.factor(as.numeric(iris_train$Species == &amp;#39;setosa&amp;#39;))
iris_y_test &amp;lt;- as.factor(iris_test$Species == &amp;#39;setosa&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data augmentation steps I went through created numeric variables, but we could use decision trees or similar techniques to create categorical variables from numeric. We use those methods when it’s unnecessary to retain the continuous nature of a numeric variable (often the case with age or physiological measurements like height or weight).&lt;/p&gt;
&lt;p&gt;After we’ve gone through the feature engineering step, we can think about which variables we’ll actually want to use in our model. There can be considerable back-and-forth between feature engineering and feature selection, just like iterating on a recipe may involve different ingredients and different ways of preparing those ingredients.&lt;/p&gt;
&lt;p&gt;To do feature selection, I’m going to once again turn to the &lt;strong&gt;&lt;code&gt;caret&lt;/code&gt;&lt;/strong&gt; package and use the &lt;code&gt;rfe&lt;/code&gt; function. We could also use the infrastructure of the &lt;strong&gt;&lt;code&gt;glmnet&lt;/code&gt;&lt;/strong&gt; package to do some feature selection, as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Lasso_(statistics)&#34;&gt;LASSO&lt;/a&gt; helps us perform variable selection.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
rf_control &amp;lt;- rfeControl(rfFuncs, method = &amp;#39;cv&amp;#39;, number = 5)

iris_rfe &amp;lt;- rfe(x = iris_X_train_cs, 
                y = iris_y_train,
                sizes = 2:14, # select at least two variables
                rfeControl = rf_control)

iris_rfe$optVariables&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Petal.Length:Petal.Width&amp;quot; &amp;quot;Sepal.Length:Petal.Width&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The procedure we used for variable selection suggested an interaction between the petal length and petal width variables, and petal length squared. It’s usually a bad idea to include higher ordered terms without also including the lower order terms, so our final model will have three variables: petal width, petal length, and petal width * petal length interaction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cooking-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cooking Methods&lt;/h2&gt;
&lt;p&gt;In elastic net regression, we have two parameters to select: &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; controls the weight we give to the &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L_2\)&lt;/span&gt; penalties (with &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; being placed on the &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt; norm, and &lt;span class=&#34;math inline&#34;&gt;\(1-\alpha\)&lt;/span&gt; being placed on the &lt;span class=&#34;math inline&#34;&gt;\(L_2\)&lt;/span&gt; norm). Putting all the weight on &lt;span class=&#34;math inline&#34;&gt;\(L_2\)&lt;/span&gt; norm is better known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Tikhonov_regularization&#34;&gt;Tikhonov regularization or ridge regression&lt;/a&gt;. I’ve already made my choice of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; for this post, so we don’t have to tune it.&lt;/p&gt;
&lt;p&gt;The other parameter we need to tune is &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, which controls the strength of the penalty we’ll place on the &lt;span class=&#34;math inline&#34;&gt;\(L_2\)&lt;/span&gt; norm of the coefficients. A higher &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; value will “shrink” the model’s coefficients, whereas a smaller &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; value will result in a model more similar to the standard unregularized logistic regression fit.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;cv.glmnet&lt;/code&gt; function allows us to use cross-validation to tune &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vv &amp;lt;- c(&amp;quot;Petal.Width&amp;quot;, &amp;quot;Petal.Length&amp;quot;, &amp;quot;Petal.Length:Petal.Width&amp;quot;)

iris_X_train_cs_sub &amp;lt;- iris_X_train_cs[, colnames(iris_X_train_cs) %in% vv]
iris_X_test_cs_sub &amp;lt;- iris_X_test_cs[, colnames(iris_X_test_cs) %in% vv]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_glm1 &amp;lt;- cv.glmnet(x = iris_X_train_cs_sub, y = iris_y_train,
                     alpha = 0, family = &amp;#39;binomial&amp;#39;,
                     nfolds = length(iris_y_train) - 1,# LOOCV
                     standardize = FALSE, intercept = FALSE) 

lambda_1se_1 &amp;lt;- cv_glm1$lambda.1se # store &amp;quot;best&amp;quot; lambda for now

plot(cv_glm1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-06-everything-i-know-about-machine-learning-i-learned-from-making-soup_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can explore &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; a bit more to improve the model fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_glm2 &amp;lt;- cv.glmnet(x = iris_X_train_cs_sub, y = iris_y_train,
                     alpha = 0, lambda = exp(seq(-10, log(lambda_1se_1), length.out = 500)),
                     family = &amp;#39;binomial&amp;#39;, nfolds = length(iris_y_train) - 1,# LOOCV
                     standardize = FALSE, intercept = FALSE)

lambda_2 &amp;lt;- exp(-6.5)
plot(cv_glm2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-06-everything-i-know-about-machine-learning-i-learned-from-making-soup_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Cross-validation suggests a rather small value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, indicating that the model’s fit doesn’t improve with a high degree of regularization. In the next section, we’ll use two values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; to fit the model (&lt;span class=&#34;math inline&#34;&gt;\(\lambda_1\)&lt;/span&gt; = 0.0698, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_2\)&lt;/span&gt; = 0.0015), and then investigate the results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tasting-the-soup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tasting the soup&lt;/h2&gt;
&lt;p&gt;After we fit the model (or make the soup), we have to determine how good it is. For this example, I’m going to use a simple method for determining the classification accuracy and check the % of time the classifier got the species correct. Using this metric implies that we’re treating false positive and false negatives as equally bad errors. In most real world situations, this is not the case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# naive classification accuracy
class_acc &amp;lt;- function(preds, labels, thresh = 0.5){
  tt &amp;lt;- table(preds &amp;gt; thresh, labels)
  sum(diag(tt)) / sum(tt)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to the two models fit using regularization, I’m going to fit an unregularized model with the selected features along with an unregularized model with only the variables that were present in the original dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# regularized models
ridge_1 &amp;lt;- glmnet(x = iris_X_train_cs_sub, y = iris_y_train,
                  family = &amp;#39;binomial&amp;#39;, lambda = lambda_1se_1, alpha = 0,
                  standardize = FALSE, intercept = FALSE)
ridge_2 &amp;lt;- glmnet(x = iris_X_train_cs_sub, y = iris_y_train,
                  family = &amp;#39;binomial&amp;#39;, lambda = lambda_2, alpha = 0,
                  standardize = FALSE, intercept = FALSE)
# unregularized
glm1 &amp;lt;- glm(iris_y_train ~ -1 + ., data = data.frame(iris_X_train_cs_sub), 
            family = &amp;#39;binomial&amp;#39;)
glm2 &amp;lt;- glm(iris_y_train ~ -1 + Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
            data = data.frame(iris_X_train_cs), 
            family = &amp;#39;binomial&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-12&#34;&gt;Table 1: &lt;/span&gt;Table of Model Accuracy&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;model&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;type&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;class accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ridge_1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;regularized-lambda1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;84.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ridge_2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;regularized-lambda2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;95.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;glm1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;full&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;100.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;glm2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;original&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;100.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the table above, we can see that the unregularized models resulted in better predictions on our test set. This shouldn’t be too surprising, as this classification example is somewhat contrived. In more realistic settings, we may see better predictive performance from models fit using regularization.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wrapping Up&lt;/h1&gt;
&lt;p&gt;When we compare making a soup to machine learning, we get a simple and understandable lens through which we can look at machine learning. Just like making soup or cooking in general, iteration is a key component of machine learning. If you ask anyone that’s trying to develop a recipe, they probably won’t get it right the first time. If they do get it right the first time, maybe that’s because&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;they got lucky&lt;/li&gt;
&lt;li&gt;they aren’t trying to make too difficult of a dish&lt;/li&gt;
&lt;li&gt;they’re an experienced cook&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These situations have clear parallels to machine learning. Maybe you got lucky and your prediction task is fairly easy or maybe all you need is a simple model or maybe you’re an experienced data scientist.&lt;/p&gt;
&lt;p&gt;I feel like this analogy is a pretty straightforward way to explain machine learning to a broad audience of people that are interested in the topic. I found a &lt;a href=&#34;https://www.becomingadatascientist.com/2017/07/17/introductory-machine-learning-terminology-with-food/&#34;&gt;similar article&lt;/a&gt; which discussed ideas that are related to the ones I talked about in my post. If you know of any other posts with similar sentiments, I hope you’ll share them with me!&lt;/p&gt;
&lt;p&gt;Thanks for reading my post and leave a comment below if you have any thoughts or feedback!&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded><div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>In this post, I’m going to make the claim that we can simplify some parts of the machine learning process by using the analogy of making soup. I think this analogy can improve how a data scientist explains machine learning to a broad audience, and it provides a helpful framework throughout the model building process.</p>
<p>Relying on some insight from the <a href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining">CRISP-DM framework</a>, my own experience as an amateur chef, and the well-known <a href="https://archive.ics.uci.edu/ml/datasets/iris">iris data set</a>, I’m going to explain why I think that the soup making and machine learning connection is a pretty decent first approximation you could use to understand the machine learning process.</p>
<p>This post is pretty light on code, with just a few code chunks for illustrative purposes. These are the packages we’ll need.</p>
<pre class="r"><code>library(tidyverse)
library(glmnet)
library(caret) # caret or carrot? :)</code></pre>
<p>The code for this post can be found on <a href="https://github.com/bgstieber/files_for_blog/tree/master/soup-machine-learning">my GitHub</a>.</p>
</div>
<div id="some-background" class="section level1">
<h1>Some Background</h1>
<p>
<a href="https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png#/media/File:CRISP-DM_Process_Diagram.png"><img src="https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png" alt="CRISP-DM Process Diagram.png" height="400" width="400"></a><br> <strong>The CRISP-DM Framework (Kenneth Jensen)</strong> <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=24930610">Link</a>
</p>
<p>I recently gave a presentation on the <a href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining">CRISP-DM framework</a> to the various teams that make up the IT Department at my organization. While I was discussing the Modeling phase of CRISP-DM, I got some questions that come up when you talk about data science with software-minded audiences.</p>
<p><em>When you’re building a model, what are you doing? Where are you spending your time? How long does that take?</em></p>
<p>The people in IT know that data, machine learning, and artificial intelligence are impacting their daily lives, from <a href="https://www.kdnuggets.com/2017/08/deep-learning-train-chatbot-talk-like-me.html">chat bots</a> to <a href="https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering">spam email detection</a> to the <a href="http://fortune.com/facebook-machine-learning/">curation of news feeds</a>. While they have awareness of the <em>impact</em> of data science, they may not have as much awareness of the <em>processes</em> of data science. I think the responsibility of demystifying machine learning rests on data scientists, and it’s imperative to have comprehensible mental models that can be employed to describe and de-clutter the machine learning process.</p>
<p>In my response to the questions, I thought I did a fairly good job of breaking down the three components of machine learning and the typical amount of iteration within each component by mirroring the CRISP-DM breakdown:</p>
<ul>
<li>Problem type and associated modeling technique
<ul>
<li>Iteration level: low</li>
</ul></li>
<li>Parameter tuning
<ul>
<li>Iteration level: high</li>
</ul></li>
<li>Feature engineering and selection
<ul>
<li>Iteration level: high</li>
</ul></li>
</ul>
<p>Of course, I was in a room full of very smart people, trying to extemporaneously explain parameter tuning and feature engineering in a coherent way, so I probably could have done a better job.</p>
<p>A few minutes after the meeting, I realized I could have used a simple analogy to explain the machine learning process.</p>
<p><strong>Machine learning is like making a soup. First, you pick the type of soup you want to make. Second, you figure out the ingredients that are going to be in the soup and how they should be prepared. Third, you determine how you’re going to cook the soup. Finally, you taste the soup and iterate to make it taste better.</strong></p>
</div>
<div id="making-a-soup-machine-learning" class="section level1">
<h1>Making a Soup = Machine Learning</h1>
<p>While I’m certainly not an expert chef, I think you can boil down making a soup into a few simple components.</p>
<iframe src="https://giphy.com/embed/xT9DPhWvvzbSI5vrYQ" width="480" height="270" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>
<a href="https://giphy.com/gifs/cravetvcanada-seinfeld-xT9DPhWvvzbSI5vrYQ">via GIPHY</a>
</p>
<div id="picking-the-soup-selecting-a-modeling-technique" class="section level2">
<h2>Picking the Soup = Selecting a Modeling Technique</h2>
<p>In this step, we’re just trying to figure out what we want to make. In the machine learning world, this is where we need to think carefully about the problem we’re trying to solve, and which of the many machine learning algorithms can be used to attack it.</p>
<p><strong>Soup Making:</strong> what type of soup are we trying to make? are there external characteristics (season, weather, mood) we should consider? what soup will we enjoy? how difficult is this soup to make?</p>
<p><strong>Machine Learning:</strong> what type of question(s) are we trying to answer? what type of model will allow us to answer this question? how is this model implemented? what are its assumptions?</p>
</div>
<div id="ingredients-feature-engineering-selection" class="section level2">
<h2>Ingredients = Feature Engineering &amp; Selection</h2>
<p>Now that we’ve decided what we’re going to make, we need to head to the grocery store and pick up the ingredients. After that we’ll need to prepare the ingredients for cooking. Similarly, we’ll need to understand the variables and context for our data set, and create/transform/aggregate our variables to get them into a useful form for modeling.</p>
<p><strong>Soup Making:</strong> what vegetables are needed and how should they be prepped? what type of protein will we be using? do we need some type of stock for the soup?</p>
<p><strong>Machine Learning:</strong> what variables are needed for this model? do we need to standardize any of the variables? are there non-numeric variables? if so, how should those be handled?</p>
</div>
<div id="cooking-methods-parameter-tuning" class="section level2">
<h2>Cooking Methods = Parameter Tuning</h2>
<p>Once we’ve decided upon the type of soup and the ingredient, it comes time to make the soup. This step involves select the best combination of different cooking methods to make the optimal (i.e. most tasty) soup. When we perform parameter tuning, we’re trying to pick the best combination of values to make our machine learning algorithms reach optimal performance. These values are different from the variables we previously discussed, as the variables are the <em>inputs</em> for our ML algorithms, while the tuning parameters describe the algorithm itself.</p>
<p><strong>Soup Making:</strong> what heat are we cooking at and for how long? is the pot covered or uncovered? will the pot be on the stove top for the entirety of cooking or will we move it to the oven? how long will we let the soup simmer? how big of a batch are we making?</p>
<p><strong>Machine Learning:</strong> what is our loss function? is there a learning rate? what’s the k in our k-fold cross validation? how many trees in our random forest? how much should we penalize complexity? what is the training/validation split? does an ensemble model outperform a single model?</p>
</div>
</div>
<div id="building-a-model" class="section level1">
<h1>Building a Model</h1>
<p>Let’s see this framework in action. I’m going to pick a straightforward classification task to demonstrate. I’m going to use the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris dataset</a> and try to predict whether a flower is from the setosa species.</p>
<p>Here’s a quick look at the data:</p>
<table>
<thead>
<tr class="header">
<th align="left">Sepal.Length</th>
<th align="left">Sepal.Width</th>
<th align="left">Petal.Length</th>
<th align="left">Petal.Width</th>
<th align="left">Species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">5.1</td>
<td align="left">2.5</td>
<td align="left">3.0</td>
<td align="left">1.1</td>
<td align="left">versicolor</td>
</tr>
<tr class="even">
<td align="left">6.0</td>
<td align="left">2.2</td>
<td align="left">5.0</td>
<td align="left">1.5</td>
<td align="left">virginica</td>
</tr>
<tr class="odd">
<td align="left">5.1</td>
<td align="left">3.5</td>
<td align="left">1.4</td>
<td align="left">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="left">6.6</td>
<td align="left">3.0</td>
<td align="left">4.4</td>
<td align="left">1.4</td>
<td align="left">versicolor</td>
</tr>
<tr class="odd">
<td align="left">4.9</td>
<td align="left">2.5</td>
<td align="left">4.5</td>
<td align="left">1.7</td>
<td align="left">virginica</td>
</tr>
<tr class="even">
<td align="left">6.3</td>
<td align="left">3.3</td>
<td align="left">4.7</td>
<td align="left">1.6</td>
<td align="left">versicolor</td>
</tr>
</tbody>
</table>
<p><img src="/post/2018-08-06-everything-i-know-about-machine-learning-i-learned-from-making-soup_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Just looking at the <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/scatterb.htm">scatterplot matrix</a> above, we can see that trying to classify flowers into the setosa species is a bit of a toy problem (the red points are well-separated from the blue and green). In fact, just by examining if the petal length of a flower is smaller than 2.45, we can determine if the flower is from the setosa species.</p>
<pre class="r"><code>table(&#39;Petal Cut&#39; = iris$Petal.Length &gt;= 2.45,&#39;Setosa&#39; = iris$Species == &#39;setosa&#39;)</code></pre>
<pre><code>##          Setosa
## Petal Cut FALSE TRUE
##     FALSE     0   50
##     TRUE    100    0</code></pre>
<p>The iris data is used as a <a href="https://en.wikipedia.org/wiki/%22Hello,_World!%22_program">“hello, world”</a> in data science. It has nice applications across a broad spectrum of applications: <a href="https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html">clustering</a>, <a href="https://warwick.ac.uk/fac/sci/moac/people/students/peter_cock/r/iris_lm/">regression</a>, <a href="http://michael.hahsler.net/SMU/EMIS7332/R/logistic_regression.html">classification</a>, and <a href="https://bl.ocks.org/mbostock/4063663">visualization</a>. It’s worth getting <a href="https://eagereyes.org/blog/2018/how-to-get-excited-about-standard-datasets">excited</a> about!</p>
<div id="picking-the-soup" class="section level2">
<h2>Picking the Soup</h2>
<p>As discussed earlier, the problem we’re trying to tackle is predicting if a flower is from the setosa species. Since we already know that there is something we’re trying to predict, we only have to explore supervised machine learning algorithms. We also know that we’re not interested in building a model which generates predictions on a continuous range. We only want the answer a <em>binary</em> question: is this a setosa or not?</p>
<p>This narrows the set of algorithms even further, and we only need to explore models which will either generate a classification for a flower, or will generate a probability of the flower’s species being setosa.</p>
<p>For this post, I’m going to use <a href="https://en.wikipedia.org/wiki/Elastic_net_regularization">elastic net</a> logistic regression. Elastic net fits a logistic regression while also penalizing the complexity of the model. The excellent <a href="https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"><strong><code>glmnet</code></strong></a> package allows us to build models in this fashion. I’m going to use ridge regression (elastic net with <span class="math inline">\(\alpha=0\)</span>), which penalizes the <span class="math inline">\(L_2\)</span> norm of the coefficients. Fitting an elastic net with <span class="math inline">\(\alpha=1\)</span> generates a LASSO model, which can also be useful for variable selection, but I’m using a different technique to do variable selection in this post, so I’m sticking with ridge here.</p>
</div>
<div id="ingredients" class="section level2">
<h2>Ingredients</h2>
<p>Feature engineering and selection is one of the most time-consuming parts of the machine learning process. To keep this post brief, I’m going to go through just a few feature engineering steps.</p>
<p>First, we split the data into <a href="https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets">training and testing</a> sets. After this, I extract only the numeric predictor variables for the model, and then add squared terms as well as interactions between each of the first order effects. These two steps transform the predictor matrix from four columns into fourteen. Finally, we use the <code>preProcess</code> function from the <a href="https://topepo.github.io/caret/index.html"><strong><code>caret</code></strong></a> package to center and scale each variable so that it has mean = 0 and variance = 1.</p>
<pre class="r"><code>set.seed(123)
# create training and testing split
training_index &lt;- sample(nrow(iris), 0.7 * nrow(iris))
iris_train &lt;- iris[training_index,]
iris_test &lt;- iris[-training_index,]
# grab X data add squared term for each column
iris_X_train &lt;- iris_train[,-5] %&gt;% mutate_all(funs(&#39;sq&#39; = . ^ 2))
iris_X_test &lt;- iris_test[,-5] %&gt;% mutate_all(funs(&#39;sq&#39; = . ^ 2))
# all two way interactions with first order terms
iris_X_train &lt;- 
  model.matrix(~-1 + . + (Sepal.Length+Sepal.Width+Petal.Length+Petal.Width) ^ 2,
               data = iris_X_train)
iris_X_test &lt;-
  model.matrix(~-1 + . + (Sepal.Length+Sepal.Width+Petal.Length+Petal.Width) ^ 2,
               data = iris_X_test)
# fit center and scale processor on training data
preProc &lt;- preProcess(iris_X_train)
iris_X_train_cs &lt;- predict(preProc, iris_X_train)
iris_X_test_cs &lt;- predict(preProc, iris_X_test)
# labels, convert to factor for glmnet
iris_y_train &lt;- as.factor(as.numeric(iris_train$Species == &#39;setosa&#39;))
iris_y_test &lt;- as.factor(iris_test$Species == &#39;setosa&#39;)</code></pre>
<p>The data augmentation steps I went through created numeric variables, but we could use decision trees or similar techniques to create categorical variables from numeric. We use those methods when it’s unnecessary to retain the continuous nature of a numeric variable (often the case with age or physiological measurements like height or weight).</p>
<p>After we’ve gone through the feature engineering step, we can think about which variables we’ll actually want to use in our model. There can be considerable back-and-forth between feature engineering and feature selection, just like iterating on a recipe may involve different ingredients and different ways of preparing those ingredients.</p>
<p>To do feature selection, I’m going to once again turn to the <strong><code>caret</code></strong> package and use the <code>rfe</code> function. We could also use the infrastructure of the <strong><code>glmnet</code></strong> package to do some feature selection, as the <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">LASSO</a> helps us perform variable selection.</p>
<pre class="r"><code>set.seed(123)
rf_control &lt;- rfeControl(rfFuncs, method = &#39;cv&#39;, number = 5)

iris_rfe &lt;- rfe(x = iris_X_train_cs, 
                y = iris_y_train,
                sizes = 2:14, # select at least two variables
                rfeControl = rf_control)

iris_rfe$optVariables</code></pre>
<pre><code>## [1] &quot;Petal.Length:Petal.Width&quot; &quot;Sepal.Length:Petal.Width&quot;</code></pre>
<p>The procedure we used for variable selection suggested an interaction between the petal length and petal width variables, and petal length squared. It’s usually a bad idea to include higher ordered terms without also including the lower order terms, so our final model will have three variables: petal width, petal length, and petal width * petal length interaction.</p>
</div>
<div id="cooking-methods" class="section level2">
<h2>Cooking Methods</h2>
<p>In elastic net regression, we have two parameters to select: <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span>. <span class="math inline">\(\alpha\)</span> controls the weight we give to the <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> penalties (with <span class="math inline">\(\alpha\)</span> being placed on the <span class="math inline">\(L_1\)</span> norm, and <span class="math inline">\(1-\alpha\)</span> being placed on the <span class="math inline">\(L_2\)</span> norm). Putting all the weight on <span class="math inline">\(L_2\)</span> norm is better known as <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov regularization or ridge regression</a>. I’ve already made my choice of <span class="math inline">\(\alpha\)</span> for this post, so we don’t have to tune it.</p>
<p>The other parameter we need to tune is <span class="math inline">\(\lambda\)</span>, which controls the strength of the penalty we’ll place on the <span class="math inline">\(L_2\)</span> norm of the coefficients. A higher <span class="math inline">\(\lambda\)</span> value will “shrink” the model’s coefficients, whereas a smaller <span class="math inline">\(\lambda\)</span> value will result in a model more similar to the standard unregularized logistic regression fit.</p>
<p>The <code>cv.glmnet</code> function allows us to use cross-validation to tune <span class="math inline">\(\lambda\)</span>.</p>
<pre class="r"><code>vv &lt;- c(&quot;Petal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Length:Petal.Width&quot;)

iris_X_train_cs_sub &lt;- iris_X_train_cs[, colnames(iris_X_train_cs) %in% vv]
iris_X_test_cs_sub &lt;- iris_X_test_cs[, colnames(iris_X_test_cs) %in% vv]</code></pre>
<pre class="r"><code>cv_glm1 &lt;- cv.glmnet(x = iris_X_train_cs_sub, y = iris_y_train,
                     alpha = 0, family = &#39;binomial&#39;,
                     nfolds = length(iris_y_train) - 1,# LOOCV
                     standardize = FALSE, intercept = FALSE) 

lambda_1se_1 &lt;- cv_glm1$lambda.1se # store &quot;best&quot; lambda for now

plot(cv_glm1)</code></pre>
<p><img src="/post/2018-08-06-everything-i-know-about-machine-learning-i-learned-from-making-soup_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>We can explore <span class="math inline">\(\lambda\)</span> a bit more to improve the model fit.</p>
<pre class="r"><code>cv_glm2 &lt;- cv.glmnet(x = iris_X_train_cs_sub, y = iris_y_train,
                     alpha = 0, lambda = exp(seq(-10, log(lambda_1se_1), length.out = 500)),
                     family = &#39;binomial&#39;, nfolds = length(iris_y_train) - 1,# LOOCV
                     standardize = FALSE, intercept = FALSE)

lambda_2 &lt;- exp(-6.5)
plot(cv_glm2)</code></pre>
<p><img src="/post/2018-08-06-everything-i-know-about-machine-learning-i-learned-from-making-soup_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Cross-validation suggests a rather small value of <span class="math inline">\(\lambda\)</span>, indicating that the model’s fit doesn’t improve with a high degree of regularization. In the next section, we’ll use two values of <span class="math inline">\(\lambda\)</span> to fit the model (<span class="math inline">\(\lambda_1\)</span> = 0.0698, <span class="math inline">\(\lambda_2\)</span> = 0.0015), and then investigate the results.</p>
</div>
<div id="tasting-the-soup" class="section level2">
<h2>Tasting the soup</h2>
<p>After we fit the model (or make the soup), we have to determine how good it is. For this example, I’m going to use a simple method for determining the classification accuracy and check the % of time the classifier got the species correct. Using this metric implies that we’re treating false positive and false negatives as equally bad errors. In most real world situations, this is not the case.</p>
<pre class="r"><code># naive classification accuracy
class_acc &lt;- function(preds, labels, thresh = 0.5){
  tt &lt;- table(preds &gt; thresh, labels)
  sum(diag(tt)) / sum(tt)
}</code></pre>
<p>In addition to the two models fit using regularization, I’m going to fit an unregularized model with the selected features along with an unregularized model with only the variables that were present in the original dataset.</p>
<pre class="r"><code># regularized models
ridge_1 &lt;- glmnet(x = iris_X_train_cs_sub, y = iris_y_train,
                  family = &#39;binomial&#39;, lambda = lambda_1se_1, alpha = 0,
                  standardize = FALSE, intercept = FALSE)
ridge_2 &lt;- glmnet(x = iris_X_train_cs_sub, y = iris_y_train,
                  family = &#39;binomial&#39;, lambda = lambda_2, alpha = 0,
                  standardize = FALSE, intercept = FALSE)
# unregularized
glm1 &lt;- glm(iris_y_train ~ -1 + ., data = data.frame(iris_X_train_cs_sub), 
            family = &#39;binomial&#39;)
glm2 &lt;- glm(iris_y_train ~ -1 + Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
            data = data.frame(iris_X_train_cs), 
            family = &#39;binomial&#39;) </code></pre>
<table>
<caption><span id="tab:unnamed-chunk-12">Table 1: </span>Table of Model Accuracy</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">type</th>
<th align="left">class accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ridge_1</td>
<td align="left">regularized-lambda1</td>
<td align="left">84.4%</td>
</tr>
<tr class="even">
<td align="left">ridge_2</td>
<td align="left">regularized-lambda2</td>
<td align="left">95.6%</td>
</tr>
<tr class="odd">
<td align="left">glm1</td>
<td align="left">full</td>
<td align="left">100.0%</td>
</tr>
<tr class="even">
<td align="left">glm2</td>
<td align="left">original</td>
<td align="left">100.0%</td>
</tr>
</tbody>
</table>
<p>In the table above, we can see that the unregularized models resulted in better predictions on our test set. This shouldn’t be too surprising, as this classification example is somewhat contrived. In more realistic settings, we may see better predictive performance from models fit using regularization.</p>
</div>
</div>
<div id="wrapping-up" class="section level1">
<h1>Wrapping Up</h1>
<p>When we compare making a soup to machine learning, we get a simple and understandable lens through which we can look at machine learning. Just like making soup or cooking in general, iteration is a key component of machine learning. If you ask anyone that’s trying to develop a recipe, they probably won’t get it right the first time. If they do get it right the first time, maybe that’s because</p>
<ul>
<li>they got lucky</li>
<li>they aren’t trying to make too difficult of a dish</li>
<li>they’re an experienced cook</li>
</ul>
<p>These situations have clear parallels to machine learning. Maybe you got lucky and your prediction task is fairly easy or maybe all you need is a simple model or maybe you’re an experienced data scientist.</p>
<p>I feel like this analogy is a pretty straightforward way to explain machine learning to a broad audience of people that are interested in the topic. I found a <a href="https://www.becomingadatascientist.com/2017/07/17/introductory-machine-learning-terminology-with-food/">similar article</a> which discussed ideas that are related to the ones I talked about in my post. If you know of any other posts with similar sentiments, I hope you’ll share them with me!</p>
<p>Thanks for reading my post and leave a comment below if you have any thoughts or feedback!</p>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>Golf, Tidy Data, and Using Data Analysis to Guide Strategy</title>
      <link>/post/golf-tidy-data-and-using-data-analysis-to-guide-strategy/</link>
      <pubDate>June 24, 2018</pubDate>
      
      <guid>/post/golf-tidy-data-and-using-data-analysis-to-guide-strategy/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I’m going to use this post to discuss some of the aspects of data science that interest me most (tidy data as well as using data to guide strategy). I’ll be discussing these topics through the lens of a data analysis of results from a few high school golf tournaments.&lt;/p&gt;
&lt;p&gt;I’m going to take a little bit of time to talk about &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html&#34;&gt;&lt;strong&gt;tidy data&lt;/strong&gt;&lt;/a&gt;. When I scraped the data used for this analysis, it wasn’t really stored in a tidy format, and there’s a good reason for that. I’ll briefly discuss what makes the original data “untidy”, and what we can do to whip it into tidy shape.&lt;/p&gt;
&lt;p&gt;After that, I’ll explore the data a little bit, with the goal of using the findings from this analysis to&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;improve strategy on the golf course&lt;/li&gt;
&lt;li&gt;inspire ideas for looking deeper at the data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, I’ll show how we can use &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_model&#34;&gt;linear models&lt;/a&gt; along with &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf&#34;&gt;mixed-effects linear models&lt;/a&gt; to build statistical models which allow us to quantify differences between groups of interest.&lt;/p&gt;
&lt;p&gt;Here are the questions I hope to answer using the data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the most difficult holes? Why are they difficult?&lt;/li&gt;
&lt;li&gt;What is the easiest hole? What makes it easier than the rest?&lt;/li&gt;
&lt;li&gt;Are there clear differences in scores between regional and sectional golf tournaments?&lt;/li&gt;
&lt;li&gt;What separates the better high school golfers (those that break 90) from the other golfers?&lt;/li&gt;
&lt;li&gt;What are some general strategy guidelines for playing this golf course?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of the analysis in this post was performed using &lt;code&gt;R&lt;/code&gt;. I’ve omitted some of the code used to generate plots to improve readability. The full .Rmd file can be found &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/blob/master/golf-tidy-data/2018-05-27-golf-tidy-data-and-using-data-analysis-for-strategy.Rmd&#34;&gt;on my GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-useful-context&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some Useful Context&lt;/h2&gt;
&lt;p&gt;This analysis will involve looking at results from WIAA golf tournaments that were hosted at &lt;a href=&#34;https://www.golfpinevalley.net/&#34;&gt;Pine Valley Golf Course (PV)&lt;/a&gt;. Since 2011, either a regional or sectional tournament was played at PV for Division 3 high schools. Pine Valley is a par 71 golf course, with two par 3’s, six par 4’s and one par 5 on the front nine, and two par 3’s, five par 4’s and two par 5’s on the back nine. Here’s an image of the scorecard:&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://photos.bluegolf.com/bd/10/80/e2/4bef45efb375fe1ae2739672_l.jpg&#34;&gt;
&lt;/center&gt;
&lt;p&gt;Regional tournaments come before sectionals, with teams and individuals that played well in the regional tournament advancing to sectionals. Teams and individuals that play well in the sectional tournament will advance to the state tournament. It is a fair (and testable!) hypothesis that scores will tend to be better in sectional tournaments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hitting-the-range-show-me-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hitting the Range: Show me the data&lt;/h2&gt;
&lt;p&gt;First I’m going to load a few packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(scales)
library(lme4)
theme_set(theme_bw())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ve already scraped the data and cleaned it up a bit. I’ve thrown the code and data in a &lt;a href=&#34;https://github.com/bgstieber/files_for_blog/tree/master/golf-tidy-data&#34;&gt;github repository&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gh &amp;lt;- &amp;quot;https://raw.githubusercontent.com/bgstieber/files_for_blog/master/golf-tidy-data/data&amp;quot;
# read in data and only keep scores lower than 121
tidy_scores &amp;lt;- read_csv(paste0(gh, &amp;#39;/tidy_golf_scores.csv&amp;#39;)) %&amp;gt;%
  filter(tot &amp;lt;= 120)
untidy_scores &amp;lt;- read_csv(paste0(gh, &amp;#39;/untidy_golf_scores.csv&amp;#39;)) %&amp;gt;%
  filter(tot &amp;lt;= 120)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is comprised of 472 golfers with scores for each of the 18 holes they played. In total, there are 8496 scores in this data set.&lt;/p&gt;
&lt;p&gt;Let’s take a peek at the first few rows of each data set. The first data set we’ll look at is the untidy data, this is fairly close to what the WIAA provides on the webpages for each of the tournaments.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;year&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;6&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;7&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;8&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;9&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;out&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;10&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;11&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;12&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;13&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;14&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;15&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;16&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;17&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;18&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;in&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tot&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;tourn_type&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tourn_year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;91&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;87&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;52&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;101&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;115&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;44&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;85&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is an example of a “wide” data set.&lt;/p&gt;
&lt;p&gt;Now let’s take a look at the first few rows of the tidy data.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;year&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;out&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;in&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tot&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;tourn_type&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tourn_year&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;hole&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;par&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ob&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;water&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;side&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rel_to_par&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;91&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;front&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;87&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;front&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;52&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;101&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;front&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;front&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;115&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;front&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;44&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;85&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sectionals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;front&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;On inspection, there are some clear differences between the tidy and untidy data sets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-practice-what-makes-data-tidy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting Practice: What makes data tidy?&lt;/h2&gt;
&lt;p&gt;In some ways, recognizing tidy/untidy data can be one of those &lt;a href=&#34;https://en.wikipedia.org/wiki/I_know_it_when_I_see_it&#34;&gt;&lt;em&gt;I know it when I see it&lt;/em&gt;&lt;/a&gt; things. We can follow the fairly solid guidelines &lt;a href=&#34;https://www.jstatsoft.org/article/view/v059i10&#34;&gt;Hadley Wickham has proposed&lt;/a&gt; to make the distinction a bit more concrete (if you haven’t read that paper, I &lt;strong&gt;highly&lt;/strong&gt; recommend it):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each &lt;strong&gt;variable&lt;/strong&gt; is a column&lt;/li&gt;
&lt;li&gt;Each &lt;strong&gt;observation&lt;/strong&gt; is a row&lt;/li&gt;
&lt;li&gt;Each type of observational unit is a table (this isn’t as important for this post)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It’s important to think about those two bolded words within the context of this analysis. We’re interested in making our way around a golf course, to hopefully play our best. Playing our best means minimizing mistakes &lt;em&gt;throughout&lt;/em&gt; a round of golf.&lt;/p&gt;
&lt;p&gt;Some people like to think of golf as a game composed of 18 “mini games” within it. For an analysis with the primary focus of identifying ways to get around the course as strategically as possible, I think the best way to look at the data is to have each observational unit be the score on &lt;strong&gt;one hole&lt;/strong&gt; for &lt;strong&gt;each competitor&lt;/strong&gt;. Our tidy data set is constructed this way, with &lt;strong&gt;one row per competitor per hole&lt;/strong&gt;. The untidy data has a structure of &lt;strong&gt;one row per competitor&lt;/strong&gt;. This structure may be useful if we’re only interested in looking at final scores for each competitor. It’s also useful for a concise summary of a competitor’s performance on a website (which was its original purpose).&lt;/p&gt;
&lt;p&gt;Transforming the data from untidy to tidy is fairly simple using the &lt;a href=&#34;https://tidyr.tidyverse.org/reference/gather.html&#34;&gt;&lt;code&gt;gather&lt;/code&gt;&lt;/a&gt; function from the &lt;a href=&#34;https://tidyr.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;tidyr&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;untidy_scores %&amp;gt;%
  # only select a handful of columns to make printing easier
  select(-year, -out, -`in`, -tot, -tourn_type, -tourn_year) %&amp;gt;%
  gather(hole, score, -name)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,496 x 3
##    name  hole  score
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
##  1 A1    1         7
##  2 A2    1         5
##  3 A3    1         7
##  4 A4    1         8
##  5 A5    1        14
##  6 A6    1         6
##  7 A7    1         7
##  8 A8    1         6
##  9 A9    1         7
## 10 A10   1        11
## # ... with 8,486 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the data in a way that should be straightforward to analyze, let’s start taking a look at some summaries of the results from these competitions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;teeing-off-exploring-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Teeing Off: Exploring the Data&lt;/h2&gt;
&lt;p&gt;We could either look at the scores on each hole or we could look at the score &lt;em&gt;relative to par&lt;/em&gt; for each hole. Scores will generally be higher depending on the par of the hole. Par 3’s are shorter than par 4’s which are shorter than par 5’s, and the longer the hole the more strokes (usually) it will take to complete. Since this represents an inherent “bias” in the score data, I think it’s better to analyze the score relative to par.&lt;/p&gt;
&lt;p&gt;First, we’ll visualize the distribution of scores relative to par.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’re able to see a few things pretty quickly from the heatmap above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Holes 4 and 12 are clearly the hardest on the course, with over 50% of the field making double bogey or worse&lt;/li&gt;
&lt;li&gt;Hole 11 is the easiest on the course, with the highest percentage of the field making a birdie or better, and the lowest percentage making double or worse.&lt;/li&gt;
&lt;li&gt;About 50% of the field made a bogey on 13, which is a par 3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, we’re going to take a look at the average score relative to par for each of the 18 holes played at PV. Each of the 18 holes played at least one stroke over par.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The four hardest holes are (in order of difficulty): 12, 4, 7, and 1.&lt;/p&gt;
&lt;p&gt;PV was my home course growing up, so here’s a little &lt;em&gt;local knowledge&lt;/em&gt; about each of those holes.&lt;/p&gt;
&lt;div id=&#34;hole-12&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hole 12&lt;/h4&gt;
&lt;p&gt;12 is the longest par 4 on the course, playing at about 430 yards from the back tees. The tee shot can be intimidating with out-of-bounds along the left, and some trees and mounds on the right. This hole seemed to play into the wind more often than downwind, making it even longer than the 430 yards on the card.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hole-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hole 4&lt;/h4&gt;
&lt;p&gt;4 is rated as the hardest hole on the golf course according to the USGA handicapping system. The tee shot can be somewhat difficult, with out-of-bounds, trees, and water on the left, and more trees on the right. That being said, the hole is not overly long, so hitting something less than driver is not a bad route. The green is guarded by a stream in front of it, and hilly terrain surrounding it. Putting on this green can be difficult, as it has a lot of slope and undulation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hole-7&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hole 7&lt;/h4&gt;
&lt;p&gt;7 is a short dogleg left. The main difficulty on this hole is the tight dogleg golfers must navigate off the tee. There are trees which can block an approach shot if the tee shot veers too far right, and hills along with trees and water on the left side for those getting too aggressive off the tee. The approach into the green (provided one has a clear shot) is not that difficult, with no bunkers and a lot of room to miss. The major difficulty on this hole is the tee shot, but from there it’s fairly straightforward.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hole-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hole 1&lt;/h4&gt;
&lt;p&gt;1 is a shorter par 5 with a daunting tee shot. There’s OB on the left side, and trees and fescue along the right side. On top of that, a golfer must steady their first tee jitters and focus on hitting a solid tee shot in front of the “gallery” of other players, coaches, and spectators. After hitting the tee shot, the fun doesn’t end. The approach into the green is challenging, as trees surround the green and the hole narrows all the way into the green&lt;/p&gt;
&lt;p&gt;Each of the four most difficult holes requires a well-struck tee shot. On the 12th, it’s important to hit a long and straight shot. The 4th and 1st holes require precision and steady nerves to avoid trouble and hit a narrow fairway. The 7th requires a controlled and well-executed tee shot that is shaped right to left.&lt;/p&gt;
&lt;p&gt;The next visualization is a &lt;a href=&#34;http://datavizproject.com/data-type/lollipop-chart/&#34;&gt;lollipop chart&lt;/a&gt;, which is a great way to avoid the “visually aggressive” &lt;a href=&#34;https://en.wikipedia.org/wiki/Moir%C3%A9_pattern&#34;&gt;moire effect&lt;/a&gt; that bar charts can sometimes fall victim to without losing the perceptually sound concept of &lt;a href=&#34;https://pdfs.semanticscholar.org/565d/843c2c0e60915709268ac4224894469d82d5.pdf&#34;&gt;position along a common scale&lt;/a&gt;. &lt;a href=&#34;https://juliasilge.com&#34;&gt;Julia Silge&lt;/a&gt; uses this chart type to great effect in some of her &lt;a href=&#34;https://juliasilge.com/blog/gender-pronouns/&#34;&gt;blog posts&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We took the same data that was displayed in the previous chart, but augmented it slightly to tell a different story. The orange lollipops on the chart show the holes that have higher scores relative to par than average. Three of the four hardest holes are on the front nine, occurring in the first seven holes of the course. We can also see how much more difficult holes 1, 4, 7, and 12 are than the rest (the “sticks” of the lollipops rise much higher than the other holes which played harder than average).&lt;/p&gt;
&lt;p&gt;The easiest holes on the course can be identified as well. Hole 11 stands out as being the easiest hole at PV. Hole 11 is a straightforward par 5 with almost no trouble off the tee and a fairly generous fairway all the way to the green. The hole typically plays downwind (it’s routed in the opposite direction of 12), allowing longer hitters the option of trying to reach it in two.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;making-the-turn-statistical-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Making the Turn: Statistical Modeling&lt;/h2&gt;
&lt;p&gt;After a bit of exploratory analysis, we can move on to using statistical modeling to briefly investigate the differences between holes with a little more precision. Additionally, we can use a simple model to test the hypothesis that scores (in relation to par) will be lower at sectional tournaments as opposed to regional.&lt;/p&gt;
&lt;p&gt;First, we’ll fit a linear mixed-effects model. I haven’t really worked with mixed-effects models too much since grad school, but this model shouldn’t be too hard to describe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create factor variables for tournament year and hole to make
# modeling a bit easier
hardest_holes &amp;lt;- c(12L, 4L, 7L, 1L, 18L, 
                   15L, 10L, 3L, 13L, 
                   17L, 6L, 2L, 16L, 
                   8L, 14L, 9L, 5L, 11L)

tidy_scores2 &amp;lt;- tidy_scores %&amp;gt;%
  mutate(tourn_year_f = factor(tourn_year),
         hole_f = factor(hole,levels = hardest_holes))
## look at past performance to find most difficult holes
simple_mod &amp;lt;- lmer(rel_to_par ~ hole_f+tourn_type+(1|tourn_year_f)+(1|tourn_year_f:name),
                   data = tidy_scores2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the model above, we’re fitting &lt;em&gt;fixed effects&lt;/em&gt; for the hole variable and the tournament type variable. We’re using &lt;code&gt;(1|tourn_year_f)&lt;/code&gt; to fit a random effect for the tournament year, and using &lt;code&gt;(1|tourn_year_f:name)&lt;/code&gt; to fit a random effect for the competitor &lt;em&gt;nested within&lt;/em&gt; tournament year. A simple way to think about the fixed versus random effects divide is that if we’re interested in understanding the impact of a variable on our target, we should probably fit it as a fixed effect, if we’re not that interested a random effect is the way to go (this is a gross over-simplification, I’d see &lt;a href=&#34;https://dynamicecology.wordpress.com/2015/11/04/is-it-a-fixed-or-random-effect/&#34;&gt;this post&lt;/a&gt; if you’re looking for something more in-depth).&lt;/p&gt;
&lt;p&gt;Alright, so we fit the model, now what?&lt;/p&gt;
&lt;div id=&#34;check-out-the-coefficients&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Check out the coefficients&lt;/h3&gt;
&lt;p&gt;Let’s see what a summary of the model looks like&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(simple_mod)$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                         Estimate Std. Error    t value
## (Intercept)           1.88291299 0.09070789  20.757985
## hole_f4              -0.06991525 0.06812175  -1.026328
## hole_f7              -0.18432203 0.06812175  -2.705774
## hole_f1              -0.22881356 0.06812175  -3.358891
## hole_f18             -0.40889831 0.06812175  -6.002463
## hole_f15             -0.44279661 0.06812175  -6.500077
## hole_f10             -0.44703390 0.06812175  -6.562278
## hole_f3              -0.48728814 0.06812175  -7.153194
## hole_f13             -0.53601695 0.06812175  -7.868514
## hole_f17             -0.53813559 0.06812175  -7.899615
## hole_f6              -0.54661017 0.06812175  -8.024018
## hole_f2              -0.55296610 0.06812175  -8.117321
## hole_f16             -0.61864407 0.06812175  -9.081447
## hole_f8              -0.64406780 0.06812175  -9.454657
## hole_f14             -0.67372881 0.06812175  -9.890069
## hole_f9              -0.68008475 0.06812175  -9.983371
## hole_f5              -0.68432203 0.06812175 -10.045573
## hole_f11             -0.80084746 0.06812175 -11.756119
## tourn_typesectionals -0.06353004 0.09519262  -0.667384&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These results aren’t too interesting, but note that I set up the &lt;code&gt;hole&lt;/code&gt; factor variable to compare the other holes to #12, the hardest on the course. The only hole that wasn’t significantly easier than 12 was #4.&lt;/p&gt;
&lt;p&gt;We can also look at the estimate for the &lt;code&gt;tourn_type&lt;/code&gt; variable. &lt;em&gt;Directionally&lt;/em&gt;, we got the result we were expecting, i.e. sectionals have lower scores relative to par than regionals on average. However, the coefficient is not significant (thumb rule &lt;span class=&#34;math inline&#34;&gt;\(|t| &amp;lt; 2\)&lt;/span&gt;) and its effect size is small, indicating that although the directional effect is negative, we’d have a hard time concluding that the effect is really that meaningful.&lt;/p&gt;
&lt;p&gt;Oh well, let’s do something more interesting with the results from the model&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pulling-by-our-bootstraps&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pulling by our bootstraps&lt;/h3&gt;
&lt;p&gt;What I really want is to be able to visualize not only the difficulty of the holes (i.e. a point estimate), but also look at the uncertainty in that difficulty.&lt;/p&gt;
&lt;p&gt;Using the solution from this &lt;a href=&#34;https://stats.stackexchange.com/a/147837/99673&#34;&gt;CrossValidated post&lt;/a&gt;, we’ll use the bootstrap to generate predictions on a “dummy” data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# initial setup
dummy_table &amp;lt;- tidy_scores2 %&amp;gt;%
  select(hole_f, tourn_type) %&amp;gt;%
  unique()

# taken from https://stats.stackexchange.com/a/147837/99673
predFun &amp;lt;- function(fit) {
  predict(fit, dummy_table, re.form = NA)
}
# fit the bootstrap...takes a longish time
bb &amp;lt;- bootMer(simple_mod,
              nsim = 500,
              FUN = predFun,
              seed = 101)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After munging the data a bit, we’re left with a data set of predictions from the bootstrap.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# use the results from the boostrapped model
dummy_table_boot &amp;lt;- cbind(dummy_table, t(bb$t)) %&amp;gt;%
  gather(iter, rel_to_par, -hole_f, -tourn_type) %&amp;gt;%
  mutate(hole_n = as.numeric(as.character(hole_f)))

dummy_table_boot %&amp;gt;% head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   hole_f tourn_type iter rel_to_par hole_n
## 1      1 sectionals    1   1.586058      1
## 2      1  regionals    1   1.637165      1
## 3      2 sectionals    1   1.293660      2
## 4      2  regionals    1   1.344768      2
## 5      3 sectionals    1   1.262038      3
## 6      3  regionals    1   1.313146      3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use these results to visualize the distributions of predictions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that for both regional and sectional tournaments, holes 1, 4, 7, and 12 were much more difficult than the rest. We can also see that scores relative to par were slightly higher for regional tournaments, and that scores tended to vary a bit more in regional tournaments (the boxes for regionals tend to be a bit longer).&lt;/p&gt;
&lt;p&gt;As I’ll mention in the conclusion of this post, I think we could dive a bit deeper into the mixed-effects models and potentially investigate interaction effects, or build some more interesting models using feature engineering, but this is as far as I want to go for this post.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;walking-up-18-what-separates-the-best-from-the-rest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Walking up 18: What Separates the Best from the Rest&lt;/h2&gt;
&lt;p&gt;Finally, we’re going to take a look at what separates the golfers that broke 90 from the rest of the field.&lt;/p&gt;
&lt;p&gt;To make the next visualization, we’ll use a little bit of &lt;a href=&#34;https://purrr.tidyverse.org/reference/map.html&#34;&gt;&lt;code&gt;map_df&lt;/code&gt;&lt;/a&gt; magic from the &lt;a href=&#34;https://purrr.tidyverse.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package to make a plot that is a cousin of the q-q plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(0,1,.025) %&amp;gt;%
  map_df(~untidy_scores %&amp;gt;% 
           group_by(tourn_type, &amp;#39;p&amp;#39; = .x) %&amp;gt;% 
           summarise(&amp;#39;perc&amp;#39; = quantile(tot, .x))) %&amp;gt;%
  ggplot(aes(p, perc, colour = tourn_type))+
  geom_point(size = 2)+
  scale_x_continuous(&amp;#39;Percentile (lower is better)&amp;#39;, labels = percent)+
  scale_y_continuous(breaks = seq(70, 160, 10),
                     name = &amp;#39;Final Score&amp;#39;)+
  scale_colour_brewer(palette = &amp;#39;Set1&amp;#39;,
                      name = &amp;#39;&amp;#39;)+
  ggtitle(&amp;#39;Final Score Percentile Plot by Tournament Type&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Breaking 90 is generally a sign of a fairly accomplished high school golfer. In the plot above, we can see that the 90 mark is right around the 30th percentile for both the regional and sectional tournaments. This means that about 30% of the competitors scored 90 or lower. One other interesting insight from this chart is that the performance between the regional and sectional tournament is fairly similar (the scores by percentile are pretty close) except for between the 50th and 75th percentiles (the red dots tend to rise a bit above the blue). It could be interesting to dig into this divergence a bit more in a follow-up analysis.&lt;/p&gt;
&lt;p&gt;To explore the difference between those that broke 90 (30% of the field) from those that didn’t, we’ll fit a simple linear model. We’ll use the results of the model to make predictions on a dummy data set and then look at the differences between predicted scores relative to par for each hole for the top 30% and the bottom 70%. We use a linear model to give our analysis a bit more precision than using simple averages across the data. We could also return to this analysis and investigate the coefficients or add complexity to the model if appropriate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_scores3 &amp;lt;- tidy_scores2 %&amp;gt;%
  mutate(broke_90 = ifelse(tot &amp;lt; 90, &amp;#39;Broke 90&amp;#39;, &amp;#39;Did not Break 90&amp;#39;))
# linear model with effects for hole, broke_90 indicator
# and interaction between hole and broke_90
fit2 &amp;lt;- lm(rel_to_par ~ hole_f * broke_90, data = tidy_scores3)

dummy_table2 &amp;lt;- tidy_scores3 %&amp;gt;%
  select(hole_f, broke_90) %&amp;gt;%
  unique()

cbind(dummy_table2, 
      predict(fit2, newdata = dummy_table2, interval = &amp;#39;prediction&amp;#39;)) %&amp;gt;%
  select(hole_f, broke_90, fit) %&amp;gt;%
  spread(broke_90, fit) %&amp;gt;%
  mutate(diff_avg = `Did not Break 90` - `Broke 90`) %&amp;gt;%
  ggplot(aes(reorder(hole_f, -diff_avg), diff_avg))+
  geom_col()+
  xlab(&amp;#39;Hole (ordered by average difference)&amp;#39;)+
  ylab(&amp;#39;Average Stroke Improvement from Bottom 70% to Top 30%&amp;#39;)+
  ggtitle(&amp;#39;Where do the Better Golfers Shine?&amp;#39;,
          subtitle = paste0(&amp;#39;The golfers finishing in the top 30% tended to&amp;#39;,
                            &amp;#39; perform better on the more difficult holes.&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The chart above demonstrates two clear reasons why the top 30% fared better than the rest:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;They played the hardest holes better
&lt;ul&gt;
&lt;li&gt;The top 30% were more than a stroke better than the bottom 70% on holes 1, 4, 7, and 12&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;They took advantage of the easiest holes
&lt;ul&gt;
&lt;li&gt;The top 30% were also more than a stroke better than the bottom 70% on hole 11 (the easiest hole on the course)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results from this quick analysis show that the better players &lt;strong&gt;take advantage of the easiest holes&lt;/strong&gt; and &lt;strong&gt;minimize their mistakes on the hardest holes&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;signing-the-scorecard-final-thoughts-and-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Signing the Scorecard: Final Thoughts and Summary&lt;/h2&gt;
&lt;p&gt;In this blog post I talked about tidy data and used data analysis to inform decisions on the golf course. Of course, data science can be used to do more than just guide golfers to lower scores, but I thought it was an interesting application.&lt;/p&gt;
&lt;p&gt;The four toughest holes were 1, 4, 7, and 12. Each of these holes require thought off the tee, and some holes have challenging approach shots to the green. We saw that players who broke 90 tended to outperform their higher-scoring counterparts on these holes by more than a stroke.&lt;/p&gt;
&lt;p&gt;The easiest hole was number 11, a straightforward par 5 with little trouble, a generous fairway, and usually has a helping wind. This hole had the highest percentage of birdies on the course, and the top golfers took advantage of it. The top 30% played this hole more than a stroke better than the bottom 70%.&lt;/p&gt;
&lt;p&gt;It was difficult to identify clear differences between the results from regional and sectional tournaments, but we did see some divergence in final scores between the 50th through 75th percentiles.&lt;/p&gt;
&lt;p&gt;We can use the results from this analysis to guide golfers’ strategy a bit. First, they should take advantage of hole 11, as there is almost no risk to being aggressive on this hole. Second, they should think carefully and formulate a game plan for the tee shots on 1, 4, 7, and 12. These holes play as the most difficult, and a lot of the challenge comes from the tee shot. Finally, it’s always a good idea to remember that you’re playing golf, and that it’s a game and it’s supposed to be fun!&lt;/p&gt;
&lt;p&gt;In this blog post I used some data science techniques to explore an interesting data set. I think this analysis could be expanded to include more statistical modeling aided by some feature engineering (are there certain characteristics of holes we should investigate? what about player characteristics?). We could also dig deeper into the top 30% and try to determine differences &lt;em&gt;within&lt;/em&gt; that group, to find commonalities among the best of the best. Finally, it might be interesting to use data from a weather service to identify which years had difficult conditions, and estimate a rain or wind effect on final scores.&lt;/p&gt;
&lt;p&gt;Thanks for reading this post and feel free to leave a comment below if you have any thoughts or feedback!&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded><div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>I’m going to use this post to discuss some of the aspects of data science that interest me most (tidy data as well as using data to guide strategy). I’ll be discussing these topics through the lens of a data analysis of results from a few high school golf tournaments.</p>
<p>I’m going to take a little bit of time to talk about <a href="https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html"><strong>tidy data</strong></a>. When I scraped the data used for this analysis, it wasn’t really stored in a tidy format, and there’s a good reason for that. I’ll briefly discuss what makes the original data “untidy”, and what we can do to whip it into tidy shape.</p>
<p>After that, I’ll explore the data a little bit, with the goal of using the findings from this analysis to</p>
<ol style="list-style-type: decimal">
<li>improve strategy on the golf course</li>
<li>inspire ideas for looking deeper at the data</li>
</ol>
<p>Finally, I’ll show how we can use <a href="https://en.wikipedia.org/wiki/Linear_model">linear models</a> along with <a href="https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf">mixed-effects linear models</a> to build statistical models which allow us to quantify differences between groups of interest.</p>
<p>Here are the questions I hope to answer using the data:</p>
<ul>
<li>What are the most difficult holes? Why are they difficult?</li>
<li>What is the easiest hole? What makes it easier than the rest?</li>
<li>Are there clear differences in scores between regional and sectional golf tournaments?</li>
<li>What separates the better high school golfers (those that break 90) from the other golfers?</li>
<li>What are some general strategy guidelines for playing this golf course?</li>
</ul>
<p>All of the analysis in this post was performed using <code>R</code>. I’ve omitted some of the code used to generate plots to improve readability. The full .Rmd file can be found <a href="https://github.com/bgstieber/files_for_blog/blob/master/golf-tidy-data/2018-05-27-golf-tidy-data-and-using-data-analysis-for-strategy.Rmd">on my GitHub</a>.</p>
</div>
<div id="some-useful-context" class="section level2">
<h2>Some Useful Context</h2>
<p>This analysis will involve looking at results from WIAA golf tournaments that were hosted at <a href="https://www.golfpinevalley.net/">Pine Valley Golf Course (PV)</a>. Since 2011, either a regional or sectional tournament was played at PV for Division 3 high schools. Pine Valley is a par 71 golf course, with two par 3’s, six par 4’s and one par 5 on the front nine, and two par 3’s, five par 4’s and two par 5’s on the back nine. Here’s an image of the scorecard:</p>
<center>
<img src="https://photos.bluegolf.com/bd/10/80/e2/4bef45efb375fe1ae2739672_l.jpg">
</center>
<p>Regional tournaments come before sectionals, with teams and individuals that played well in the regional tournament advancing to sectionals. Teams and individuals that play well in the sectional tournament will advance to the state tournament. It is a fair (and testable!) hypothesis that scores will tend to be better in sectional tournaments.</p>
</div>
<div id="hitting-the-range-show-me-the-data" class="section level2">
<h2>Hitting the Range: Show me the data</h2>
<p>First I’m going to load a few packages.</p>
<pre class="r"><code>library(tidyverse)
library(scales)
library(lme4)
theme_set(theme_bw())</code></pre>
<p>I’ve already scraped the data and cleaned it up a bit. I’ve thrown the code and data in a <a href="https://github.com/bgstieber/files_for_blog/tree/master/golf-tidy-data">github repository</a>.</p>
<pre class="r"><code>gh &lt;- &quot;https://raw.githubusercontent.com/bgstieber/files_for_blog/master/golf-tidy-data/data&quot;
# read in data and only keep scores lower than 121
tidy_scores &lt;- read_csv(paste0(gh, &#39;/tidy_golf_scores.csv&#39;)) %&gt;%
  filter(tot &lt;= 120)
untidy_scores &lt;- read_csv(paste0(gh, &#39;/untidy_golf_scores.csv&#39;)) %&gt;%
  filter(tot &lt;= 120)</code></pre>
<p>The data is comprised of 472 golfers with scores for each of the 18 holes they played. In total, there are 8496 scores in this data set.</p>
<p>Let’s take a peek at the first few rows of each data set. The first data set we’ll look at is the untidy data, this is fairly close to what the WIAA provides on the webpages for each of the tournaments.</p>
<table>
<thead>
<tr class="header">
<th align="left">name</th>
<th align="right">year</th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">7</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">out</th>
<th align="right">10</th>
<th align="right">11</th>
<th align="right">12</th>
<th align="right">13</th>
<th align="right">14</th>
<th align="right">15</th>
<th align="right">16</th>
<th align="right">17</th>
<th align="right">18</th>
<th align="right">in</th>
<th align="right">tot</th>
<th align="left">tourn_type</th>
<th align="right">tourn_year</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A1</td>
<td align="right">12</td>
<td align="right">7</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">45</td>
<td align="right">6</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">46</td>
<td align="right">91</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
</tr>
<tr class="even">
<td align="left">A2</td>
<td align="right">12</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">41</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="right">5</td>
<td align="right">3</td>
<td align="right">5</td>
<td align="right">46</td>
<td align="right">87</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
</tr>
<tr class="odd">
<td align="left">A3</td>
<td align="right">12</td>
<td align="right">7</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">49</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">4</td>
<td align="right">7</td>
<td align="right">6</td>
<td align="right">7</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">52</td>
<td align="right">101</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
</tr>
<tr class="even">
<td align="left">A4</td>
<td align="right">11</td>
<td align="right">8</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="right">7</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">51</td>
<td align="right">6</td>
<td align="right">7</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">45</td>
<td align="right">96</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
</tr>
<tr class="odd">
<td align="left">A5</td>
<td align="right">11</td>
<td align="right">14</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="right">7</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">51</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">11</td>
<td align="right">9</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="right">64</td>
<td align="right">115</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
</tr>
<tr class="even">
<td align="left">A6</td>
<td align="right">11</td>
<td align="right">6</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="right">41</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="right">3</td>
<td align="right">4</td>
<td align="right">44</td>
<td align="right">85</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
</tr>
</tbody>
</table>
<p>This is an example of a “wide” data set.</p>
<p>Now let’s take a look at the first few rows of the tidy data.</p>
<table>
<thead>
<tr class="header">
<th align="left">name</th>
<th align="right">year</th>
<th align="right">out</th>
<th align="right">in</th>
<th align="right">tot</th>
<th align="left">tourn_type</th>
<th align="right">tourn_year</th>
<th align="right">hole</th>
<th align="right">score</th>
<th align="right">par</th>
<th align="left">ob</th>
<th align="left">water</th>
<th align="left">side</th>
<th align="right">rel_to_par</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A1</td>
<td align="right">12</td>
<td align="right">45</td>
<td align="right">46</td>
<td align="right">91</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
<td align="right">1</td>
<td align="right">7</td>
<td align="right">5</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">front</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">A2</td>
<td align="right">12</td>
<td align="right">41</td>
<td align="right">46</td>
<td align="right">87</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">front</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">A3</td>
<td align="right">12</td>
<td align="right">49</td>
<td align="right">52</td>
<td align="right">101</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
<td align="right">1</td>
<td align="right">7</td>
<td align="right">5</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">front</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">A4</td>
<td align="right">11</td>
<td align="right">51</td>
<td align="right">45</td>
<td align="right">96</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
<td align="right">1</td>
<td align="right">8</td>
<td align="right">5</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">front</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="left">A5</td>
<td align="right">11</td>
<td align="right">51</td>
<td align="right">64</td>
<td align="right">115</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
<td align="right">1</td>
<td align="right">14</td>
<td align="right">5</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">front</td>
<td align="right">9</td>
</tr>
<tr class="even">
<td align="left">A6</td>
<td align="right">11</td>
<td align="right">41</td>
<td align="right">44</td>
<td align="right">85</td>
<td align="left">sectionals</td>
<td align="right">2011</td>
<td align="right">1</td>
<td align="right">6</td>
<td align="right">5</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">front</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<p>On inspection, there are some clear differences between the tidy and untidy data sets.</p>
</div>
<div id="putting-practice-what-makes-data-tidy" class="section level2">
<h2>Putting Practice: What makes data tidy?</h2>
<p>In some ways, recognizing tidy/untidy data can be one of those <a href="https://en.wikipedia.org/wiki/I_know_it_when_I_see_it"><em>I know it when I see it</em></a> things. We can follow the fairly solid guidelines <a href="https://www.jstatsoft.org/article/view/v059i10">Hadley Wickham has proposed</a> to make the distinction a bit more concrete (if you haven’t read that paper, I <strong>highly</strong> recommend it):</p>
<ul>
<li>Each <strong>variable</strong> is a column</li>
<li>Each <strong>observation</strong> is a row</li>
<li>Each type of observational unit is a table (this isn’t as important for this post)</li>
</ul>
<p>It’s important to think about those two bolded words within the context of this analysis. We’re interested in making our way around a golf course, to hopefully play our best. Playing our best means minimizing mistakes <em>throughout</em> a round of golf.</p>
<p>Some people like to think of golf as a game composed of 18 “mini games” within it. For an analysis with the primary focus of identifying ways to get around the course as strategically as possible, I think the best way to look at the data is to have each observational unit be the score on <strong>one hole</strong> for <strong>each competitor</strong>. Our tidy data set is constructed this way, with <strong>one row per competitor per hole</strong>. The untidy data has a structure of <strong>one row per competitor</strong>. This structure may be useful if we’re only interested in looking at final scores for each competitor. It’s also useful for a concise summary of a competitor’s performance on a website (which was its original purpose).</p>
<p>Transforming the data from untidy to tidy is fairly simple using the <a href="https://tidyr.tidyverse.org/reference/gather.html"><code>gather</code></a> function from the <a href="https://tidyr.tidyverse.org/"><strong><code>tidyr</code></strong></a> package.</p>
<pre class="r"><code>untidy_scores %&gt;%
  # only select a handful of columns to make printing easier
  select(-year, -out, -`in`, -tot, -tourn_type, -tourn_year) %&gt;%
  gather(hole, score, -name)</code></pre>
<pre><code>## # A tibble: 8,496 x 3
##    name  hole  score
##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;
##  1 A1    1         7
##  2 A2    1         5
##  3 A3    1         7
##  4 A4    1         8
##  5 A5    1        14
##  6 A6    1         6
##  7 A7    1         7
##  8 A8    1         6
##  9 A9    1         7
## 10 A10   1        11
## # ... with 8,486 more rows</code></pre>
<p>Now that we have the data in a way that should be straightforward to analyze, let’s start taking a look at some summaries of the results from these competitions.</p>
</div>
<div id="teeing-off-exploring-the-data" class="section level2">
<h2>Teeing Off: Exploring the Data</h2>
<p>We could either look at the scores on each hole or we could look at the score <em>relative to par</em> for each hole. Scores will generally be higher depending on the par of the hole. Par 3’s are shorter than par 4’s which are shorter than par 5’s, and the longer the hole the more strokes (usually) it will take to complete. Since this represents an inherent “bias” in the score data, I think it’s better to analyze the score relative to par.</p>
<p>First, we’ll visualize the distribution of scores relative to par.</p>
<p><img src="/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>We’re able to see a few things pretty quickly from the heatmap above:</p>
<ul>
<li>Holes 4 and 12 are clearly the hardest on the course, with over 50% of the field making double bogey or worse</li>
<li>Hole 11 is the easiest on the course, with the highest percentage of the field making a birdie or better, and the lowest percentage making double or worse.</li>
<li>About 50% of the field made a bogey on 13, which is a par 3</li>
</ul>
<p>Next, we’re going to take a look at the average score relative to par for each of the 18 holes played at PV. Each of the 18 holes played at least one stroke over par.</p>
<p><img src="/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>The four hardest holes are (in order of difficulty): 12, 4, 7, and 1.</p>
<p>PV was my home course growing up, so here’s a little <em>local knowledge</em> about each of those holes.</p>
<div id="hole-12" class="section level4">
<h4>Hole 12</h4>
<p>12 is the longest par 4 on the course, playing at about 430 yards from the back tees. The tee shot can be intimidating with out-of-bounds along the left, and some trees and mounds on the right. This hole seemed to play into the wind more often than downwind, making it even longer than the 430 yards on the card.</p>
</div>
<div id="hole-4" class="section level4">
<h4>Hole 4</h4>
<p>4 is rated as the hardest hole on the golf course according to the USGA handicapping system. The tee shot can be somewhat difficult, with out-of-bounds, trees, and water on the left, and more trees on the right. That being said, the hole is not overly long, so hitting something less than driver is not a bad route. The green is guarded by a stream in front of it, and hilly terrain surrounding it. Putting on this green can be difficult, as it has a lot of slope and undulation.</p>
</div>
<div id="hole-7" class="section level4">
<h4>Hole 7</h4>
<p>7 is a short dogleg left. The main difficulty on this hole is the tight dogleg golfers must navigate off the tee. There are trees which can block an approach shot if the tee shot veers too far right, and hills along with trees and water on the left side for those getting too aggressive off the tee. The approach into the green (provided one has a clear shot) is not that difficult, with no bunkers and a lot of room to miss. The major difficulty on this hole is the tee shot, but from there it’s fairly straightforward.</p>
</div>
<div id="hole-1" class="section level4">
<h4>Hole 1</h4>
<p>1 is a shorter par 5 with a daunting tee shot. There’s OB on the left side, and trees and fescue along the right side. On top of that, a golfer must steady their first tee jitters and focus on hitting a solid tee shot in front of the “gallery” of other players, coaches, and spectators. After hitting the tee shot, the fun doesn’t end. The approach into the green is challenging, as trees surround the green and the hole narrows all the way into the green</p>
<p>Each of the four most difficult holes requires a well-struck tee shot. On the 12th, it’s important to hit a long and straight shot. The 4th and 1st holes require precision and steady nerves to avoid trouble and hit a narrow fairway. The 7th requires a controlled and well-executed tee shot that is shaped right to left.</p>
<p>The next visualization is a <a href="http://datavizproject.com/data-type/lollipop-chart/">lollipop chart</a>, which is a great way to avoid the “visually aggressive” <a href="https://en.wikipedia.org/wiki/Moir%C3%A9_pattern">moire effect</a> that bar charts can sometimes fall victim to without losing the perceptually sound concept of <a href="https://pdfs.semanticscholar.org/565d/843c2c0e60915709268ac4224894469d82d5.pdf">position along a common scale</a>. <a href="https://juliasilge.com">Julia Silge</a> uses this chart type to great effect in some of her <a href="https://juliasilge.com/blog/gender-pronouns/">blog posts</a>.</p>
<p><img src="/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>We took the same data that was displayed in the previous chart, but augmented it slightly to tell a different story. The orange lollipops on the chart show the holes that have higher scores relative to par than average. Three of the four hardest holes are on the front nine, occurring in the first seven holes of the course. We can also see how much more difficult holes 1, 4, 7, and 12 are than the rest (the “sticks” of the lollipops rise much higher than the other holes which played harder than average).</p>
<p>The easiest holes on the course can be identified as well. Hole 11 stands out as being the easiest hole at PV. Hole 11 is a straightforward par 5 with almost no trouble off the tee and a fairly generous fairway all the way to the green. The hole typically plays downwind (it’s routed in the opposite direction of 12), allowing longer hitters the option of trying to reach it in two.</p>
</div>
</div>
<div id="making-the-turn-statistical-modeling" class="section level2">
<h2>Making the Turn: Statistical Modeling</h2>
<p>After a bit of exploratory analysis, we can move on to using statistical modeling to briefly investigate the differences between holes with a little more precision. Additionally, we can use a simple model to test the hypothesis that scores (in relation to par) will be lower at sectional tournaments as opposed to regional.</p>
<p>First, we’ll fit a linear mixed-effects model. I haven’t really worked with mixed-effects models too much since grad school, but this model shouldn’t be too hard to describe.</p>
<pre class="r"><code># create factor variables for tournament year and hole to make
# modeling a bit easier
hardest_holes &lt;- c(12L, 4L, 7L, 1L, 18L, 
                   15L, 10L, 3L, 13L, 
                   17L, 6L, 2L, 16L, 
                   8L, 14L, 9L, 5L, 11L)

tidy_scores2 &lt;- tidy_scores %&gt;%
  mutate(tourn_year_f = factor(tourn_year),
         hole_f = factor(hole,levels = hardest_holes))
## look at past performance to find most difficult holes
simple_mod &lt;- lmer(rel_to_par ~ hole_f+tourn_type+(1|tourn_year_f)+(1|tourn_year_f:name),
                   data = tidy_scores2)</code></pre>
<p>In the model above, we’re fitting <em>fixed effects</em> for the hole variable and the tournament type variable. We’re using <code>(1|tourn_year_f)</code> to fit a random effect for the tournament year, and using <code>(1|tourn_year_f:name)</code> to fit a random effect for the competitor <em>nested within</em> tournament year. A simple way to think about the fixed versus random effects divide is that if we’re interested in understanding the impact of a variable on our target, we should probably fit it as a fixed effect, if we’re not that interested a random effect is the way to go (this is a gross over-simplification, I’d see <a href="https://dynamicecology.wordpress.com/2015/11/04/is-it-a-fixed-or-random-effect/">this post</a> if you’re looking for something more in-depth).</p>
<p>Alright, so we fit the model, now what?</p>
<div id="check-out-the-coefficients" class="section level3">
<h3>Check out the coefficients</h3>
<p>Let’s see what a summary of the model looks like</p>
<pre class="r"><code>summary(simple_mod)$coefficients</code></pre>
<pre><code>##                         Estimate Std. Error    t value
## (Intercept)           1.88291299 0.09070789  20.757985
## hole_f4              -0.06991525 0.06812175  -1.026328
## hole_f7              -0.18432203 0.06812175  -2.705774
## hole_f1              -0.22881356 0.06812175  -3.358891
## hole_f18             -0.40889831 0.06812175  -6.002463
## hole_f15             -0.44279661 0.06812175  -6.500077
## hole_f10             -0.44703390 0.06812175  -6.562278
## hole_f3              -0.48728814 0.06812175  -7.153194
## hole_f13             -0.53601695 0.06812175  -7.868514
## hole_f17             -0.53813559 0.06812175  -7.899615
## hole_f6              -0.54661017 0.06812175  -8.024018
## hole_f2              -0.55296610 0.06812175  -8.117321
## hole_f16             -0.61864407 0.06812175  -9.081447
## hole_f8              -0.64406780 0.06812175  -9.454657
## hole_f14             -0.67372881 0.06812175  -9.890069
## hole_f9              -0.68008475 0.06812175  -9.983371
## hole_f5              -0.68432203 0.06812175 -10.045573
## hole_f11             -0.80084746 0.06812175 -11.756119
## tourn_typesectionals -0.06353004 0.09519262  -0.667384</code></pre>
<p>These results aren’t too interesting, but note that I set up the <code>hole</code> factor variable to compare the other holes to #12, the hardest on the course. The only hole that wasn’t significantly easier than 12 was #4.</p>
<p>We can also look at the estimate for the <code>tourn_type</code> variable. <em>Directionally</em>, we got the result we were expecting, i.e. sectionals have lower scores relative to par than regionals on average. However, the coefficient is not significant (thumb rule <span class="math inline">\(|t| &lt; 2\)</span>) and its effect size is small, indicating that although the directional effect is negative, we’d have a hard time concluding that the effect is really that meaningful.</p>
<p>Oh well, let’s do something more interesting with the results from the model</p>
</div>
<div id="pulling-by-our-bootstraps" class="section level3">
<h3>Pulling by our bootstraps</h3>
<p>What I really want is to be able to visualize not only the difficulty of the holes (i.e. a point estimate), but also look at the uncertainty in that difficulty.</p>
<p>Using the solution from this <a href="https://stats.stackexchange.com/a/147837/99673">CrossValidated post</a>, we’ll use the bootstrap to generate predictions on a “dummy” data set.</p>
<pre class="r"><code># initial setup
dummy_table &lt;- tidy_scores2 %&gt;%
  select(hole_f, tourn_type) %&gt;%
  unique()

# taken from https://stats.stackexchange.com/a/147837/99673
predFun &lt;- function(fit) {
  predict(fit, dummy_table, re.form = NA)
}
# fit the bootstrap...takes a longish time
bb &lt;- bootMer(simple_mod,
              nsim = 500,
              FUN = predFun,
              seed = 101)</code></pre>
<p>After munging the data a bit, we’re left with a data set of predictions from the bootstrap.</p>
<pre class="r"><code># use the results from the boostrapped model
dummy_table_boot &lt;- cbind(dummy_table, t(bb$t)) %&gt;%
  gather(iter, rel_to_par, -hole_f, -tourn_type) %&gt;%
  mutate(hole_n = as.numeric(as.character(hole_f)))

dummy_table_boot %&gt;% head</code></pre>
<pre><code>##   hole_f tourn_type iter rel_to_par hole_n
## 1      1 sectionals    1   1.586058      1
## 2      1  regionals    1   1.637165      1
## 3      2 sectionals    1   1.293660      2
## 4      2  regionals    1   1.344768      2
## 5      3 sectionals    1   1.262038      3
## 6      3  regionals    1   1.313146      3</code></pre>
<p>We can use these results to visualize the distributions of predictions.</p>
<p><img src="/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-15-1.png" width="768" /></p>
<p>We see that for both regional and sectional tournaments, holes 1, 4, 7, and 12 were much more difficult than the rest. We can also see that scores relative to par were slightly higher for regional tournaments, and that scores tended to vary a bit more in regional tournaments (the boxes for regionals tend to be a bit longer).</p>
<p>As I’ll mention in the conclusion of this post, I think we could dive a bit deeper into the mixed-effects models and potentially investigate interaction effects, or build some more interesting models using feature engineering, but this is as far as I want to go for this post.</p>
</div>
</div>
<div id="walking-up-18-what-separates-the-best-from-the-rest" class="section level2">
<h2>Walking up 18: What Separates the Best from the Rest</h2>
<p>Finally, we’re going to take a look at what separates the golfers that broke 90 from the rest of the field.</p>
<p>To make the next visualization, we’ll use a little bit of <a href="https://purrr.tidyverse.org/reference/map.html"><code>map_df</code></a> magic from the <a href="https://purrr.tidyverse.org/"><strong><code>purrr</code></strong></a> package to make a plot that is a cousin of the q-q plot.</p>
<pre class="r"><code>seq(0,1,.025) %&gt;%
  map_df(~untidy_scores %&gt;% 
           group_by(tourn_type, &#39;p&#39; = .x) %&gt;% 
           summarise(&#39;perc&#39; = quantile(tot, .x))) %&gt;%
  ggplot(aes(p, perc, colour = tourn_type))+
  geom_point(size = 2)+
  scale_x_continuous(&#39;Percentile (lower is better)&#39;, labels = percent)+
  scale_y_continuous(breaks = seq(70, 160, 10),
                     name = &#39;Final Score&#39;)+
  scale_colour_brewer(palette = &#39;Set1&#39;,
                      name = &#39;&#39;)+
  ggtitle(&#39;Final Score Percentile Plot by Tournament Type&#39;)</code></pre>
<p><img src="/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Breaking 90 is generally a sign of a fairly accomplished high school golfer. In the plot above, we can see that the 90 mark is right around the 30th percentile for both the regional and sectional tournaments. This means that about 30% of the competitors scored 90 or lower. One other interesting insight from this chart is that the performance between the regional and sectional tournament is fairly similar (the scores by percentile are pretty close) except for between the 50th and 75th percentiles (the red dots tend to rise a bit above the blue). It could be interesting to dig into this divergence a bit more in a follow-up analysis.</p>
<p>To explore the difference between those that broke 90 (30% of the field) from those that didn’t, we’ll fit a simple linear model. We’ll use the results of the model to make predictions on a dummy data set and then look at the differences between predicted scores relative to par for each hole for the top 30% and the bottom 70%. We use a linear model to give our analysis a bit more precision than using simple averages across the data. We could also return to this analysis and investigate the coefficients or add complexity to the model if appropriate.</p>
<pre class="r"><code>tidy_scores3 &lt;- tidy_scores2 %&gt;%
  mutate(broke_90 = ifelse(tot &lt; 90, &#39;Broke 90&#39;, &#39;Did not Break 90&#39;))
# linear model with effects for hole, broke_90 indicator
# and interaction between hole and broke_90
fit2 &lt;- lm(rel_to_par ~ hole_f * broke_90, data = tidy_scores3)

dummy_table2 &lt;- tidy_scores3 %&gt;%
  select(hole_f, broke_90) %&gt;%
  unique()

cbind(dummy_table2, 
      predict(fit2, newdata = dummy_table2, interval = &#39;prediction&#39;)) %&gt;%
  select(hole_f, broke_90, fit) %&gt;%
  spread(broke_90, fit) %&gt;%
  mutate(diff_avg = `Did not Break 90` - `Broke 90`) %&gt;%
  ggplot(aes(reorder(hole_f, -diff_avg), diff_avg))+
  geom_col()+
  xlab(&#39;Hole (ordered by average difference)&#39;)+
  ylab(&#39;Average Stroke Improvement from Bottom 70% to Top 30%&#39;)+
  ggtitle(&#39;Where do the Better Golfers Shine?&#39;,
          subtitle = paste0(&#39;The golfers finishing in the top 30% tended to&#39;,
                            &#39; perform better on the more difficult holes.&#39;))</code></pre>
<p><img src="/post/2018-06-24-golf-tidy-data-and-using-data-analysis-to-guide-strategy_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The chart above demonstrates two clear reasons why the top 30% fared better than the rest:</p>
<ol style="list-style-type: decimal">
<li>They played the hardest holes better
<ul>
<li>The top 30% were more than a stroke better than the bottom 70% on holes 1, 4, 7, and 12</li>
</ul></li>
<li>They took advantage of the easiest holes
<ul>
<li>The top 30% were also more than a stroke better than the bottom 70% on hole 11 (the easiest hole on the course)</li>
</ul></li>
</ol>
<p>The results from this quick analysis show that the better players <strong>take advantage of the easiest holes</strong> and <strong>minimize their mistakes on the hardest holes</strong>.</p>
</div>
<div id="signing-the-scorecard-final-thoughts-and-summary" class="section level2">
<h2>Signing the Scorecard: Final Thoughts and Summary</h2>
<p>In this blog post I talked about tidy data and used data analysis to inform decisions on the golf course. Of course, data science can be used to do more than just guide golfers to lower scores, but I thought it was an interesting application.</p>
<p>The four toughest holes were 1, 4, 7, and 12. Each of these holes require thought off the tee, and some holes have challenging approach shots to the green. We saw that players who broke 90 tended to outperform their higher-scoring counterparts on these holes by more than a stroke.</p>
<p>The easiest hole was number 11, a straightforward par 5 with little trouble, a generous fairway, and usually has a helping wind. This hole had the highest percentage of birdies on the course, and the top golfers took advantage of it. The top 30% played this hole more than a stroke better than the bottom 70%.</p>
<p>It was difficult to identify clear differences between the results from regional and sectional tournaments, but we did see some divergence in final scores between the 50th through 75th percentiles.</p>
<p>We can use the results from this analysis to guide golfers’ strategy a bit. First, they should take advantage of hole 11, as there is almost no risk to being aggressive on this hole. Second, they should think carefully and formulate a game plan for the tee shots on 1, 4, 7, and 12. These holes play as the most difficult, and a lot of the challenge comes from the tee shot. Finally, it’s always a good idea to remember that you’re playing golf, and that it’s a game and it’s supposed to be fun!</p>
<p>In this blog post I used some data science techniques to explore an interesting data set. I think this analysis could be expanded to include more statistical modeling aided by some feature engineering (are there certain characteristics of holes we should investigate? what about player characteristics?). We could also dig deeper into the top 30% and try to determine differences <em>within</em> that group, to find commonalities among the best of the best. Finally, it might be interesting to use data from a weather service to identify which years had difficult conditions, and estimate a rain or wind effect on final scores.</p>
<p>Thanks for reading this post and feel free to leave a comment below if you have any thoughts or feedback!</p>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>An Introduction to the kmeans Algorithm</title>
      <link>/post/an-introduction-to-the-kmeans-algorithm/</link>
      <pubDate>May 28, 2018</pubDate>
      
      <guid>/post/an-introduction-to-the-kmeans-algorithm/</guid>
      <description>&lt;p&gt;This post will provide an &lt;code&gt;R&lt;/code&gt; code-heavy, math-light introduction to selecting the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; in &lt;strong&gt;k&lt;/strong&gt; means. It presents the main idea of kmeans, demonstrates how to fit a kmeans in &lt;code&gt;R&lt;/code&gt;, provides some components of the kmeans fit, and displays some methods for selecting &lt;code&gt;k&lt;/code&gt;. In addition, the post provides some helpful functions which may make fitting kmeans a bit easier.&lt;/p&gt;
&lt;p&gt;kmeans clustering is an example of &lt;a href=&#34;https://en.wikipedia.org/wiki/Unsupervised_learning&#34;&gt;unsupervised learning&lt;/a&gt;, where we do not have an output we’re explicitly trying to predict. We may have reasons to believe that there are latent groups within a dataset, so a clustering method can be a useful way to explore and describe pockets of similar observations within a dataset.&lt;/p&gt;
&lt;div id=&#34;the-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Algorithm&lt;/h2&gt;
&lt;p&gt;Here’s basically what kmeans (the algorithm) does (taken from &lt;a href=&#34;https://www.r-statistics.com/2013/08/k-means-clustering-from-r-in-action/&#34;&gt;K-means Clustering&lt;/a&gt;):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Selects K centroids (K rows chosen at random)&lt;/li&gt;
&lt;li&gt;Assigns each data point to its closest centroid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recalculates the centroids as the average of all data points in a cluster (i.e., the centroids are p-length mean vectors, where p is the number of variables)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Assigns data points to their closest centroids&lt;/li&gt;
&lt;li&gt;Continues steps 3 and 4 until the observations are not reassigned or the &lt;strong&gt;maximum number of iterations&lt;/strong&gt; (&lt;code&gt;R&lt;/code&gt; uses 10 as a default) is reached.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here it is in gif form (taken from &lt;a href=&#34;http://simplystatistics.org/2014/02/18/k-means-clustering-in-a-gif/&#34;&gt;k-means clustering in a GIF&lt;/a&gt;):&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;post_images/kmeans.gif&#34; /&gt;
&lt;/center&gt;
&lt;div id=&#34;a-balls-and-urns-explanation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A balls and urns explanation&lt;/h3&gt;
&lt;p&gt;As a statistician, I have hard time avoiding resorting to using balls and urns to describe statistical concepts.&lt;/p&gt;
&lt;p&gt;Suppose we have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; balls, and each ball has &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; characteristics, like &lt;span class=&#34;math inline&#34;&gt;\(shape\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(size\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(density\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;, and we want to put those &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; balls into &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; urns (clusters) according to the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; characteristics.&lt;/p&gt;
&lt;p&gt;First, we randomly select &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; balls (&lt;span class=&#34;math inline&#34;&gt;\(balls_{init}\)&lt;/span&gt;), and assign the rest of the balls (&lt;span class=&#34;math inline&#34;&gt;\(n-k\)&lt;/span&gt;) to whichever &lt;span class=&#34;math inline&#34;&gt;\(balls_{init}\)&lt;/span&gt; it is closest to. After this first assignment, we calculate the centroid of each (&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;) collection of balls. The centroids are the averages of the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; characteristics of the balls in each cluster. So, for each cluster, there will be a vector of length &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; with the means of the characteristics of the balls &lt;em&gt;in that cluster&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;After the calculation of the centroid, we then calculate (for each ball) the distances between its &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; characteristics and the centroids for each cluster. We assign the ball to the cluster with the centroid it is closest to. Then, we recalculate the centroids and repeat the process. We leave the number of clusters (&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;) fixed, but we allow the balls to move between the clusters, depending on which cluster they are closest to.&lt;/p&gt;
&lt;p&gt;Either the algorithm will “converge” and between time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt; no reassignments will occur, or we’ll reach the maximum number of iterations allowed by the algorithm.&lt;/p&gt;
&lt;p&gt;##&lt;code&gt;kmeans&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here’s how we use the &lt;code&gt;kmeans&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kmeans(x, centers, iters.max, nstart)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;arguments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;arguments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is our data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;centers&lt;/code&gt; is the &lt;strong&gt;k&lt;/strong&gt; in kmeans&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iters.max&lt;/code&gt; controls the &lt;strong&gt;maximum number of iterations&lt;/strong&gt;, if the algorithm has not converged, it’s good to bump this number up&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nstart&lt;/code&gt; controls the initial configurations (step 1 in the algorithm), bumping this number up is a good idea, since kmeans tends to be sensitive to initial conditions (which may remind you of &lt;a href=&#34;https://en.wikipedia.org/wiki/Chaos_theory#Sensitivity_to_initial_conditions&#34;&gt;sensitivity to initial conditions in chaos theory&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;###values it returns&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kmeans&lt;/code&gt; returns an object of class “kmeans” which has a &lt;code&gt;print&lt;/code&gt; and a &lt;code&gt;fitted&lt;/code&gt; method. It is a list with at least the following components:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;cluster&lt;/strong&gt; - A vector of integers (from 1:k) indicating the cluster to which each point is allocated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;centers&lt;/strong&gt; - A matrix of cluster centers &lt;strong&gt;these are the centroids for each cluster&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;totss&lt;/strong&gt; - The total sum of squares.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;withinss&lt;/strong&gt; - Vector of within-cluster sum of squares, one component per cluster.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tot.withinss&lt;/strong&gt; - Total within-cluster sum of squares, i.e. sum(withinss).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;betweenss&lt;/strong&gt; - The between-cluster sum of squares, i.e. totss-tot.withinss.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;size&lt;/strong&gt; - The number of points in each cluster.&lt;/p&gt;
&lt;p&gt;To use &lt;code&gt;kmeans&lt;/code&gt;, we first need to specify the &lt;code&gt;k&lt;/code&gt;. How should we do this?&lt;/p&gt;
&lt;p&gt;##Data&lt;/p&gt;
&lt;p&gt;For this post, we’ll be using the &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Housing&#34;&gt;Boston housing data set&lt;/a&gt;. This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston, MA.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##      crim zn indus   nox    rm  age    dis rad tax ptratio  black lstat medv
## 1 0.00632 18  2.31 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98 24.0
## 2 0.02731  0  7.07 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14 21.6
## 3 0.02729  0  7.07 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03 34.7
## 4 0.03237  0  2.18 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94 33.4
## 5 0.06905  0  2.18 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33 36.2
## 6 0.02985  0  2.18 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21 28.7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-within-cluster-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualize within Cluster Error&lt;/h2&gt;
&lt;p&gt;Use a scree plot to visualize the reduction in within-cluster error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ss_kmeans &amp;lt;- t(sapply(2:14, 
                  FUN = function(k) 
                  kmeans(x = b_housing, 
                         centers = k, 
                         nstart = 20, 
                         iter.max = 25)[c(&amp;#39;tot.withinss&amp;#39;,&amp;#39;betweenss&amp;#39;)]))

plot(2:14, unlist(ss_kmeans[,1]), xlab = &amp;#39;Clusters&amp;#39;, ylab = &amp;#39;Within Cluster SSE&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we look at the scree plot, we’re looking for the “elbow”. We can see the SSE dropping, but at some point it discontinues its rapid dropping. At what cluster does it stop dropping abruptly?&lt;/p&gt;
&lt;p&gt;Stated more verbosely from &lt;a href=&#34;https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set&#34;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The elbow method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn’t give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the “elbow criterion”. This “elbow” cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can get the percentage of variance explained by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tot.ss &amp;lt;- sum(apply(b_housing, 2, var)) * (nrow(b_housing) - 1)

var_explained &amp;lt;- unlist(ss_kmeans[,2]) / tot.ss

plot(2:14, var_explained, xlab = &amp;#39;Clusters&amp;#39;, ylab = &amp;#39;% of Total Variation Explained&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Where does the elbow occur in the above plot? That’s pretty subjective (a common theme in unsupervised learning), but for our task we would prefer to have &lt;span class=&#34;math inline&#34;&gt;\(\leq 10\)&lt;/span&gt; clusters, probably.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-aic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualize AIC&lt;/h2&gt;
&lt;p&gt;We could also opt for the AIC, which basically looks at how well the clusters are fitting to the data, while also penalizing how many clusters are in the final fit. The general rule with AIC is that lower values are better.&lt;/p&gt;
&lt;p&gt;First, we define a &lt;a href=&#34;http://stackoverflow.com/questions/15839774/how-to-calculate-bic-for-k-means-clustering-in-r&#34;&gt;function which calculates the AIC&lt;/a&gt; from the output of &lt;code&gt;kmeans&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kmeansAIC &amp;lt;- function(fit){

  m = ncol(fit$centers) 
  k = nrow(fit$centers)
  D = fit$tot.withinss
  return(D + 2*m*k)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aic_k &amp;lt;- sapply(2:14, FUN = 
         function(k) 
           kmeansAIC(kmeans(b_housing, centers = k, nstart = 20, iter.max = 25)))

plot(2:14, aic_k, xlab = &amp;#39;Clusters&amp;#39;, ylab = &amp;#39;AIC from kmeans&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Look familiar? It is remarkably similar to looking at the SSE. This is because the main component in calculating AIC is the within-cluster sum of squared errors. Once again, we’re looking for an elbow in the plot, indicating that the decrease in AIC is not happening so rapidly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-bic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualize BIC&lt;/h2&gt;
&lt;p&gt;BIC is related to AIC in that BIC is AIC’s conservative cousin. When we evaluate models using BIC rather than AIC as our metric, we tend to select smaller models. Calculating BIC is rather similar to that of AIC (we replaced 2 in the AIC calculation with &lt;code&gt;log(n)&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kmeansBIC &amp;lt;- function(fit){
  m = ncol(fit$centers) 
  n = length(fit$cluster)
  k = nrow(fit$centers)
  D = fit$tot.withinss
  return(D + log(n) * m * k) # using log(n) instead of 2, penalize model complexity
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bic_k &amp;lt;- sapply(2:14, FUN = 
         function(k) 
           kmeansBIC(kmeans(b_housing, centers = k, nstart = 20, iter.max = 25)))

plot(2:14, aic_k, xlab = &amp;#39;Clusters&amp;#39;, ylab = &amp;#39;BIC from kmeans&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once again, the plots are rather similar for this toy example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-it-all-together-in-kmeans2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrap it all together in &lt;code&gt;kmeans2&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;We can wrap all the previous parts together in a function to get a broad look at the fit of &lt;code&gt;kmeans&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We’ll fit &lt;code&gt;kmeans&lt;/code&gt; across a range of centers (&lt;code&gt;center_range&lt;/code&gt;). Using the results from these fits, we’ll look at AIC, BIC, within cluster variation, and the % of total variation explained. We can choose to spit out a table to the user (&lt;code&gt;plot = FALSE&lt;/code&gt;) or we’ll plot each of the four metrics by the number of clusters (&lt;code&gt;plot = TRUE&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kmeans2 &amp;lt;- function(data, center_range, iter.max, nstart, plot = TRUE){
  
  #fit kmeans for each center
  all_kmeans &amp;lt;- lapply(center_range, 
                       FUN = function(k) 
                         kmeans(data, center = k, iter.max = iter.max, nstart = nstart))
  
  #extract AIC from each
  all_aic &amp;lt;- sapply(all_kmeans, kmeansAIC)
  #extract BIC from each
  all_bic &amp;lt;- sapply(all_kmeans, kmeansBIC)
  #extract tot.withinss
  all_wss &amp;lt;- sapply(all_kmeans, FUN = function(fit) fit$tot.withinss)
  #extract between ss
  btwn_ss &amp;lt;- sapply(all_kmeans, FUN = function(fit) fit$betweenss)
  #extract totall sum of squares
  tot_ss &amp;lt;- all_kmeans[[1]]$totss
  #put in data.frame
  clust_res &amp;lt;- 
    data.frame(&amp;#39;Clusters&amp;#39; = center_range, 
             &amp;#39;AIC&amp;#39; = all_aic, 
             &amp;#39;BIC&amp;#39; = all_bic, 
             &amp;#39;WSS&amp;#39; = all_wss,
             &amp;#39;BSS&amp;#39; = btwn_ss,
             &amp;#39;TSS&amp;#39; = tot_ss)
  #plot or no plot?
  if(plot){
    par(mfrow = c(2,2))
    with(clust_res,{
      plot(Clusters, AIC)
      plot(Clusters, BIC)
      plot(Clusters, WSS, ylab = &amp;#39;Within Cluster SSE&amp;#39;)
      plot(Clusters, BSS / TSS, ylab = &amp;#39;Prop of Var. Explained&amp;#39;)
    })
  }else{
    return(clust_res)
  }
  
}


kmeans2(data = b_housing, center_range = 2:15, iter.max = 20, nstart = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-a-package-to-determine-k&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use a package to determine &lt;code&gt;k&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;This is &lt;code&gt;R&lt;/code&gt; after all, so surely there must be at least one package to help in determining the “best” number of clusters. &lt;a href=&#34;https://cran.r-project.org/web/packages/NbClust/index.html&#34;&gt;&lt;strong&gt;&lt;code&gt;NbClust&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; is a viable option.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(NbClust)

best.clust &amp;lt;- NbClust(data = b_housing, 
                      min.nc = 2, 
                      max.nc = 15, 
                      method = &amp;#39;kmeans&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 7 proposed 2 as the best number of clusters 
## * 2 proposed 3 as the best number of clusters 
## * 12 proposed 4 as the best number of clusters 
## * 1 proposed 5 as the best number of clusters 
## * 1 proposed 8 as the best number of clusters 
## * 1 proposed 15 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  4 
##  
##  
## *******************************************************************&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;NbClust&lt;/code&gt;&lt;/strong&gt; returns a big object with some information that may or may not be useless for this use case (I stored the rest of the output in &lt;code&gt;best.clust&lt;/code&gt;, but the package still spit out a bunch of stuff). But, it does tell you the best number of clusters as selected by a slew of indices. This function must iterate through all the possible clusters from &lt;code&gt;min.nc&lt;/code&gt; to &lt;code&gt;max.nc&lt;/code&gt;, &lt;strong&gt;so it may not be very quick&lt;/strong&gt;, but it does give another way of selecting the number of clusters.&lt;/p&gt;
&lt;p&gt;You may want to find a &lt;em&gt;reasonable&lt;/em&gt; range for &lt;code&gt;min.nc&lt;/code&gt; and &lt;code&gt;max.nc&lt;/code&gt; before resorting to the &lt;code&gt;NbClust&lt;/code&gt; function. If you know that 3 clusters won’t be enough, don’t make &lt;code&gt;NbClust&lt;/code&gt; even consider it as an option.&lt;/p&gt;
&lt;p&gt;There’s also an argument called &lt;code&gt;index&lt;/code&gt; in the &lt;code&gt;NbClust&lt;/code&gt; function. This value controls which indices are used to determine the best number of clusters. The calculation methods differ between indices and if your data isn’t so nice (e.g. variables with few unique values), the function may fail. The default value is &lt;code&gt;all&lt;/code&gt;, which is a collection of 30 (!) indices all used to help determine the best number of clusters.&lt;/p&gt;
&lt;p&gt;It may be helpful to try different indices such as &lt;code&gt;tracew&lt;/code&gt;, &lt;code&gt;kl&lt;/code&gt;, &lt;code&gt;dindex&lt;/code&gt; or &lt;code&gt;duda&lt;/code&gt;. Unfortunately, you’ll need to specify only one index for each &lt;code&gt;NbClust&lt;/code&gt; call (unless you use &lt;code&gt;index = &#39;all&#39;&lt;/code&gt; or &lt;code&gt;index = &#39;alllong&#39;&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;For more details look at the &lt;a href=&#34;https://www.rdocumentation.org/packages/NbClust/versions/3.0/topics/NbClust&#34;&gt;function’s documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-the-centroids&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizing the centroids&lt;/h2&gt;
&lt;p&gt;This function helps to visualize the centroids for each cluster. It can allow for interpretation of clusters.&lt;/p&gt;
&lt;p&gt;The arguments for this function are &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;levels&lt;/code&gt;, and &lt;code&gt;show_N&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fit&lt;/code&gt;: object returned from a call to &lt;code&gt;kmeans&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;levels&lt;/code&gt;: a character vector representing the levels of the variables in the data set used to fit &lt;code&gt;kmeans&lt;/code&gt;, this vector will allow a user to control the order in which variables are plotted&lt;/li&gt;
&lt;li&gt;&lt;code&gt;show_N&lt;/code&gt;: a logical value, if TRUE, the plot will contain information about the size of each cluster, if FALSE, a table of counts will be printed prior to the plot&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To use the &lt;code&gt;levels&lt;/code&gt; argument, the character vector you supply must have the same number of elements as the number of unique variables in the data set used to fit the &lt;code&gt;kmeans&lt;/code&gt;. If you specify &lt;code&gt;levels = c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;)&lt;/code&gt; the plotting device will display (from top to bottom) &lt;code&gt;&#39;c&#39;,&#39;b&#39;,&#39;a&#39;&lt;/code&gt;. If you are not satisfied with the plotting order, the &lt;code&gt;rev&lt;/code&gt; function may come in handy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kmeans_viz &amp;lt;- function(fit, levels = NULL, show_N = TRUE){
  require(ggplot2)
  require(dplyr)
  #extract number of clusters
  clusts &amp;lt;- length(unique(fit$cluster))
  #centroids
  kmeans.table &amp;lt;- as.data.frame(t(fit$center), stringsAsFactors = FALSE)
  #variable names
  kmeans.table$Variable &amp;lt;- row.names(kmeans.table)
  #name clusters
  names(kmeans.table)[1:clusts] &amp;lt;- paste0(&amp;#39;cluster&amp;#39;, 1:clusts)
  #reshape from wide table to long (makes plotting easier)
  kmeans.table &amp;lt;- reshape(kmeans.table, direction = &amp;#39;long&amp;#39;,
                        idvar = &amp;#39;Variable&amp;#39;, 
                        varying = paste0(&amp;#39;cluster&amp;#39;, 1:clusts),
                        v.names = &amp;#39;cluster&amp;#39;)
  
  #number of observations in each cluster
  #should we show N in the graph or just print it?
  if(show_N){
    #show it in the graph
  kmeans.table$time &amp;lt;- paste0(kmeans.table$time,
                             &amp;#39; (N = &amp;#39;,
                             fit$size[kmeans.table$time],
                             &amp;#39;)&amp;#39;)
  }else{
    #just print it
    print(rbind(&amp;#39;Cluster&amp;#39; = 1:clusts,
          &amp;#39;N&amp;#39; = fit$size))
  }
  #standardize the cluster means to make a nice plot
  kmeans.table %&amp;gt;%
    group_by(Variable) %&amp;gt;%
    mutate(cluster_stdzd = (cluster - mean(cluster)) / sd(cluster)) -&amp;gt; kmeans.table
  #did user specify a variable levels vector?
  if(length(levels) == length(unique(kmeans.table$Variable))){
    kmeans.table$Variable &amp;lt;- factor(kmeans.table$Variable, levels = levels)
  }
  
  #make the plot
  ggplot(kmeans.table, aes(x = Variable, y = time))+
    geom_tile(colour = &amp;#39;black&amp;#39;, aes(fill = cluster_stdzd))+
    geom_text(aes(label = round(cluster,2)))+
    coord_flip()+
    xlab(&amp;#39;&amp;#39;)+ylab(&amp;#39;Cluster&amp;#39;)+
    scale_fill_gradient(low = &amp;#39;white&amp;#39;, high = &amp;#39;grey60&amp;#39;)+
    theme_bw()+
    theme(legend.position = &amp;#39;none&amp;#39;,
          axis.title.y = element_blank(),
          axis.title.x = element_text(size = 16),
          panel.grid = element_blank(),
          axis.text = element_text(size = 14),
          axis.ticks = element_blank())

}

opt.kmeans &amp;lt;- kmeans(b_housing, centers = 4, nstart = 50, iter.max = 50)

kmeans_viz(opt.kmeans)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-kmeans-to-predict&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using &lt;code&gt;kmeans&lt;/code&gt; to predict&lt;/h2&gt;
&lt;p&gt;We can predict cluster membership using a few techniques. For the simple plug-and-play method, we can use the &lt;code&gt;cl_predict&lt;/code&gt; function from the &lt;a href=&#34;https://cran.r-project.org/web/packages/clue/index.html&#34;&gt;&lt;strong&gt;&lt;code&gt;clue&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; package. For those interested in a more manual approach, we can calculate the centroid distances for the new data, and select whichever cluster is the shortest distance away.&lt;/p&gt;
&lt;p&gt;I will demonstrate both techniques.&lt;/p&gt;
&lt;p&gt;First, we’re going to select a subset of the Boston dataset to fit a &lt;code&gt;kmeans&lt;/code&gt; on. Using the result of &lt;code&gt;kmeans&lt;/code&gt; fit on &lt;code&gt;b_hous.train&lt;/code&gt;, we’ll try to predict the clusters for a “new” dataset, &lt;code&gt;b_hous.test&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#rows to select
set.seed(123)
train_samps &amp;lt;- sample(nrow(b_housing), .7 * nrow(b_housing), replace = F)
#create training and testing set
b_hous.train &amp;lt;- b_housing[train_samps,]
b_hous.test &amp;lt;- b_housing[-train_samps,]

#fit our new kmeans
train.kmeans &amp;lt;- kmeans(b_hous.train, centers = 4, nstart = 50, iter.max = 50)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;use-cl_predict&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use &lt;code&gt;cl_predict&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The interface is fairly simple to get the predicted values.&lt;/p&gt;
&lt;p&gt;We’re going to use &lt;code&gt;system.time&lt;/code&gt; to time how long it takes &lt;code&gt;R&lt;/code&gt; to do what we want it to.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(clue)

system.time(
  test_clusters.clue &amp;lt;- cl_predict(object = train.kmeans, newdata = b_hous.test)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##    0.02    0.00    0.02&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test_clusters.clue)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## test_clusters.clue
##  1  2  3  4 
## 84 15 27 26&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;by-hand&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;By hand&lt;/h3&gt;
&lt;p&gt;Taken from &lt;a href=&#34;http://stats.stackexchange.com/questions/78322/is-there-a-function-in-r-that-takes-the-centers-of-clusters-that-were-found-and&#34;&gt;this nice CrossValidated solution&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clusters &amp;lt;- function(x, centers) {
  # compute squared euclidean distance from each sample to each cluster center
  tmp &amp;lt;- sapply(seq_len(nrow(x)),
                function(i) apply(centers, 1,
                                  function(v) sum((x[i, ]-v)^2)))
  max.col(-t(tmp))  # find index of min distance
}

system.time(
  test_clusters.hand &amp;lt;- clusters(x = b_hous.test, centers = train.kmeans$centers)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##    1.81    0.00    1.81&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test_clusters.hand)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## test_clusters.hand
##  1  2  3  4 
## 84 15 27 26&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(test_clusters.hand == test_clusters.clue) #TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that &lt;code&gt;clusters&lt;/code&gt; is slower than &lt;code&gt;cl_predict&lt;/code&gt;, but they return the same result. It would be prudent to use &lt;code&gt;cl_predict&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wrapping Up&lt;/h3&gt;
&lt;p&gt;In this post I walked through the kmeans algorithm, and its implementation in &lt;code&gt;R&lt;/code&gt;. Additionally, I discussed some of the ways to select the &lt;code&gt;k&lt;/code&gt; in kmeans. The process of selecting and evaluating choices of &lt;code&gt;k&lt;/code&gt; will vary from project to project and depend strongly on the goals of an analysis.&lt;/p&gt;
&lt;p&gt;It is worth noting that one of the drawbacks of kmeans clustering is that it must put &lt;em&gt;every&lt;/em&gt; observation into a cluster. There may be anomalies or outliers present in a dataset, so it may not always make sense to enforce the condition that each observation is assigned to a cluster. A different unsupervised learning technique, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/DBSCAN&#34;&gt;dbscan&lt;/a&gt; (density-based spatial clustering of applications with noise) may be more appropriate for tasks in which anomaly detection is necessary. I hope to explore this technique in a future post. In the meantime, &lt;a href=&#34;https://medium.com/netflix-techblog/tracking-down-the-villains-outlier-detection-at-netflix-40360b31732&#34;&gt;here’s an example&lt;/a&gt; of Netflix applying dbscan for anomaly detection.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      <content:encoded><p>This post will provide an <code>R</code> code-heavy, math-light introduction to selecting the <span class="math inline">\(k\)</span> in <strong>k</strong> means. It presents the main idea of kmeans, demonstrates how to fit a kmeans in <code>R</code>, provides some components of the kmeans fit, and displays some methods for selecting <code>k</code>. In addition, the post provides some helpful functions which may make fitting kmeans a bit easier.</p>
<p>kmeans clustering is an example of <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a>, where we do not have an output we’re explicitly trying to predict. We may have reasons to believe that there are latent groups within a dataset, so a clustering method can be a useful way to explore and describe pockets of similar observations within a dataset.</p>
<div id="the-algorithm" class="section level2">
<h2>The Algorithm</h2>
<p>Here’s basically what kmeans (the algorithm) does (taken from <a href="https://www.r-statistics.com/2013/08/k-means-clustering-from-r-in-action/">K-means Clustering</a>):</p>
<ol style="list-style-type: decimal">
<li>Selects K centroids (K rows chosen at random)</li>
<li>Assigns each data point to its closest centroid</li>
<li><strong>Recalculates the centroids as the average of all data points in a cluster (i.e., the centroids are p-length mean vectors, where p is the number of variables)</strong></li>
<li>Assigns data points to their closest centroids</li>
<li>Continues steps 3 and 4 until the observations are not reassigned or the <strong>maximum number of iterations</strong> (<code>R</code> uses 10 as a default) is reached.</li>
</ol>
<p>Here it is in gif form (taken from <a href="http://simplystatistics.org/2014/02/18/k-means-clustering-in-a-gif/">k-means clustering in a GIF</a>):</p>
<center>
<img src="post_images/kmeans.gif" />
</center>
<div id="a-balls-and-urns-explanation" class="section level3">
<h3>A balls and urns explanation</h3>
<p>As a statistician, I have hard time avoiding resorting to using balls and urns to describe statistical concepts.</p>
<p>Suppose we have <span class="math inline">\(n\)</span> balls, and each ball has <span class="math inline">\(p\)</span> characteristics, like <span class="math inline">\(shape\)</span>, <span class="math inline">\(size\)</span>, <span class="math inline">\(density\)</span>, <span class="math inline">\(\ldots\)</span>, and we want to put those <span class="math inline">\(n\)</span> balls into <span class="math inline">\(k\)</span> urns (clusters) according to the <span class="math inline">\(p\)</span> characteristics.</p>
<p>First, we randomly select <span class="math inline">\(k\)</span> balls (<span class="math inline">\(balls_{init}\)</span>), and assign the rest of the balls (<span class="math inline">\(n-k\)</span>) to whichever <span class="math inline">\(balls_{init}\)</span> it is closest to. After this first assignment, we calculate the centroid of each (<span class="math inline">\(k\)</span>) collection of balls. The centroids are the averages of the <span class="math inline">\(p\)</span> characteristics of the balls in each cluster. So, for each cluster, there will be a vector of length <span class="math inline">\(p\)</span> with the means of the characteristics of the balls <em>in that cluster</em>.</p>
<p>After the calculation of the centroid, we then calculate (for each ball) the distances between its <span class="math inline">\(p\)</span> characteristics and the centroids for each cluster. We assign the ball to the cluster with the centroid it is closest to. Then, we recalculate the centroids and repeat the process. We leave the number of clusters (<span class="math inline">\(k\)</span>) fixed, but we allow the balls to move between the clusters, depending on which cluster they are closest to.</p>
<p>Either the algorithm will “converge” and between time <span class="math inline">\(t\)</span> and <span class="math inline">\(t+1\)</span> no reassignments will occur, or we’ll reach the maximum number of iterations allowed by the algorithm.</p>
<p>##<code>kmeans</code> in <code>R</code></p>
<p>Here’s how we use the <code>kmeans</code> function in <code>R</code>:</p>
<pre class="r"><code>kmeans(x, centers, iters.max, nstart)</code></pre>
</div>
<div id="arguments" class="section level3">
<h3>arguments</h3>
<ul>
<li><code>x</code> is our data</li>
<li><code>centers</code> is the <strong>k</strong> in kmeans</li>
<li><code>iters.max</code> controls the <strong>maximum number of iterations</strong>, if the algorithm has not converged, it’s good to bump this number up</li>
<li><code>nstart</code> controls the initial configurations (step 1 in the algorithm), bumping this number up is a good idea, since kmeans tends to be sensitive to initial conditions (which may remind you of <a href="https://en.wikipedia.org/wiki/Chaos_theory#Sensitivity_to_initial_conditions">sensitivity to initial conditions in chaos theory</a>)</li>
</ul>
<p>###values it returns</p>
<p><code>kmeans</code> returns an object of class “kmeans” which has a <code>print</code> and a <code>fitted</code> method. It is a list with at least the following components:</p>
<p><strong>cluster</strong> - A vector of integers (from 1:k) indicating the cluster to which each point is allocated.</p>
<p><strong>centers</strong> - A matrix of cluster centers <strong>these are the centroids for each cluster</strong></p>
<p><strong>totss</strong> - The total sum of squares.</p>
<p><strong>withinss</strong> - Vector of within-cluster sum of squares, one component per cluster.</p>
<p><strong>tot.withinss</strong> - Total within-cluster sum of squares, i.e. sum(withinss).</p>
<p><strong>betweenss</strong> - The between-cluster sum of squares, i.e. totss-tot.withinss.</p>
<p><strong>size</strong> - The number of points in each cluster.</p>
<p>To use <code>kmeans</code>, we first need to specify the <code>k</code>. How should we do this?</p>
<p>##Data</p>
<p>For this post, we’ll be using the <a href="https://archive.ics.uci.edu/ml/datasets/Housing">Boston housing data set</a>. This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston, MA.</p>
<pre><code>##      crim zn indus   nox    rm  age    dis rad tax ptratio  black lstat medv
## 1 0.00632 18  2.31 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98 24.0
## 2 0.02731  0  7.07 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14 21.6
## 3 0.02729  0  7.07 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03 34.7
## 4 0.03237  0  2.18 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94 33.4
## 5 0.06905  0  2.18 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33 36.2
## 6 0.02985  0  2.18 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21 28.7</code></pre>
</div>
</div>
<div id="visualize-within-cluster-error" class="section level2">
<h2>Visualize within Cluster Error</h2>
<p>Use a scree plot to visualize the reduction in within-cluster error:</p>
<pre class="r"><code>ss_kmeans &lt;- t(sapply(2:14, 
                  FUN = function(k) 
                  kmeans(x = b_housing, 
                         centers = k, 
                         nstart = 20, 
                         iter.max = 25)[c(&#39;tot.withinss&#39;,&#39;betweenss&#39;)]))

plot(2:14, unlist(ss_kmeans[,1]), xlab = &#39;Clusters&#39;, ylab = &#39;Within Cluster SSE&#39;)</code></pre>
<p><img src="/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>When we look at the scree plot, we’re looking for the “elbow”. We can see the SSE dropping, but at some point it discontinues its rapid dropping. At what cluster does it stop dropping abruptly?</p>
<p>Stated more verbosely from <a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set">Wikipedia</a>:</p>
<blockquote>
<p>The elbow method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn’t give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the “elbow criterion”. This “elbow” cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance.</p>
</blockquote>
<p>We can get the percentage of variance explained by typing:</p>
<pre class="r"><code>tot.ss &lt;- sum(apply(b_housing, 2, var)) * (nrow(b_housing) - 1)

var_explained &lt;- unlist(ss_kmeans[,2]) / tot.ss

plot(2:14, var_explained, xlab = &#39;Clusters&#39;, ylab = &#39;% of Total Variation Explained&#39;)</code></pre>
<p><img src="/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Where does the elbow occur in the above plot? That’s pretty subjective (a common theme in unsupervised learning), but for our task we would prefer to have <span class="math inline">\(\leq 10\)</span> clusters, probably.</p>
</div>
<div id="visualize-aic" class="section level2">
<h2>Visualize AIC</h2>
<p>We could also opt for the AIC, which basically looks at how well the clusters are fitting to the data, while also penalizing how many clusters are in the final fit. The general rule with AIC is that lower values are better.</p>
<p>First, we define a <a href="http://stackoverflow.com/questions/15839774/how-to-calculate-bic-for-k-means-clustering-in-r">function which calculates the AIC</a> from the output of <code>kmeans</code>.</p>
<pre class="r"><code>kmeansAIC &lt;- function(fit){

  m = ncol(fit$centers) 
  k = nrow(fit$centers)
  D = fit$tot.withinss
  return(D + 2*m*k)
  
}</code></pre>
<pre class="r"><code>aic_k &lt;- sapply(2:14, FUN = 
         function(k) 
           kmeansAIC(kmeans(b_housing, centers = k, nstart = 20, iter.max = 25)))

plot(2:14, aic_k, xlab = &#39;Clusters&#39;, ylab = &#39;AIC from kmeans&#39;)</code></pre>
<p><img src="/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Look familiar? It is remarkably similar to looking at the SSE. This is because the main component in calculating AIC is the within-cluster sum of squared errors. Once again, we’re looking for an elbow in the plot, indicating that the decrease in AIC is not happening so rapidly.</p>
</div>
<div id="visualize-bic" class="section level2">
<h2>Visualize BIC</h2>
<p>BIC is related to AIC in that BIC is AIC’s conservative cousin. When we evaluate models using BIC rather than AIC as our metric, we tend to select smaller models. Calculating BIC is rather similar to that of AIC (we replaced 2 in the AIC calculation with <code>log(n)</code>):</p>
<pre class="r"><code>kmeansBIC &lt;- function(fit){
  m = ncol(fit$centers) 
  n = length(fit$cluster)
  k = nrow(fit$centers)
  D = fit$tot.withinss
  return(D + log(n) * m * k) # using log(n) instead of 2, penalize model complexity
}</code></pre>
<pre class="r"><code>bic_k &lt;- sapply(2:14, FUN = 
         function(k) 
           kmeansBIC(kmeans(b_housing, centers = k, nstart = 20, iter.max = 25)))

plot(2:14, aic_k, xlab = &#39;Clusters&#39;, ylab = &#39;BIC from kmeans&#39;)</code></pre>
<p><img src="/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Once again, the plots are rather similar for this toy example.</p>
</div>
<div id="wrap-it-all-together-in-kmeans2" class="section level2">
<h2>Wrap it all together in <code>kmeans2</code></h2>
<p>We can wrap all the previous parts together in a function to get a broad look at the fit of <code>kmeans</code>.</p>
<p>We’ll fit <code>kmeans</code> across a range of centers (<code>center_range</code>). Using the results from these fits, we’ll look at AIC, BIC, within cluster variation, and the % of total variation explained. We can choose to spit out a table to the user (<code>plot = FALSE</code>) or we’ll plot each of the four metrics by the number of clusters (<code>plot = TRUE</code>).</p>
<pre class="r"><code>kmeans2 &lt;- function(data, center_range, iter.max, nstart, plot = TRUE){
  
  #fit kmeans for each center
  all_kmeans &lt;- lapply(center_range, 
                       FUN = function(k) 
                         kmeans(data, center = k, iter.max = iter.max, nstart = nstart))
  
  #extract AIC from each
  all_aic &lt;- sapply(all_kmeans, kmeansAIC)
  #extract BIC from each
  all_bic &lt;- sapply(all_kmeans, kmeansBIC)
  #extract tot.withinss
  all_wss &lt;- sapply(all_kmeans, FUN = function(fit) fit$tot.withinss)
  #extract between ss
  btwn_ss &lt;- sapply(all_kmeans, FUN = function(fit) fit$betweenss)
  #extract totall sum of squares
  tot_ss &lt;- all_kmeans[[1]]$totss
  #put in data.frame
  clust_res &lt;- 
    data.frame(&#39;Clusters&#39; = center_range, 
             &#39;AIC&#39; = all_aic, 
             &#39;BIC&#39; = all_bic, 
             &#39;WSS&#39; = all_wss,
             &#39;BSS&#39; = btwn_ss,
             &#39;TSS&#39; = tot_ss)
  #plot or no plot?
  if(plot){
    par(mfrow = c(2,2))
    with(clust_res,{
      plot(Clusters, AIC)
      plot(Clusters, BIC)
      plot(Clusters, WSS, ylab = &#39;Within Cluster SSE&#39;)
      plot(Clusters, BSS / TSS, ylab = &#39;Prop of Var. Explained&#39;)
    })
  }else{
    return(clust_res)
  }
  
}


kmeans2(data = b_housing, center_range = 2:15, iter.max = 20, nstart = 25)</code></pre>
<p><img src="/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-9-1.png" width="768" /></p>
</div>
<div id="use-a-package-to-determine-k" class="section level2">
<h2>Use a package to determine <code>k</code></h2>
<p>This is <code>R</code> after all, so surely there must be at least one package to help in determining the “best” number of clusters. <a href="https://cran.r-project.org/web/packages/NbClust/index.html"><strong><code>NbClust</code></strong></a> is a viable option.</p>
<pre class="r"><code>library(NbClust)

best.clust &lt;- NbClust(data = b_housing, 
                      min.nc = 2, 
                      max.nc = 15, 
                      method = &#39;kmeans&#39;)</code></pre>
<p><img src="/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-10-1.png" width="768" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-10-2.png" width="768" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 7 proposed 2 as the best number of clusters 
## * 2 proposed 3 as the best number of clusters 
## * 12 proposed 4 as the best number of clusters 
## * 1 proposed 5 as the best number of clusters 
## * 1 proposed 8 as the best number of clusters 
## * 1 proposed 15 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  4 
##  
##  
## *******************************************************************</code></pre>
<p><strong><code>NbClust</code></strong> returns a big object with some information that may or may not be useless for this use case (I stored the rest of the output in <code>best.clust</code>, but the package still spit out a bunch of stuff). But, it does tell you the best number of clusters as selected by a slew of indices. This function must iterate through all the possible clusters from <code>min.nc</code> to <code>max.nc</code>, <strong>so it may not be very quick</strong>, but it does give another way of selecting the number of clusters.</p>
<p>You may want to find a <em>reasonable</em> range for <code>min.nc</code> and <code>max.nc</code> before resorting to the <code>NbClust</code> function. If you know that 3 clusters won’t be enough, don’t make <code>NbClust</code> even consider it as an option.</p>
<p>There’s also an argument called <code>index</code> in the <code>NbClust</code> function. This value controls which indices are used to determine the best number of clusters. The calculation methods differ between indices and if your data isn’t so nice (e.g. variables with few unique values), the function may fail. The default value is <code>all</code>, which is a collection of 30 (!) indices all used to help determine the best number of clusters.</p>
<p>It may be helpful to try different indices such as <code>tracew</code>, <code>kl</code>, <code>dindex</code> or <code>duda</code>. Unfortunately, you’ll need to specify only one index for each <code>NbClust</code> call (unless you use <code>index = 'all'</code> or <code>index = 'alllong'</code>).</p>
<p>For more details look at the <a href="https://www.rdocumentation.org/packages/NbClust/versions/3.0/topics/NbClust">function’s documentation</a>.</p>
</div>
<div id="visualizing-the-centroids" class="section level2">
<h2>Visualizing the centroids</h2>
<p>This function helps to visualize the centroids for each cluster. It can allow for interpretation of clusters.</p>
<p>The arguments for this function are <code>fit</code>, <code>levels</code>, and <code>show_N</code>:</p>
<ul>
<li><code>fit</code>: object returned from a call to <code>kmeans</code></li>
<li><code>levels</code>: a character vector representing the levels of the variables in the data set used to fit <code>kmeans</code>, this vector will allow a user to control the order in which variables are plotted</li>
<li><code>show_N</code>: a logical value, if TRUE, the plot will contain information about the size of each cluster, if FALSE, a table of counts will be printed prior to the plot</li>
</ul>
<p>To use the <code>levels</code> argument, the character vector you supply must have the same number of elements as the number of unique variables in the data set used to fit the <code>kmeans</code>. If you specify <code>levels = c('a','b','c')</code> the plotting device will display (from top to bottom) <code>'c','b','a'</code>. If you are not satisfied with the plotting order, the <code>rev</code> function may come in handy.</p>
<pre class="r"><code>kmeans_viz &lt;- function(fit, levels = NULL, show_N = TRUE){
  require(ggplot2)
  require(dplyr)
  #extract number of clusters
  clusts &lt;- length(unique(fit$cluster))
  #centroids
  kmeans.table &lt;- as.data.frame(t(fit$center), stringsAsFactors = FALSE)
  #variable names
  kmeans.table$Variable &lt;- row.names(kmeans.table)
  #name clusters
  names(kmeans.table)[1:clusts] &lt;- paste0(&#39;cluster&#39;, 1:clusts)
  #reshape from wide table to long (makes plotting easier)
  kmeans.table &lt;- reshape(kmeans.table, direction = &#39;long&#39;,
                        idvar = &#39;Variable&#39;, 
                        varying = paste0(&#39;cluster&#39;, 1:clusts),
                        v.names = &#39;cluster&#39;)
  
  #number of observations in each cluster
  #should we show N in the graph or just print it?
  if(show_N){
    #show it in the graph
  kmeans.table$time &lt;- paste0(kmeans.table$time,
                             &#39; (N = &#39;,
                             fit$size[kmeans.table$time],
                             &#39;)&#39;)
  }else{
    #just print it
    print(rbind(&#39;Cluster&#39; = 1:clusts,
          &#39;N&#39; = fit$size))
  }
  #standardize the cluster means to make a nice plot
  kmeans.table %&gt;%
    group_by(Variable) %&gt;%
    mutate(cluster_stdzd = (cluster - mean(cluster)) / sd(cluster)) -&gt; kmeans.table
  #did user specify a variable levels vector?
  if(length(levels) == length(unique(kmeans.table$Variable))){
    kmeans.table$Variable &lt;- factor(kmeans.table$Variable, levels = levels)
  }
  
  #make the plot
  ggplot(kmeans.table, aes(x = Variable, y = time))+
    geom_tile(colour = &#39;black&#39;, aes(fill = cluster_stdzd))+
    geom_text(aes(label = round(cluster,2)))+
    coord_flip()+
    xlab(&#39;&#39;)+ylab(&#39;Cluster&#39;)+
    scale_fill_gradient(low = &#39;white&#39;, high = &#39;grey60&#39;)+
    theme_bw()+
    theme(legend.position = &#39;none&#39;,
          axis.title.y = element_blank(),
          axis.title.x = element_text(size = 16),
          panel.grid = element_blank(),
          axis.text = element_text(size = 14),
          axis.ticks = element_blank())

}

opt.kmeans &lt;- kmeans(b_housing, centers = 4, nstart = 50, iter.max = 50)

kmeans_viz(opt.kmeans)</code></pre>
<p><img src="/post/2018-05-28-an-introduction-to-the-kmeans-algorithm_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="using-kmeans-to-predict" class="section level2">
<h2>Using <code>kmeans</code> to predict</h2>
<p>We can predict cluster membership using a few techniques. For the simple plug-and-play method, we can use the <code>cl_predict</code> function from the <a href="https://cran.r-project.org/web/packages/clue/index.html"><strong><code>clue</code></strong></a> package. For those interested in a more manual approach, we can calculate the centroid distances for the new data, and select whichever cluster is the shortest distance away.</p>
<p>I will demonstrate both techniques.</p>
<p>First, we’re going to select a subset of the Boston dataset to fit a <code>kmeans</code> on. Using the result of <code>kmeans</code> fit on <code>b_hous.train</code>, we’ll try to predict the clusters for a “new” dataset, <code>b_hous.test</code>.</p>
<pre class="r"><code>#rows to select
set.seed(123)
train_samps &lt;- sample(nrow(b_housing), .7 * nrow(b_housing), replace = F)
#create training and testing set
b_hous.train &lt;- b_housing[train_samps,]
b_hous.test &lt;- b_housing[-train_samps,]

#fit our new kmeans
train.kmeans &lt;- kmeans(b_hous.train, centers = 4, nstart = 50, iter.max = 50)</code></pre>
<div id="use-cl_predict" class="section level3">
<h3>Use <code>cl_predict</code></h3>
<p>The interface is fairly simple to get the predicted values.</p>
<p>We’re going to use <code>system.time</code> to time how long it takes <code>R</code> to do what we want it to.</p>
<pre class="r"><code>library(clue)

system.time(
  test_clusters.clue &lt;- cl_predict(object = train.kmeans, newdata = b_hous.test)
  )</code></pre>
<pre><code>##    user  system elapsed 
##    0.02    0.00    0.02</code></pre>
<pre class="r"><code>table(test_clusters.clue)</code></pre>
<pre><code>## test_clusters.clue
##  1  2  3  4 
## 84 15 27 26</code></pre>
</div>
<div id="by-hand" class="section level3">
<h3>By hand</h3>
<p>Taken from <a href="http://stats.stackexchange.com/questions/78322/is-there-a-function-in-r-that-takes-the-centers-of-clusters-that-were-found-and">this nice CrossValidated solution</a>.</p>
<pre class="r"><code>clusters &lt;- function(x, centers) {
  # compute squared euclidean distance from each sample to each cluster center
  tmp &lt;- sapply(seq_len(nrow(x)),
                function(i) apply(centers, 1,
                                  function(v) sum((x[i, ]-v)^2)))
  max.col(-t(tmp))  # find index of min distance
}

system.time(
  test_clusters.hand &lt;- clusters(x = b_hous.test, centers = train.kmeans$centers)
  )</code></pre>
<pre><code>##    user  system elapsed 
##    1.81    0.00    1.81</code></pre>
<pre class="r"><code>table(test_clusters.hand)</code></pre>
<pre><code>## test_clusters.hand
##  1  2  3  4 
## 84 15 27 26</code></pre>
<pre class="r"><code>all(test_clusters.hand == test_clusters.clue) #TRUE</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>We see that <code>clusters</code> is slower than <code>cl_predict</code>, but they return the same result. It would be prudent to use <code>cl_predict</code>.</p>
</div>
<div id="wrapping-up" class="section level3">
<h3>Wrapping Up</h3>
<p>In this post I walked through the kmeans algorithm, and its implementation in <code>R</code>. Additionally, I discussed some of the ways to select the <code>k</code> in kmeans. The process of selecting and evaluating choices of <code>k</code> will vary from project to project and depend strongly on the goals of an analysis.</p>
<p>It is worth noting that one of the drawbacks of kmeans clustering is that it must put <em>every</em> observation into a cluster. There may be anomalies or outliers present in a dataset, so it may not always make sense to enforce the condition that each observation is assigned to a cluster. A different unsupervised learning technique, such as <a href="https://en.wikipedia.org/wiki/DBSCAN">dbscan</a> (density-based spatial clustering of applications with noise) may be more appropriate for tasks in which anomaly detection is necessary. I hope to explore this technique in a future post. In the meantime, <a href="https://medium.com/netflix-techblog/tracking-down-the-villains-outlier-detection-at-netflix-40360b31732">here’s an example</a> of Netflix applying dbscan for anomaly detection.</p>
</div>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>My First Post</title>
      <link>/post/my-first-post/</link>
      <pubDate>May 26, 2018</pubDate>
      
      <guid>/post/my-first-post/</guid>
      <description>&lt;div id=&#34;welcome-to-my-blog&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Welcome to my blog!&lt;/h1&gt;
&lt;p&gt;I plan to use this website to present data explorations and analyses in a way that’s understandable to a broad audience. I hope to demonstrate the utility of applying ideas like &lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_visualization&#34;&gt;data visualization&lt;/a&gt;, and &lt;a href=&#34;https://en.wikipedia.org/wiki/Exploratory_data_analysis&#34;&gt;exploratory data analysis&lt;/a&gt; to day-to-day life to improve decision-making processes.&lt;/p&gt;
&lt;p&gt;I was inspired to create a blog after reading &lt;a href=&#34;http://varianceexplained.org/r/start-blog/&#34;&gt;this post&lt;/a&gt; by David Robinson.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New blog post: &amp;quot;Advice to aspiring data scientists: start a blog&amp;quot; &lt;a href=&#34;https://t.co/yMDHqviiBN&#34;&gt;https://t.co/yMDHqviiBN&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://t.co/9AdPUdbjtE&#34;&gt;pic.twitter.com/9AdPUdbjtE&lt;/a&gt;&lt;/p&gt;&amp;mdash; David Robinson (@drob) &lt;a href=&#34;https://twitter.com/drob/status/930492543187513345?ref_src=twsrc%5Etfw&#34;&gt;November 14, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;I’m a big believer in treating data analysis as an iterative process, and I hope this blog will reinforce the idea that nearly anyone can learn the skills to do data analysis. I’m going to try as hard as I can to avoid the buzzwords and esoteric language that unnecessarily obfuscate data science discussions, so that this blog is accessible to an audience with varying levels of mathematical and statistical sophistication. That being said, I’ll still try to sneak a few data science nuggets for the hardcore data nerds out there!&lt;/p&gt;
&lt;p&gt;I’ll mostly be using the &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;&lt;strong&gt;&lt;code&gt;R&lt;/code&gt;&lt;/strong&gt; programming language&lt;/a&gt; to extract and manipulate data (you’ll find out I’m a huge &lt;a href=&#34;https://www.crummy.com/software/BeautifulSoup/bs4/doc/&#34;&gt;&lt;strong&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; fan), but you may see me using a bit of &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;strong&gt;&lt;code&gt;Python&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; as well (usually &lt;a href=&#34;http://scikit-learn.org/stable/&#34;&gt;scikit-learn&lt;/a&gt; and some web scraping with &lt;a href=&#34;https://www.crummy.com/software/BeautifulSoup/bs4/doc/&#34;&gt;BeautifulSoup&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Thanks for checking my blog out!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-26-my-first-post_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded><div id="welcome-to-my-blog" class="section level1">
<h1>Welcome to my blog!</h1>
<p>I plan to use this website to present data explorations and analyses in a way that’s understandable to a broad audience. I hope to demonstrate the utility of applying ideas like <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, <a href="https://en.wikipedia.org/wiki/Data_visualization">data visualization</a>, and <a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">exploratory data analysis</a> to day-to-day life to improve decision-making processes.</p>
<p>I was inspired to create a blog after reading <a href="http://varianceexplained.org/r/start-blog/">this post</a> by David Robinson.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New blog post: &quot;Advice to aspiring data scientists: start a blog&quot; <a href="https://t.co/yMDHqviiBN">https://t.co/yMDHqviiBN</a> <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <a href="https://t.co/9AdPUdbjtE">pic.twitter.com/9AdPUdbjtE</a></p>&mdash; David Robinson (@drob) <a href="https://twitter.com/drob/status/930492543187513345?ref_src=twsrc%5Etfw">November 14, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>I’m a big believer in treating data analysis as an iterative process, and I hope this blog will reinforce the idea that nearly anyone can learn the skills to do data analysis. I’m going to try as hard as I can to avoid the buzzwords and esoteric language that unnecessarily obfuscate data science discussions, so that this blog is accessible to an audience with varying levels of mathematical and statistical sophistication. That being said, I’ll still try to sneak a few data science nuggets for the hardcore data nerds out there!</p>
<p>I’ll mostly be using the <a href="https://www.r-project.org/about.html"><strong><code>R</code></strong> programming language</a> to extract and manipulate data (you’ll find out I’m a huge <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/"><strong><code>tidyverse</code></strong></a> fan), but you may see me using a bit of <a href="https://www.python.org/"><strong><code>Python</code></strong></a> as well (usually <a href="http://scikit-learn.org/stable/">scikit-learn</a> and some web scraping with <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a>).</p>
<p>Thanks for checking my blog out!</p>
<p><img src="/post/2018-05-26-my-first-post_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
</content:encoded>

    </item>
    
    <item>
      <title>About me</title>
      <link>/about/</link>
      <pubDate>January 1, 0001</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Work:&lt;/strong&gt; I currently work as a Senior Analyst in the healthcare industry. Prior to that, I was a Senior Data Analyst in the non-profit/fundraising sector. I got my start in data through working in analytics and reporting for a private financing company. On the side, I’ve been a volunteer data scientist for a few different organizations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Education:&lt;/strong&gt; I graduated with an MS in Statistics from the &lt;a href=&#34;https://www.wisc.edu/&#34;&gt;University of Wisconsin&lt;/a&gt; in 2017 (go Badgers! 👐). Prior to that, I graduated with a BS in Mathematical Statistics and Mathematical Economics from &lt;a href=&#34;https://www.stcloudstate.edu/&#34;&gt;St. Cloud State University&lt;/a&gt; in 2015 (go Huskies! 🐺).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Personal:&lt;/strong&gt; I live in &lt;a href=&#34;http://www.cityofmadison.com/&#34;&gt;Madison, WI&lt;/a&gt;. I like to spend my spare time reading, playing guitar, playing golf, biking, and watching whatever Wisconsin sports team is on television.&lt;/p&gt;
&lt;p&gt;My email is bgstieber (at) gmail (dot) com&lt;/p&gt;
</description>
      <content:encoded><p><strong>Work:</strong> I currently work as a Senior Analyst in the healthcare industry. Prior to that, I was a Senior Data Analyst in the non-profit/fundraising sector. I got my start in data through working in analytics and reporting for a private financing company. On the side, I’ve been a volunteer data scientist for a few different organizations.</p>
<p><strong>Education:</strong> I graduated with an MS in Statistics from the <a href="https://www.wisc.edu/">University of Wisconsin</a> in 2017 (go Badgers! 👐). Prior to that, I graduated with a BS in Mathematical Statistics and Mathematical Economics from <a href="https://www.stcloudstate.edu/">St. Cloud State University</a> in 2015 (go Huskies! 🐺).</p>
<p><strong>Personal:</strong> I live in <a href="http://www.cityofmadison.com/">Madison, WI</a>. I like to spend my spare time reading, playing guitar, playing golf, biking, and watching whatever Wisconsin sports team is on television.</p>
<p>My email is bgstieber (at) gmail (dot) com</p>
</content:encoded>

    </item>
    
    <item>
      <title>Blogroll</title>
      <link>/blogroll/</link>
      <pubDate>January 1, 0001</pubDate>
      
      <guid>/blogroll/</guid>
      <description>&lt;div id=&#34;blogs-im-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Blogs I’m reading:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.r-bloggers.com/&#34;&gt;R-bloggers&lt;/a&gt;
- Blog aggregator of content contributed by bloggers who write about R (in English).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://blog.revolutionanalytics.com/&#34;&gt;Revolutions&lt;/a&gt;
- Blog dedicated to news and information of interest to members of the R community.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://andrewgelman.com/&#34;&gt;Statistical Modeling, Causal Inference, and Social Science&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://eagereyes.org/&#34;&gt;eagereyes&lt;/a&gt;
- Robert Kosara’s website, mostly about data visualization&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://juliasilge.com/blog/&#34;&gt;Julia Silge’s blog&lt;/a&gt;
- Great blog about a wide array of data science topics&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://varianceexplained.org/&#34;&gt;Variance Explained&lt;/a&gt;
- David Robinson’s (the data scientist, not the &lt;a href=&#34;https://en.wikipedia.org/wiki/David_Robinson_(basketball)&#34;&gt;basketball player&lt;/a&gt;) blog on data science.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://flowingdata.com/&#34;&gt;FlowingData&lt;/a&gt;
- Nathan Yau’s blog, mostly about data visualization&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/&#34;&gt;Analytics Vidhya&lt;/a&gt;
- Good blog with data science tutorials&lt;/p&gt;
&lt;p&gt;…more to come&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded><div id="blogs-im-reading" class="section level2">
<h2>Blogs I’m reading:</h2>
<p><a href="https://www.r-bloggers.com/">R-bloggers</a>
- Blog aggregator of content contributed by bloggers who write about R (in English).</p>
<p><a href="http://blog.revolutionanalytics.com/">Revolutions</a>
- Blog dedicated to news and information of interest to members of the R community.</p>
<p><a href="http://andrewgelman.com/">Statistical Modeling, Causal Inference, and Social Science</a></p>
<p><a href="https://eagereyes.org/">eagereyes</a>
- Robert Kosara’s website, mostly about data visualization</p>
<p><a href="https://juliasilge.com/blog/">Julia Silge’s blog</a>
- Great blog about a wide array of data science topics</p>
<p><a href="http://varianceexplained.org/">Variance Explained</a>
- David Robinson’s (the data scientist, not the <a href="https://en.wikipedia.org/wiki/David_Robinson_(basketball)">basketball player</a>) blog on data science.</p>
<p><a href="https://flowingdata.com/">FlowingData</a>
- Nathan Yau’s blog, mostly about data visualization</p>
<p><a href="https://www.analyticsvidhya.com/blog/">Analytics Vidhya</a>
- Good blog with data science tutorials</p>
<p>…more to come</p>
</div>
</content:encoded>

    </item>
    
  </channel>
</rss>